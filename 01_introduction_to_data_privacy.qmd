---
title: "Introduction to Data Privacy"
date: today
output:
  html:
    toc: true
    embed-resources: true
    code-line-numbers: true
editor_options:
  chunk_output_type: console
execute:
  warning: false
  message: false
bibliography: references.bib
---

```{=html}
<style>
@import url('https://fonts.googleapis.com/css?family=Lato&display=swap');
</style>
```

```{r}
#| label: setup
#| echo: false

library(tidyverse)
library(palmerpenguins)
library(kableExtra)
library(gt)

options(scipen = 999)

source(here::here("R", "create_table.R"))

```

```{r}
#| echo: false

exercise_number <- 1

```

## Why is Data Privacy important?

-   Modern computing and technology have made it easy to collect and process large amounts of data.
-   Malicious actors can use computing power and advanced techniques to reidentify individuals by linking supposedly anonymized records with public databases.
-   This kind of attack is called a "record linkage" attack. The following are some examples of famous record linkage attacks.
    -   In 1997, MA Gov. Bill Weld announced the public release of insurance data for researchers. He assured the public that PII had been deleted. A few days later, Dr. Latanya Sweeney, then a MIT graduate student, mailed to Weld's office his personal medical information. She purchased voter data and linked Weld's birth date, gender, and zip code to his health records. And this was back in 1997, when computing power was minuscule, and social media didn't exist!
    -   A study by Dr. Latanya Sweeney based on the 1990 Census [@sweeney2000simple] found that the 87% of the US population had reported characteristics that likely made them unique based only on ZIP, gender, and date of birth.
- Malicious actors can also reconstruct databases or by reconstructing databases from statistics.
-   Releasing granular data can be of immense value to researchers. For example, cell phone data are invaluable for emergency responses to natural disasters, and granular medical data will lead to better treatment and development of cures.

![](www/images/racial_data_covid.png){width="600"}

-   More granular data are also important for understanding equity, particularly for smaller populations and subgroups.

## What is Data Privacy?

::: {.callout-tip}
## Data Privacy

**Data Privacy** is the ability "to determine what information about ourselves we will share with others." [@fellegi1972question].

:::

- There are at least three major threats to data privacy.
    1. **Hackers:** adversaries who steal confidential information through unauthorized access.
    2. **Snoopers:** adversaries who reconstruct confidential information from data releases. 
    3. **Hoarders:** stewards who collect data but don't release the data even if respondents want the information released. 

-   There are differing notions of what should and shouldn't be private, which may include being able to opt out of or opt into disclosure protections.

-   Data privacy is a broad topic, which includes data security, encryption, access to data, etc. We will not be covering privacy breaches from unauthorized access to a database (e.g., hackers).

-   We are instead focused on privacy preserving access to data.

-   Although data privacy and data confidentiality are certainly related, they are different, and both play a role in limiting statistical disclosure risk.

::: {.callout-tip}
## Confidentiality

**Confidentiality** is "the agreement, explicit or implicit, between data subject and data collector regarding the extent to which access by others to personal information is allowed" [@fienberg2018statistical].
:::  

-   There is often a tension between privacy and data utility (or usefulness). This tension is referred to in the data privacy literature as the "privacy-utility trade-off."

    -   For example, some universities require students to install an app that tracks their movements on campus. This allows professors teaching large classes with 100+ students to know their students' punctuality, tardiness, or class absences. This tracking can be invasive, especially for students who rarely leave campus except during holidays, because the university could track their movements outside of class. However, the tracking app could alert students about an active shooter on campus, identify safe buildings to seek refuge, and notify emergency contacts regarding the students' safety.

::: {.callout-tip}
## Data utility, quality, accuracy, or usefulness

**Data utility, quality, accuracy, or usefulness** is how practically useful or accurate to the data are for research and analysis purposes.
::: 

-   *Generally*, higher utility = more privacy risks and vice versa.

-   In the data privacy ecosystem there are the following stakeholders:

::: {.callout-tip}
## Data users and practitioners

**Data users and practitioners** are individuals who consume the data, such as analysts, researchers, planners, and decision-makers.
::: 

::: {.callout-tip}
## Data privacy experts or researchers

**Data privacy experts or researchers** are individuals who specialize in developing data privacy and confidentiality methods.
::: 

::: {.callout-tip}
## Data curators, maintainers, or stewards

**Data curators, maintainers, or stewards** are individuals who posess the data and are responsible for its safekeeping.
::: 

::: {.callout-tip}
## Data intruders, attackers, or adversaries

**Data intruders, attackers, or adversaries** are individuals who try to gather sensitive information from the confidential data.
::: 

-   In addition, there are many versions of the data we should define:

::: {.callout-tip}
## Original dataset:

**Original dataset** is the uncleaned, unprotected version of the data.

For example, raw Decennial census microdata, which are never publicly released.
::: 

::: {.callout-tip}
## Confidential or gold standard dataset

**Confidential or gold standard dataset** is the cleaned version (meaning edited for inaccuracies or inconsistencies) of the data; often referred to as the gold standard or actual data for analysis. 

For example, the Census Edited File that is the final confidential data for the 2020 Census. This dataset is never publicly released but may be made available to others who are sworn to protect confidentiality and who are provided access in a secure environment, such as a Federal Statistical Research Data Center.
::: 

::: {.callout-tip}
## Public dataset

**Public dataset** is the publicly released version of the confidential data.

For example, the US Census Bureau's public tables and datasets.
::: 

## Data Privacy Workflow

-   Data users have traditionally gained access to data via:

    a)  direct access to the confidential data if they are trusted users (e.g., obtaining Special Sworn Status to use the Federal Statistical Research Data Centers).

    b)  Access to public data or statistics, such as public microdata and summary tables, that the data curators and privacy experts produced with modification to protect confidentiality.

-   The latter is how most data users gain access to information from confidential data and what we will focus on for this course. To create public data or statistics, data curators rely on statistical disclosure control (SDC) or limitation (SDL) methods to preserve data confidentiality. The process of releasing this information publicly often involves the steps shown in @fig-process.

![Process for releasing information](www/images/data_privacy_workflow.png){#fig-process width="518"}

-   There is potential for additional disclosure risks if Step 4 and Step 5 are repeated too many times. We can think of this like data leakage in a machine learning/predictive modeling framework.

## Overview of SDC

::: {.callout-tip}
## Statistical Disclosure Control (SDC)

**Statistical Disclosure Control (SDC)** or Statistical Disclosure Limitation (SDL) is a field of study that aims to develop methods for releasing high-quality data products while preserving data confidentiality as a means of maintaining privacy.
:::

-   SDC methods have existed within statistics and the social sciences since the mid-twentieth century.

-   Below is an opinionated, and incomplete, overview of various SDC methods. For this set of training sessions, we will focus in-depth on the methods in yellow.

![](www/images/mermaid.png)

```{r echo = FALSE, eval = FALSE}
#| label: sdc_overview_mmd

# color code things that can be formally private
DiagrammeR::mermaid("
graph TB
  B(SDC Methods)-->D(Synthetic Data)
  B-->E(Suppression)
  B-->F(Swapping)
  B-->H(Generalization)
  B-->I(Noise Infusion)
  B-->J(Sampling)
  D-->K(Partial)
  D-->L(Full)
  style D fill:#fdbf11;
  style K fill:#fdbf11
  style L fill:#fdbf11
",
height = 250,
width = 800)
```

-   Here are a few traditional methods from the SDC literature. See @matthews2011data for more information.

    -   **Suppression:** Not releasing data about certain subgroups or witholding information about certaint observations.

    -   **Swapping:** The exchange of sensitive values among sample units with similar characteristics.

    -   **Generalization:** Aggregating variables into larger units (e.g., reporting state rather than zip code) or top/bottom coding (limiting values below/above a threshold to the threshold value).

    -   **Noise Infusion:** Adding random noise, often to continuous variables which can maintain univariate distributions.

    -   **Sampling:** Only releasing a sample of individual records.

-   The problem with the above approaches is that it really limits the utility of the data.

    -   Mitra and Reiter (2006) found that a 5 percent swapping of 2 identifying variables in the 1987 Survey of Youth in Custody invalidated statistical hypothesis tests in regression.

    -   Top/bottom coding eliminates information at the tails of the distributions, degrading analyses that depend on the entire distribution (Fuller 1993; Reiter, Wang, and Zhang, 2014).

-   Synthetic data can help overcome some of these issues. 

## Formal Privacy

-   Formal privacy centers around a formal mathematical definition of privacy that measures how much privacy leakage occurs.

-   A formally private method provides mathematically provable protection to respondents and allows policy makers to manage the trade-off between data accuracy and privacy protection through tunable parameters for multiple statistics.

-   Any SDC method/algorithm can be formally private if it meets the specific mathematical definition.

-   Formal privacy is a definition/threshold for methods to meet, not a method in and of itself.

-   Below in green are the SDC methods which have formally private versions.

![](www/images/mermaid2.png)

```{r sdc_fp_overview_mmd, echo = FALSE, eval = FALSE}


DiagrammeR::mermaid("
graph TB
  B(SDC Methods)-->D(Synthetic Data)
  B-->E(Suppression)
  B-->F(Swapping)
  B-->H(Generalization)
  B-->I(Noise Infusion)
  B-->J(Sampling)
  D-->K(Partial)
  D-->L(Full)
  style D fill:#55b748;
  style K fill:#55b748
  style L fill:#55b748
  style I fill:#55b748

",
height = 250,
width = 800)
```

-   In this course, we will cover 2 kinds of formal privacy definitions.

    a)  **Differential Privacy** (DP): A mathematical definition of what it means to have privacy. The definition has a parameter called epsilon that control how much privacy loss occurs.

    b)  **Approximate Differential Privacy**: Similar to DP in the same framework, but it relaxes the definition.

::: {.callout-tip}
## Threat model

A **threat model** is a set of assumptions for how data intruders will attack the data, including computing power, access to external data sources, etc.
:::

-   A key difference between traditional SDC methods and formally private SDC methods is the threat models behind each approach. 

-   Formal privacy definitions assume the worst possible scenario, where an attacker has all the information but 1 record, has every possible version of the dataset (accounts for all future datasets), and unlimited computing power.

-   Formally private methods don't label particular variables as confidential/sensitive, but instead treats them all as sensitive due to the possibility of reidentifcation.

## Tiered Access

::: {.callout-tip}
## Tiered Access

**Tiered access** is a data governance model that provides different levels of access to data users based on their needs and disclosure risks of their research projects.
:::

- Tiered access can include:
    - Public-use data files
    - Synthetic public-use data files
    - Restricted-use data in online data enclaves
    - Restricted-use data in on-premise research centers
    - Formally private query systems
- The Urban Institute is working to develop public-use synthetic data sets and formally private validation servers as a model of tiered access. 

## Measuring Utility Metrics and Disclosure Risks

### Disclosure Risks

-   Generally there are 3 kinds of disclosure risk:

1.  **Identity disclosure risk:** occurs if the data intruder associates a known individual with a public data record (e.g., a record linkage attack or when a data adversary combines one or more external data sources to identify individuals in the public data).

![](www/images/identity_disclosure_risk.png){width="344"}

2.  **Attribute disclosure risk:** occurs if the data intruder determines new characteristics (or attributes) of an individual based on the information available through public data or statistics (e.g., if a dataset shows that all people age 50 or older in a city are on Medicaid, then the data adversary knows that any person in that city above age 50 is on Medicaid). This information is learned without identifying a specific individual in the data!

3.  **Inferential disclosure risk:** occurs if the data intruder predicts the value of some characteristic from an individual more accurately with the public data or statistic than would otherwise have been possible (e.g., if a public homeownership dataset reports a high correlation between the purchase price of a home and family income, a data adversary could infer another person’s income based on purchase price listed on Redfin or Zillow). 

-   Important note: acceptable disclosure risks are usually determined by law.

### Utility Measures

-   Generally there are 2 ways to measure utility of the data:

1.  **General Utility** (aka global utilty): measures the univariate and multivariate distributional similarity between the confidential data and the public data (e.g., sample means, sample variances, and the variance-covariance matrix).

2.  **Specific Utility** (aka outcome specific utility): measures the similarity of results for a specific analysis (or analyses) of the confidential and public data (e.g., comparing the coefficients in regression models).

-   Higher utility = higher accuracy and usefulness of the data, so this is a key part of selecting an appropriate SDC method.

## `r paste("Exercise", exercise_number)`

```{r}
#| echo: false

exercise_number <- exercise_number + 1
```

Let's say a researcher generates a synthetic version of a dataset on penguins species. The first 5 rows of the gold standard dataset looks like this:

```{r}
#| echo: false

set.seed(125)

ex_data <- penguins |> 
  select(species, bill_length_mm, sex) |> 
  slice_sample(n = 5) 

ex_data |> 
  create_table()
```

One of the metrics to assess data utility was the overall counts of penguin species across the synthetic and gold standard data, which look like this:

```{r}
#| echo: false

set.seed(124)

penguins |> 
  count(species) |> 
  rowwise() |> 
  mutate(`# synthetic data` = rnorm(n = 1, mean= n, sd = 10) |>
           round(digits = 0)) |> 
  rename(`# conf. data` = n) |> 
  create_table()
```

::: {.panel-tabset}

### <font color="#55b748">Question</font>

*Would the above counts be considered a global utility metric or a specific utility metric and why?*

### <font color="#55b748">Solution</font>

*Would the above counts be considered a global utility metric or a specific utility metric and why?*

It's likely a global utility metric because it evaluates the general statistical properties of the synthetic data compared to the confidential data. 

:::

## `r paste("Exercise", exercise_number)`

```{r}
#| echo: false

exercise_number <- exercise_number + 1
```

Researchers [@mayer2016evaluating] looked at telephone metadata, which included times, duration and outgoing numbers of telephone calls. They found that 1 of the records in the data placed frequent calls to a local firearm dealer that prominently advertises a specialty in the AR semiautomatic rifle platform. The participant also placed lengthy calls to the customer support hotline for a major firearm manufacturer which produces a popular AR line of rifles. Using publicly available data, they were able to confirm the participants identity and confirm that he owned an AR-15. 

::: {.panel-tabset}

### <font color="#55b748">Question</font>

*In this example what kinds of disclosures happened? (Hint: there were two!)*

### <font color="#55b748">Solution</font>

*In this example what kinds of disclosures happened? (Hint: there were two!)*

Identity disclosure and attribute disclosure

:::



## `r paste("Exercise", exercise_number)`

```{r}
#| echo: false

exercise_number <- exercise_number + 1
```

::: {.panel-tabset}

### <font color="#55b748">Question</font>

*Why is it difficult to release public data that both maintains data utility and preserves individual privacy?*


### <font color="#55b748">Answer</font>

*Why is it difficult to release public data that both maintains data utility and preserves individual privacy?*

There is a tradeoff between privacy and utility. Releasing a public dataset with no modifications would maximize utility, but provide no privacy protections. As privacy protections are applied, the utility of the dataset is reduced.

:::

## Suggested Reading - General Data Privacy

-   Matthews, G. J., & Harel, O. (2011). Data confidentiality: A review of methods for statistical disclosure limitation and methods for assessing privacy. Statistics Surveys, 5, 1-29. [link](https://projecteuclid.org/journals/statistics-surveys/volume-5/issue-none/Data-confidentiality--A-review-of-methods-for-statistical-disclosure/10.1214/11-SS074.pdf)

-   Bowen, CMK., (2021). "Personal Privacy and the Public Good: Balancing Data Privacy and Data Utility." Urban Institute. [link](https://www.urban.org/research/publication/personal-privacy-and-public-good-balancing-data-privacy-and-data-utility)

-   Benschop, T. and Welch, M. (n.d.) Statistical Disclosure Control for Microdata: A Practice Guide. Retrieved (insert date), from <https://sdcpractice.readthedocs.io/en/latest/>
