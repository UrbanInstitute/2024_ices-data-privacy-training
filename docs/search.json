[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Data Privacy and Data Synthesis Techniques",
    "section": "",
    "text": "Preface\nThis book contains materials for Introduction to Data Privacy and Data Synthesis Techniques at the 2024 International Conference on Establishment Statistics in Glasgow, Scotland."
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Introduction to Data Privacy and Data Synthesis Techniques",
    "section": "License",
    "text": "License\nThis website is free to use and licensed under the GNU AGPLv3 license. The license is available in the GitHub repository."
  },
  {
    "objectID": "a_intro.html#urban-institute",
    "href": "a_intro.html#urban-institute",
    "title": "Introduction and Setup",
    "section": "Urban Institute",
    "text": "Urban Institute\n\n\n\n\n\n\nNon-partisan and not-for-profit social and economic policy research institution headquartered in Washington, DC"
  },
  {
    "objectID": "a_intro.html#aaron-r.-williams",
    "href": "a_intro.html#aaron-r.-williams",
    "title": "Introduction and Setup",
    "section": "Aaron R. Williams",
    "text": "Aaron R. Williams\n\n\n\n\n\n\n\n\n\n\n\nRoles\n\nLead Data Scientist for Statistical Computing at the Urban Institute\nAdjunct Professor in the McCourt School of Public Policy at Georgetown University\nAmerican Statistical Association Traveling Course Instructor\n\n\n\nProjects\n\nSynthetic data generation (rstudio::conf(2022) talk about library(tidysynthesis))\nFormal privacy/differential privacy evaluation\n\nA Feasibility Study of Differentially Private Summary Statistics and Regression Analyses with Evaluations on Administrative and Survey Data (code) (JASA)\nBenchmarking DP Linear Regression Methods for Statistical Inference (Preprint)\n\nProjects that iterate with R Markdown/Quarto\n\nMobility Metrics data pages\nState Fiscal briefs\n\nManage the Urban Institute ggplot2 theme (Examples) (Code)\nUrban Institute R Users Group"
  },
  {
    "objectID": "a_intro.html#questions-for-you",
    "href": "a_intro.html#questions-for-you",
    "title": "Introduction and Setup",
    "section": "Questions for You",
    "text": "Questions for You\n\nWhat types of analyses do you develop?\nWhat is your programming experience?\nWhat are you most interested to learn today?"
  },
  {
    "objectID": "a_intro.html#course-structure",
    "href": "a_intro.html#course-structure",
    "title": "Introduction and Setup",
    "section": "Course Structure",
    "text": "Course Structure\n\nIntroduction to Data Privacy\nSynthetic Data\nFormal Privacy and Differential Privacy"
  },
  {
    "objectID": "01_introduction_to_data_privacy.html#why-is-data-privacy-important",
    "href": "01_introduction_to_data_privacy.html#why-is-data-privacy-important",
    "title": "1  Introduction to Data Privacy",
    "section": "1.1 Why is Data Privacy important?",
    "text": "1.1 Why is Data Privacy important?\n\nModern computing and technology have made it easy to collect and process large amounts of data.\nMalicious actors can use computing power and advanced techniques to reidentify individuals by linking supposedly anonymized records with public databases.\nThis kind of attack is called a “record linkage” attack. The following are some examples of famous record linkage attacks.\n\nIn 1997, MA Gov. Bill Weld announced the public release of insurance data for researchers. He assured the public that PII had been deleted. A few days later, Dr. Latanya Sweeney, then a MIT graduate student, mailed to Weld’s office his personal medical information. She purchased voter data and linked Weld’s birth date, gender, and zip code to his health records. And this was back in 1997, when computing power was minuscule, and social media didn’t exist!\nA study by Dr. Latanya Sweeney based on the 1990 Census (Sweeney 2000) found that the 87% of the US population had reported characteristics that likely made them unique based only on ZIP, gender, and date of birth.\n\nMalicious actors can also reconstruct databases or by reconstructing databases from statistics.\nReleasing granular data can be of immense value to researchers. For example, cell phone data are invaluable for emergency responses to natural disasters, and granular medical data will lead to better treatment and development of cures.\n\n\n\nMore granular data are also important for understanding equity, particularly for smaller populations and subgroups."
  },
  {
    "objectID": "01_introduction_to_data_privacy.html#what-is-data-privacy",
    "href": "01_introduction_to_data_privacy.html#what-is-data-privacy",
    "title": "1  Introduction to Data Privacy",
    "section": "1.2 What is Data Privacy?",
    "text": "1.2 What is Data Privacy?\n\n\n\n\n\n\nData Privacy\n\n\n\nData Privacy is the ability “to determine what information about ourselves we will share with others.” (Fellegi 1972).\n\n\n\nThere are at least three major threats to data privacy.\n\nHackers: adversaries who steal confidential information through unauthorized access.\nSnoopers: adversaries who reconstruct confidential information from data releases.\nHoarders: stewards who collect data but don’t release the data even if respondents want the information released.\n\nThere are differing notions of what should and shouldn’t be private, which may include being able to opt out of or opt into disclosure protections.\nData privacy is a broad topic, which includes data security, encryption, access to data, etc. We will not be covering privacy breaches from unauthorized access to a database (e.g., hackers).\nWe are instead focused on privacy preserving access to data.\nAlthough data privacy and data confidentiality are certainly related, they are different, and both play a role in limiting statistical disclosure risk.\n\n\n\n\n\n\n\nConfidentiality\n\n\n\nConfidentiality is “the agreement, explicit or implicit, between data subject and data collector regarding the extent to which access by others to personal information is allowed” (Fienberg and Jin 2018).\n\n\n\nThere is often a tension between privacy and data utility (or usefulness). This tension is referred to in the data privacy literature as the “privacy-utility trade-off.”\n\nFor example, some universities require students to install an app that tracks their movements on campus. This allows professors teaching large classes with 100+ students to know their students’ punctuality, tardiness, or class absences. This tracking can be invasive, especially for students who rarely leave campus except during holidays, because the university could track their movements outside of class. However, the tracking app could alert students about an active shooter on campus, identify safe buildings to seek refuge, and notify emergency contacts regarding the students’ safety.\n\n\n\n\n\n\n\n\nData utility, quality, accuracy, or usefulness\n\n\n\nData utility, quality, accuracy, or usefulness is how practically useful or accurate to the data are for research and analysis purposes.\n\n\n\nGenerally, higher utility = more privacy risks and vice versa.\nIn the data privacy ecosystem there are the following stakeholders:\n\n\n\n\n\n\n\nData users and practitioners\n\n\n\nData users and practitioners are individuals who consume the data, such as analysts, researchers, planners, and decision-makers.\n\n\n\n\n\n\n\n\nData privacy experts or researchers\n\n\n\nData privacy experts or researchers are individuals who specialize in developing data privacy and confidentiality methods.\n\n\n\n\n\n\n\n\nData curators, maintainers, or stewards\n\n\n\nData curators, maintainers, or stewards are individuals who posess the data and are responsible for its safekeeping.\n\n\n\n\n\n\n\n\nData intruders, attackers, or adversaries\n\n\n\nData intruders, attackers, or adversaries are individuals who try to gather sensitive information from the confidential data.\n\n\n\nIn addition, there are many versions of the data we should define:\n\n\n\n\n\n\n\nOriginal dataset:\n\n\n\nOriginal dataset is the uncleaned, unprotected version of the data.\nFor example, raw Decennial census microdata, which are never publicly released.\n\n\n\n\n\n\n\n\nConfidential or gold standard dataset\n\n\n\nConfidential or gold standard dataset is the cleaned version (meaning edited for inaccuracies or inconsistencies) of the data; often referred to as the gold standard or actual data for analysis.\nFor example, the Census Edited File that is the final confidential data for the 2020 Census. This dataset is never publicly released but may be made available to others who are sworn to protect confidentiality and who are provided access in a secure environment, such as a Federal Statistical Research Data Center.\n\n\n\n\n\n\n\n\nPublic dataset\n\n\n\nPublic dataset is the publicly released version of the confidential data.\nFor example, the US Census Bureau’s public tables and datasets."
  },
  {
    "objectID": "01_introduction_to_data_privacy.html#data-privacy-workflow",
    "href": "01_introduction_to_data_privacy.html#data-privacy-workflow",
    "title": "1  Introduction to Data Privacy",
    "section": "1.3 Data Privacy Workflow",
    "text": "1.3 Data Privacy Workflow\n\nData users have traditionally gained access to data via:\n\ndirect access to the confidential data if they are trusted users (e.g., obtaining Special Sworn Status to use the Federal Statistical Research Data Centers).\nAccess to public data or statistics, such as public microdata and summary tables, that the data curators and privacy experts produced with modification to protect confidentiality.\n\nThe latter is how most data users gain access to information from confidential data and what we will focus on for this course. To create public data or statistics, data curators rely on statistical disclosure control (SDC) or limitation (SDL) methods to preserve data confidentiality. The process of releasing this information publicly often involves the steps shown in Figure 1.1.\n\n\n\n\nFigure 1.1: Process for releasing information\n\n\n\nThere is potential for additional disclosure risks if Step 4 and Step 5 are repeated too many times. We can think of this like data leakage in a machine learning/predictive modeling framework."
  },
  {
    "objectID": "01_introduction_to_data_privacy.html#overview-of-sdc",
    "href": "01_introduction_to_data_privacy.html#overview-of-sdc",
    "title": "1  Introduction to Data Privacy",
    "section": "1.4 Overview of SDC",
    "text": "1.4 Overview of SDC\n\n\n\n\n\n\nStatistical Disclosure Control (SDC)\n\n\n\nStatistical Disclosure Control (SDC) or Statistical Disclosure Limitation (SDL) is a field of study that aims to develop methods for releasing high-quality data products while preserving data confidentiality as a means of maintaining privacy.\n\n\n\nSDC methods have existed within statistics and the social sciences since the mid-twentieth century.\nBelow is an opinionated, and incomplete, overview of various SDC methods. For this set of training sessions, we will focus in-depth on the methods in yellow.\n\n\n\nHere are a few traditional methods from the SDC literature. See Matthews and Harel (2011) for more information.\n\nSuppression: Not releasing data about certain subgroups or witholding information about certaint observations.\nSwapping: The exchange of sensitive values among sample units with similar characteristics.\nGeneralization: Aggregating variables into larger units (e.g., reporting state rather than zip code) or top/bottom coding (limiting values below/above a threshold to the threshold value).\nNoise Infusion: Adding random noise, often to continuous variables which can maintain univariate distributions.\nSampling: Only releasing a sample of individual records.\n\nThe problem with the above approaches is that it really limits the utility of the data.\n\nMitra and Reiter (2006) found that a 5 percent swapping of 2 identifying variables in the 1987 Survey of Youth in Custody invalidated statistical hypothesis tests in regression.\nTop/bottom coding eliminates information at the tails of the distributions, degrading analyses that depend on the entire distribution (Fuller 1993; Reiter, Wang, and Zhang, 2014).\n\nSynthetic data can help overcome some of these issues."
  },
  {
    "objectID": "01_introduction_to_data_privacy.html#formal-privacy",
    "href": "01_introduction_to_data_privacy.html#formal-privacy",
    "title": "1  Introduction to Data Privacy",
    "section": "1.5 Formal Privacy",
    "text": "1.5 Formal Privacy\n\nFormal privacy centers around a formal mathematical definition of privacy that measures how much privacy leakage occurs.\nA formally private method provides mathematically provable protection to respondents and allows policy makers to manage the trade-off between data accuracy and privacy protection through tunable parameters for multiple statistics.\nAny SDC method/algorithm can be formally private if it meets the specific mathematical definition.\nFormal privacy is a definition/threshold for methods to meet, not a method in and of itself.\nBelow in green are the SDC methods which have formally private versions.\n\n\n\nIn this course, we will cover 2 kinds of formal privacy definitions.\n\nDifferential Privacy (DP): A mathematical definition of what it means to have privacy. The definition has a parameter called epsilon that control how much privacy loss occurs.\nApproximate Differential Privacy: Similar to DP in the same framework, but it relaxes the definition.\n\n\n\n\n\n\n\n\nThreat model\n\n\n\nA threat model is a set of assumptions for how data intruders will attack the data, including computing power, access to external data sources, etc.\n\n\n\nA key difference between traditional SDC methods and formally private SDC methods is the threat models behind each approach.\nFormal privacy definitions assume the worst possible scenario, where an attacker has all the information but 1 record, has every possible version of the dataset (accounts for all future datasets), and unlimited computing power.\nFormally private methods don’t label particular variables as confidential/sensitive, but instead treats them all as sensitive due to the possibility of reidentifcation."
  },
  {
    "objectID": "01_introduction_to_data_privacy.html#measuring-utility-metrics-and-disclosure-risks",
    "href": "01_introduction_to_data_privacy.html#measuring-utility-metrics-and-disclosure-risks",
    "title": "1  Introduction to Data Privacy",
    "section": "1.6 Measuring Utility Metrics and Disclosure Risks",
    "text": "1.6 Measuring Utility Metrics and Disclosure Risks\n\n1.6.1 Disclosure Risks\n\nGenerally there are 3 kinds of disclosure risk:\n\n\nIdentity disclosure risk: occurs if the data intruder associates a known individual with a public data record (e.g., a record linkage attack or when a data adversary combines one or more external data sources to identify individuals in the public data).\n\n\n\nAttribute disclosure risk: occurs if the data intruder determines new characteristics (or attributes) of an individual based on the information available through public data or statistics (e.g., if a dataset shows that all people age 50 or older in a city are on Medicaid, then the data adversary knows that any person in that city above age 50 is on Medicaid). This information is learned without identifying a specific individual in the data!\nInferential disclosure risk: occurs if the data intruder predicts the value of some characteristic from an individual more accurately with the public data or statistic than would otherwise have been possible (e.g., if a public homeownership dataset reports a high correlation between the purchase price of a home and family income, a data adversary could infer another person’s income based on purchase price listed on Redfin or Zillow).\n\n\nImportant note: acceptable disclosure risks are usually determined by law.\n\n\n\n1.6.2 Utility Measures\n\nGenerally there are 2 ways to measure utility of the data:\n\n\nGeneral Utility (aka global utilty): measures the univariate and multivariate distributional similarity between the confidential data and the public data (e.g., sample means, sample variances, and the variance-covariance matrix).\nSpecific Utility (aka outcome specific utility): measures the similarity of results for a specific analysis (or analyses) of the confidential and public data (e.g., comparing the coefficients in regression models).\n\n\nHigher utility = higher accuracy and usefulness of the data, so this is a key part of selecting an appropriate SDC method."
  },
  {
    "objectID": "01_introduction_to_data_privacy.html#exercise-1",
    "href": "01_introduction_to_data_privacy.html#exercise-1",
    "title": "1  Introduction to Data Privacy",
    "section": "1.7 Exercise 1",
    "text": "1.7 Exercise 1\nLet’s say a researcher generates a synthetic version of a dataset on penguins species. The first 5 rows of the gold standard dataset looks like this:\n\n\n\n\n\n\n  \n    \n      \n    \n    \n    \n      species\n      bill_length_mm\n      sex\n    \n  \n  \n    Chinstrap\n51.3\nmale\n    Gentoo\n44.0\nfemale\n    Chinstrap\n51.4\nmale\n    Chinstrap\n45.4\nfemale\n    Adelie\n36.2\nfemale\n  \n  \n  \n\n\n\n\nOne of the metrics to assess data utility was the overall counts of penguin species across the synthetic and gold standard data, which look like this:\n\n\n\n\n\n\n  \n    \n      \n    \n    \n    \n      species\n      # conf. data\n      # synthetic data\n    \n  \n  \n    Adelie\n152\n138\n    Chinstrap\n68\n68\n    Gentoo\n124\n116\n  \n  \n  \n\n\n\n\n\nQuestionSolution\n\n\nWould the above counts be considered a global utility metric or a specific utility metric and why?\n\n\nWould the above counts be considered a global utility metric or a specific utility metric and why?\nIt’s likely a global utility metric because it evaluates the general statistical properties of the synthetic data compared to the confidential data."
  },
  {
    "objectID": "01_introduction_to_data_privacy.html#exercise-2",
    "href": "01_introduction_to_data_privacy.html#exercise-2",
    "title": "1  Introduction to Data Privacy",
    "section": "1.8 Exercise 2",
    "text": "1.8 Exercise 2\nResearchers (Mayer, Mutchler, and Mitchell 2016) looked at telephone metadata, which included times, duration and outgoing numbers of telephone calls. They found that 1 of the records in the data placed frequent calls to a local firearm dealer that prominently advertises a specialty in the AR semiautomatic rifle platform. The participant also placed lengthy calls to the customer support hotline for a major firearm manufacturer which produces a popular AR line of rifles. Using publicly available data, they were able to confirm the participants identity and confirm that he owned an AR-15.\n\nQuestionSolution\n\n\nIn this example what kinds of disclosures happened? (Hint: there were two!)\n\n\nIn this example what kinds of disclosures happened? (Hint: there were two!)\nIdentity disclosure and attribute disclosure"
  },
  {
    "objectID": "01_introduction_to_data_privacy.html#exercise-3",
    "href": "01_introduction_to_data_privacy.html#exercise-3",
    "title": "1  Introduction to Data Privacy",
    "section": "1.9 Exercise 3",
    "text": "1.9 Exercise 3\n\nQuestionAnswer\n\n\nWhy is it difficult to release public data that both maintains data utility and preserves individual privacy?\n\n\nWhy is it difficult to release public data that both maintains data utility and preserves individual privacy?\nThere is a tradeoff between privacy and utility. Releasing a public dataset with no modifications would maximize utility, but provide no privacy protections. As privacy protections are applied, the utility of the dataset is reduced."
  },
  {
    "objectID": "01_introduction_to_data_privacy.html#suggested-reading---general-data-privacy",
    "href": "01_introduction_to_data_privacy.html#suggested-reading---general-data-privacy",
    "title": "1  Introduction to Data Privacy",
    "section": "1.10 Suggested Reading - General Data Privacy",
    "text": "1.10 Suggested Reading - General Data Privacy\n\nMatthews, G. J., & Harel, O. (2011). Data confidentiality: A review of methods for statistical disclosure limitation and methods for assessing privacy. Statistics Surveys, 5, 1-29. link\nBowen, CMK., (2021). “Personal Privacy and the Public Good: Balancing Data Privacy and Data Utility.” Urban Institute. link\nBenschop, T. and Welch, M. (n.d.) Statistical Disclosure Control for Microdata: A Practice Guide. Retrieved (insert date), from https://sdcpractice.readthedocs.io/en/latest/\n\n\n\n\n\nFellegi, Ivan P. 1972. “On the Question of Statistical Confidentiality.” Journal of the American Statistical Association 67 (337): 7–18.\n\n\nFienberg, Stephen E, and Jiashun Jin. 2018. “Statistical Disclosure Limitation for~ Data~ Access.” In Encyclopedia of Database Systems (2nd Ed.).\n\n\nMatthews, Gregory J, and Ofer Harel. 2011. “Data Confidentiality: A Review of Methods for Statistical Disclosure Limitation and Methods for Assessing Privacy.” Statistics Surveys 5: 1–29.\n\n\nMayer, Jonathan, Patrick Mutchler, and John C Mitchell. 2016. “Evaluating the Privacy Properties of Telephone Metadata.” Proceedings of the National Academy of Sciences 113 (20): 5536–41.\n\n\nSweeney, Latanya. 2000. “Simple Demographics Often Identify People Uniquely.” Health (San Francisco) 671 (2000): 1–34."
  },
  {
    "objectID": "b_data-privacy-and-synthetic-data.html",
    "href": "b_data-privacy-and-synthetic-data.html",
    "title": "Data Privacy and Synthesis",
    "section": "",
    "text": "In this section we’ll cover the motivations for and fundamentals of data privacy, and the basics of data synthesis.\nBy the end, you will be able to articulate threats to data privacy, implement basic statistical disclosure control methods, and comprehend approaches to data synthesis."
  },
  {
    "objectID": "02_synthetic-data.html#exercise-1",
    "href": "02_synthetic-data.html#exercise-1",
    "title": "2  Synthetic Data",
    "section": "2.1 Exercise 1",
    "text": "2.1 Exercise 1\nConsider the penguins data from earlier.\n\n\n\n\n\n\n  \n    \n      \n    \n    \n    \n      species\n      bill_length_mm\n      sex\n    \n  \n  \n    Chinstrap\n51.3\nmale\n    Gentoo\n44.0\nfemale\n    Chinstrap\n51.4\nmale\n    Chinstrap\n45.4\nfemale\n    Adelie\n36.2\nfemale\n  \n  \n  \n\n\n\n\nLet’s say that researchers decide that the sex of the penguins in the data are not confidential, but the species and bill length are. So, they develop regression models that predict species conditional on sex and predict bill_length conditional on species and sex. They then use those models to predict species and bill lengths for each row in the data and then release it publicly.\n\nQuestionSolution\n\n\nWhat specific Statistical Disclosure Control method are these researchers using?\n\n\nWhat specific Statistical Disclosure Control method are these researchers using?\nThey are using partially synthetic data."
  },
  {
    "objectID": "02_synthetic-data.html#exercise-2",
    "href": "02_synthetic-data.html#exercise-2",
    "title": "2  Synthetic Data",
    "section": "2.2 Exercise 2",
    "text": "2.2 Exercise 2\n\nQuestionAnswer\n\n\nA researcher has confidential data on a population. To protect the privacy of respondents, the researcher releases a synthetic version of the data. A data attacker then runs a record linkage attack against the synthetic data and is able to accurately identify 5 individuals in the data. Based on this information, can you tell whether the researcher released fully or partially synthetic data? Why or why not?\n\n\nA researcher has confidential data on a population. To protect the privacy of respondents, the researcher releases a synthetic version of the data. A data attacker then runs a record linkage attack against the synthetic data and is able to accurately identify 5 individuals in the data. Based on this information, can you tell whether the researcher released fully or partially synthetic data? Why or why not?\nRecord linkage attacks are only possible for partially synthetic data, though other types of disclosure risk still apply to fully synthetic data."
  },
  {
    "objectID": "02_synthetic-data.html#synthetic-data---imputation-connection",
    "href": "02_synthetic-data.html#synthetic-data---imputation-connection",
    "title": "2  Synthetic Data",
    "section": "2.3 Synthetic Data <-> Imputation Connection",
    "text": "2.3 Synthetic Data &lt;-&gt; Imputation Connection\n\nMultiple imputation was originally developed to address non-response problems in surveys (Rubin 1977).\nStatisticians created new observations or values to replace the missing data by developing a model based on other available respondent information.\nThis process of replacing missing data with substituted values is called imputation.\n\n\n2.3.1 Imputation Example\nImagine you are running a conference with 80 attendees. You are collecting names and ages of all your attendees. Unfortunately, when the conference is over, you realize that only about half of the attendees listed their ages. One common imputation technique is to just replace the missing values with the mean age of those in the data.\nFigure 2.3 shows the distribution of the 40 age observations that are not missing.\n\n\n\n\n\nFigure 2.3: ?(caption)\n\n\n\n\nFigure 2.4 shows the histogram after imputation.\n\n\n\n\n\nFigure 2.4: ?(caption)\n\n\n\n\n\nUsing the mean to impute the missing ages removes useful variation and conceals information from the “tails” of the distribution.\nSimply put, we used a straightforward model (replace the data with the mean) and sampled from that model to fill in the missing values.\nWhen creating synthetic data, this process is repeated for an entire variable, or set of variables.\nIn a sense, the entire column is treated as missing!"
  },
  {
    "objectID": "02_synthetic-data.html#sequential-synthesis",
    "href": "02_synthetic-data.html#sequential-synthesis",
    "title": "2  Synthetic Data",
    "section": "2.4 Sequential Synthesis",
    "text": "2.4 Sequential Synthesis\nA more advanced implementation of synthetic data generation estimates models for each predictor with previously synthesized variables used as predictors. This iterative process is called sequential synthesis or fully conditional specification (FCS). This allows us to easily model multivariate relationships (or joint distributions) without being computationally expensive.\nThe process described above may be easier to understand with the following table:\n\n\n\n\n\n\n  \n    \n      \n    \n    \n    \n      Step\n      Outcome\n      Modelled with\n      Predicted with\n    \n  \n  \n    1\nSex\n—\nRandom sampling with replacement\n    2\nAge\nSex\nSampled Sex\n    3\nSocial Security Benefits\nSex, Age\nSampled Sex, Sampled Age\n    —\n—\n—\n—\n  \n  \n  \n\n\n\n\n\nWe can select the synthesis order based on the priority of the variables or the relationships between them.\nThe earlier in the order a variable is synthesized, the better the original information is preserved in the synthetic data usually.\n(Bowen, Liu, and Su 2021) proposed a method that ranks variable importance by either practical or statistical utility and sequentially synthesizes the data accordingly."
  },
  {
    "objectID": "02_synthetic-data.html#parametric-vs.-nonparametric-data-generation-process",
    "href": "02_synthetic-data.html#parametric-vs.-nonparametric-data-generation-process",
    "title": "2  Synthetic Data",
    "section": "2.5 Parametric vs. Nonparametric Data Generation Process",
    "text": "2.5 Parametric vs. Nonparametric Data Generation Process\nParametric data synthesis is the process of data generation based on a parametric distribution or generative model.\n\nParametric models assume a finite number of parameters that capture the complexity of the data.\nThey are generally less flexible, but more interpretable than nonparametric models.\nExamples: regression to assign an age variable, sampling from a probability distribution, Bayesian models, or copula based models.\n\nNonparametric data synthesis is the process of data generation that is not based on assumptions about an underlying distribution or model.\n\nOften, nonparametric methods use frequency proportions or marginal probabilities as weights for some type of sampling scheme.\nThey are generally more flexible, but less interpretable than parametric models.\nExamples: assigning gender based on underlying proportions, CART (Classification and Regression Trees) models, RNN models, etc.\n\nImportant: Synthetic data are only as good as the models used for imputation!"
  },
  {
    "objectID": "02_synthetic-data.html#implicates",
    "href": "02_synthetic-data.html#implicates",
    "title": "2  Synthetic Data",
    "section": "2.6 Implicates",
    "text": "2.6 Implicates\n\nResearchers can create any number of versions of a partially synthetic or fully synthetic dataset. Each version of the dataset is called an implicate. These can also be referred to as replicates or simply “synthetic datasets”\n\nFor partially synthetic data, non-synthesized variables are the same across each version of the dataset.\n\nMultiple implicates are useful for understanding the uncertainty added by imputation and are required for calculating valid standard errors.\nMore than one implicate can be released for public use; each new release, however, increases disclosure risk (but allows for more complete analysis and better inferences, provided users use the correct combining rules).\nImplicates can also be analyzed internally to find which version(s) of the dataset provide the most utility in terms of data quality."
  },
  {
    "objectID": "02_synthetic-data.html#exercise-3-sequential-synthesis",
    "href": "02_synthetic-data.html#exercise-3-sequential-synthesis",
    "title": "2  Synthetic Data",
    "section": "2.7 Exercise 3: Sequential Synthesis",
    "text": "2.7 Exercise 3: Sequential Synthesis\n\nQuestionAnswer\n\n\nYou have a confidential dataset that contains information about dogs’ weight and their height. You decide to sequentially synthesize these two variables and write up your method below. Can you spot the mistake in writing up your method?\n\nTo create a synthetic record, first synthetic pet weight is assigned based on a random draw from a normal distribution with mean equal to the average of confidential weights, and standard deviation equal to the standard deviation of confidential weights. Then the confidential height is regressed on the synthetic weight. Using the resulting regression coefficients, a synthetic height variable is generated for each row in the data using just the synthetic weight values as an input.\n\n\n\nYou have a confidential dataset that contains information about dogs’ weight and their height. You decide to sequentially synthesize these two variables and write up your method below. Can you spot the mistake in writing up your method?\n\nTo create a synthetic record, first synthetic pet weight is assigned based on a random draw from a normal distribution with mean equal to the average of confidential weights, and standard deviation equal to the standard deviation of confidential weights. Then the confidential height is regressed on the synthetic weight. Using the resulting regression coefficients, a synthetic height variable is generated for each row in the data using just the synthetic weight values as an input.\n\nHeight should be regressed on the confidential values for weight, rather than the synthetic values for weight"
  },
  {
    "objectID": "02_synthetic-data.html#exercise-4-multiple-implicates",
    "href": "02_synthetic-data.html#exercise-4-multiple-implicates",
    "title": "2  Synthetic Data",
    "section": "2.8 Exercise 4: Multiple Implicates",
    "text": "2.8 Exercise 4: Multiple Implicates\n\nQuestionNotes\n\n\nWhat are the privacy implications for releasing multiple versions of a synthetic dataset (implicates)? Do these implications change for partially vs. fully synthetic data?\n\n\nWhat are the privacy implications for releasing multiple versions of a synthetic dataset (implicates)? Do these implications change for partially vs. fully synthetic data?\n\nReleasing multiple implicates improves transparency and analytical value, but increases disclosure risk (violates “security through obscurity”).\nIt is more risky to release partially synthetic implicates, since non-synthesized records are the same across each dataset and there remains a 1-to-1 relationship between confidential and synthesized records."
  },
  {
    "objectID": "02_synthetic-data.html#exercise-5-partial-vs.-fully-synthetic",
    "href": "02_synthetic-data.html#exercise-5-partial-vs.-fully-synthetic",
    "title": "2  Synthetic Data",
    "section": "2.9 Exercise 5: Partial vs. fully synthetic",
    "text": "2.9 Exercise 5: Partial vs. fully synthetic\nShown here are the first seven rows of a dataset about the prices and attributes of diamonds. Suppose you decide to synthesize the “price” variable, because that information is too sensitive for public release.\n\n\n\nprice\ncarat\ncut\ncolor\nclarity\n\n\n\n\n326\n0.23\nIdeal\nE\nSI2\n\n\n326\n0.21\nPremium\nE\nSI1\n\n\n327\n0.23\nGood\nE\nVS1\n\n\n334\n0.29\nPremium\nI\nVS2\n\n\n335\n0.31\nGood\nJ\nSI2\n\n\n336\n0.24\nVery Good\nJ\nVVS2\n\n\n336\n0.24\nVery Good\nI\nVVS1\n\n\n\n\nQuestionNotes\n\n\nAfter you synthesize the price variable, would the resulting dataset be considered partially or fully synthetic?\nWhat are the trade-offs of a partially synthetic dataset compared to a fully synthetic dataset?\nDescribe in words how you would synthesize the “price” variable. Is the method you described parametric or non-parametric? Why?\n\n\nAfter you synthesize the price variable, would the resulting dataset be considered partially or fully synthetic?\nPartially synthetic\nWhat are the trade-offs of a partially synthetic dataset compared to a fully synthetic dataset?\n\nChanging only some variables (partial synthesis) in general leads to higher utility in analysis since the relationships between variables are by definition unchanged (Drechsler et al, 2008).\nDisclosure in fully synthetic data is challenging because all values are imputed, while partial synthesis has higher disclosure risk since confidential values remain in the dataset (Drechsler et al, 2008).\n\nNote that while the risk of disclosure for fully synthetic data is very low, it is not zero.\n\nAccurate and exhaustive specification of variable relationships and constraints in fully synthetic data is difficult and if done incorrectly can lead to bias (drechslerjorgcomparingsynthetic?).\n\nIf a variable is synthesized incorrectly early in a sequential synthesis, all variables synthesized on the basis of that variable will be affected.\n\nPartially synthetic data may be publicly perceived as more reliable than fully synthetic data.\n\nDescribe in words how you would synthesize the “price” variable. Is the method you described parametric or non-parametric? Why?"
  },
  {
    "objectID": "02_synthetic-data.html#exercise-6",
    "href": "02_synthetic-data.html#exercise-6",
    "title": "2  Synthetic Data",
    "section": "2.10 Exercise 6",
    "text": "2.10 Exercise 6\nFor this exercise, we will use the starwars dataset from the dplyr package. We will practice sequentially synthesizing a binary variable (gender) and a numeric variable (height).\n\n# run this to get the dataset we will work with\nstarwars &lt;- dplyr::starwars |&gt;\n  select(gender, height) |&gt;\n  drop_na()\n\nstarwars |&gt; \n  head() |&gt; \n  create_table()\n\n\n\n\n\n  \n    \n      \n    \n    \n    \n      gender\n      height\n    \n  \n  \n    masculine\n172\n    masculine\n167\n    masculine\n96\n    masculine\n202\n    feminine\n150\n    masculine\n178\n  \n  \n  \n\n\n\n\nPart 1: Gender synthesis\n\nTemplateSolutions\n\n\nFill in the blanks in the following code to synthesize the gender variable using the underlying distribution present in the data.\n\n# set a seed so pseudo-random processes are reproducible\nset.seed(20220301)\n\n# Fill in the blanks!\n\n# vector of gender categories\ngender_categories &lt;- c(\"feminine\", \"masculine\")\n\n# size of sample to generate\nsynthetic_data_size &lt;- nrow(starwars)\n\n# probability weights\ngender_probs &lt;- starwars |&gt;\n  count(gender) |&gt;\n  mutate(relative_frequency = ### ______) |&gt;\n  pull(relative_frequency)\n\n# use sample function to generate synthetic vector of genders\ngender_synthetic &lt;- sample(\n    x = ###_____, \n    size = ###_____,\n    replace = ###_____, \n    prob = ###_____\n)\n                          \n# create starwars_synthetic dataset using generated variable\nstarwars_synthetic &lt;- tibble(\n  gender = gender_synthetic\n)\n\nPart 2: Height synthesis\nSimilarly, fill in the blanks in the code to generate the height variable using a linear regression with gender as a predictor.\n\n# set a seed so pseudo-random processes are reproducible\nset.seed(20220301)\n\n# Fill in the blanks!\n\n# linear regression\nheight_lm &lt;- lm(\n  formula = ###_____,\n  data = ###______\n)\n\n# predict flipper length with model coefficients\nheight_predicted &lt;- predict(\n  object = height_lm, \n  newdata = ###_____\n)\n\n# synthetic column using normal distribution centered on predictions with sd of residual standard error\nheight_synthetic &lt;- rnorm(\n  n = ###_______, \n  mean = ###______, \n  sd = ###______\n)\n\n# add new values to synthetic data as height\nstarwars_synthetic &lt;- starwars_synthetic |&gt;\n  mutate(height = height_synthetic)\n\n\n\nFill in the blanks in the following code to synthesize the gender variable using the underlying distribution present in the data.\n\n# set a seed so pseudo-random processes are reproducible\nset.seed(20220301)\n\n# Fill in the blanks!\n\n# vector of gender categories\ngender_categories &lt;- c(\"feminine\", \"masculine\")\n\n# size of sample to generate\nsynthetic_data_size &lt;- nrow(starwars)\n\n# probability weights\ngender_probs &lt;- starwars |&gt;\n  count(gender) |&gt;\n  mutate(relative_frequency = n / synthetic_data_size) |&gt;\n  pull(relative_frequency)\n\n# use sample function to generate synthetic vector of genders\ngender_synthetic &lt;- sample(\n    x = gender_categories, \n    size = synthetic_data_size,\n    replace = TRUE, \n    prob = gender_probs\n)\n                          \n# create starwars_synthetic dataset using generated variable\nstarwars_synthetic &lt;- tibble(\n  gender = gender_synthetic\n)\n\nPart 2: Height synthesis\nSimilarly, fill in the blanks in the code to generate the height variable using a linear regression with gender as a predictor.\n\n# set a seed so pseudo-random processes are reproducible\nset.seed(20220301)\n\n# Fill in the blanks!\n\n# linear regression\nheight_lm &lt;- lm(\n  formula = height ~ gender,\n  data = starwars\n)\n\n# predict flipper length with model coefficients\nheight_predicted &lt;- predict(\n  object = height_lm, \n  newdata = starwars_synthetic\n)\n\n# synthetic column using normal distribution centered on predictions with sd of residual standard error\nheight_synthetic &lt;- rnorm(\n  n = synthetic_data_size, \n  mean = height_predicted, \n  sd = sigma(height_lm)\n)\n\n# add new values to synthetic data as height\nstarwars_synthetic &lt;- starwars_synthetic |&gt;\n  mutate(height = height_synthetic)"
  },
  {
    "objectID": "02_synthetic-data.html#suggested-reading---synthetic-data",
    "href": "02_synthetic-data.html#suggested-reading---synthetic-data",
    "title": "2  Synthetic Data",
    "section": "2.11 Suggested Reading - Synthetic Data",
    "text": "2.11 Suggested Reading - Synthetic Data\nSnoke, J., Raab, G. M., Nowok, B., Dibben, C., & Slavkovic, A. (2018). General and specific utility measures for synthetic data. Journal of the Royal Statistical Society: Series A (Statistics in Society), 181(3), 663-688. link\nBowen, C. M., Bryant, V., Burman, L., Czajka, J., Khitatrakun, S., MacDonald, G., … & Zwiefel, N. (2022). Synthetic Individual Income Tax Data: Methodology, Utility, and Privacy Implications. In International Conference on Privacy in Statistical Databases (pp. 191-204). Springer, Cham. link\nRaghunathan, T. E. (2021). Synthetic data. Annual Review of Statistics and Its Application, 8, 129-140. link\n\n\n\n\nBowen, Claire McKay, Fang Liu, and Bingyue Su. 2021. “Differentially Private Data Release via Statistical Election to Partition Sequentially.” Metron 79 (1): 1–31.\n\n\nRubin, Donald B. 1977. “Formalizing Subjective Notions about the Effect of Nonrespondents in Sample Surveys.” Journal of the American Statistical Association 72 (359): 538–43."
  },
  {
    "objectID": "03_disclosure_utility_metrics.html#review",
    "href": "03_disclosure_utility_metrics.html#review",
    "title": "3  Utility and Disclosure Risk Metrics",
    "section": "3.1 Review",
    "text": "3.1 Review\n\nQuestion 1Question 1 Notes\n\n\nWhat’s the difference between partially synthetic data and fully synthetic data?\n\n\nWhat’s the difference between partially synthetic data and fully synthetic data?\nPartially synthetic data contains unaltered and synthesized variables. In partially synthetic data, there remains a one-to-one mapping between confidential records and synthetic records.\nFully synthetic data only contains synthesized variables. Fully synthetic data no longer directly map onto the confidential records, but remain statistically representative.\n\n\n\n\n\n\n\n\n\nSequential synthesis\n\n\n\nIn a perfect world, we would synthesize data by directly modeling the joint distribution of the variables of interest. Unfortunately, this is often computationally infeasible.\nInstead, we often decompose a joint distribution into a sequence of conditional distributions.\n\n\n\nQuestion 2Question 2 Notes\n\n\nWhat’s the difference between specific utility and general utility?\n\n\nWhat’s the difference between specific utility and general utility?\nSpecific Utility measures the similarity of results for a specific analysis (or analyses) of the confidential and altered data (e.g., comparing the coefficients in regression models).\nGeneral Utility measures the univariate and multivariate distributional similarity between the confidential data and the altered data (e.g., sample means, sample variances, and the variance-covariance matrix)."
  },
  {
    "objectID": "03_disclosure_utility_metrics.html#general-utility-metrics",
    "href": "03_disclosure_utility_metrics.html#general-utility-metrics",
    "title": "3  Utility and Disclosure Risk Metrics",
    "section": "3.2 General Utility Metrics",
    "text": "3.2 General Utility Metrics\n\nAs a refresher, general utility metrics measure the distributional similarity (i.e., all statistical properties) between the original and synthetic data.\nGeneral utility metrics are useful because they provide a sense of how “fit for use” synthetic data is for analysis without making assumptions about the uses of the synthetic data."
  },
  {
    "objectID": "03_disclosure_utility_metrics.html#univariate",
    "href": "03_disclosure_utility_metrics.html#univariate",
    "title": "3  Utility and Disclosure Risk Metrics",
    "section": "3.3 Univariate",
    "text": "3.3 Univariate\n\nCategorical variables: frequencies, relative frequencies\nNumeric variables means, standard deviations, skewness, kurtosis (i.e., first four moments), percentiles, and number of zero/non-zero values\n\n\n\n\nIt is also useful to visually compare univariate distributions using histograms (Figure 3.1), density plots (Figure 3.2), and empirical cumulative distribution function plots (Figure 3.3).\n\n\ncompare_penguins |&gt;\n  select(\n    data_source, \n    bill_length_mm, \n    flipper_length_mm\n  ) |&gt;\n  pivot_longer(-data_source, names_to = \"variable\") |&gt;\n  ggplot(aes(x = value, fill = data_source)) +\n  geom_histogram(alpha = 0.3, color = NA, position = \"identity\") +\n  facet_wrap(~ variable, scales = \"free\") +\n  scatter_grid()\n\n\n\nFigure 3.1: Compare Synthetic and Confidential Distributions with Histograms\n\n\n\n\n\n\ncompare_penguins |&gt;\n  select(\n    data_source, \n    bill_length_mm, \n    flipper_length_mm\n  ) |&gt;\n  pivot_longer(-data_source, names_to = \"variable\") |&gt;\n  ggplot(aes(x = value, fill = data_source)) +\n  geom_density(alpha = 0.3, color = NA) +\n  facet_wrap(~variable, scales = \"free\") +\n  scatter_grid()\n\n\n\nFigure 3.2: Compare Synthetic and Confidential Distributions with Density Plots\n\n\n\n\n\n\ncompare_penguins |&gt;\n  select(\n    data_source, \n    bill_length_mm, \n    flipper_length_mm\n  ) |&gt;\n  pivot_longer(-data_source, names_to = \"variable\") |&gt;\n  ggplot(aes(x = value, color = data_source)) +\n  stat_ecdf() +\n  facet_wrap(~ variable, scales = \"free\") +\n  scatter_grid()\n\n\n\nFigure 3.3: Compare Synthetic and Confidential Distributions with Empirical CDF Plots"
  },
  {
    "objectID": "03_disclosure_utility_metrics.html#bivariate",
    "href": "03_disclosure_utility_metrics.html#bivariate",
    "title": "3  Utility and Disclosure Risk Metrics",
    "section": "3.4 Bivariate",
    "text": "3.4 Bivariate\n\n\n\n\n\n\nCorrelation Fit\n\n\n\nCorrelation fit measures how well the synthesizer recreates the linear relationships between variables in the confidential dataset.\n\n\n\nCreate correlation matrices for the synthetic data and confidential data. Then measure differences across synthetic and actual data. Those differences are often summarized across all variables using L1 or L2 distance.\n\n\n\nFigure 3.4: Correlation Difference\n\n\n\n\nFigure 3.4 shows the creation of a difference matrix. Let’s summarize the difference matrix using mean absolute error. This will give us a sense of how off the average correlation will be in the synthetic data compared to the confidential data.\n\n\\[MAE_{dist} = \\frac{1}{n}\\sum_{i = 1}^n |dist|\\]\n\\[MAE_{dist} = \\frac{1}{6} \\left(|-0.15| + |0.01| + |0.1| + |-0.15| + |0.15| + |0.02|\\right) \\approx 0.0966667\\]\n\nAdvanced measures like relative mutual information can be used to measure the relationships between categorical variables."
  },
  {
    "objectID": "03_disclosure_utility_metrics.html#multivariate",
    "href": "03_disclosure_utility_metrics.html#multivariate",
    "title": "3  Utility and Disclosure Risk Metrics",
    "section": "3.5 Multivariate",
    "text": "3.5 Multivariate\n\n\n\n\n\n\nDiscriminant Based Methods\n\n\n\nDiscriminant based methods measure well a predictive model can distinguish (i.e., discriminate) between records from the confidential and synthetic data.\n\n\n\nThe confidential data and synthetic data should theoretically be drawn from the same super population.\nThe basic idea is to combine (stack) the confidential data and synthetic data and see how well a predictive model distinguish (i.e., discriminate) between synthetic observations and confidential observations.\nAn inability to distinguish between the records suggests a good synthesis.\nIt is possible to use logistic regression for the predictive modeling, but decision trees, random forests, and boosted trees are more common.\nFigure 3.5 shows three discriminant based metrics calculated on a good synthesis and a poor synthesis.\n\n\nFigure 3.5: A comparison of discriminant metrics on a good synthesis and a poor synthesis\n\n\n\n\n\nGood Synthesis\n\n\n\n\n\n\n\n\n\nPoor Synthesis\n\n\n\n\n\n\n\n\n3.5.1 Calculating Discriminant Metrics\n\npMSE ratio, SPECKS, and AUC all require calculating propensity scores (i.e., the probability that a particular data point belongs to the confidential data) and start with the same step.\n\n\nCombine the synthetic and confidential data. Add an indicator variable with 0 for the confidential data and 1 for the synthetic data\n\n\n\n\n\n\n\n  \n    \n      \n    \n    \n    \n      species\n      bill_length_mm\n      sex\n      ind\n    \n  \n  \n    Chinstrap\n49.5\nmale\n0\n    ...\n...\n...\n...\n    Adelie\n46.0\nmale\n1\n  \n  \n  \n\n\n\n\n\nCalculate propensity scores (i.e., probabilities for group membership) for whether a given row belong to the synthetic dataset.\n\n\n\n\n\n\n\n  \n    \n      \n    \n    \n    \n      species\n      bill_length_mm\n      sex\n      ind\n      prop_score\n    \n  \n  \n    Chinstrap\n49.5\nmale\n0\n0.32\n    ...\n...\n...\n...\n...\n    Adelie\n46.0\nmale\n1\n0.64\n  \n  \n  \n\n\n\n\n\npMSESPECKSROC Curves/AUC\n\n\n\npMSE: Calculates the average Mean Squared Error (MSE) between the propensity scores and the expected probabilities:\nProposed by Woo et al. (Woo et al. 2009) and enhanced by Snoke et al. (Snoke et al. 2018a)\nAfter doing steps 1) and 2) above:\n\nCalculate expected probability, i.e., the share of synthetic data in the combined data. In the cases where the synthetic and confidential datasets are of equal size, this will always be 0.5.\n\n\n\n\n\n\n  \n    \n      \n    \n\n    \n      species\n      bill_length_mm\n      sex\n      ind\n      prop_score\n      exp_prob\n    \n  \n  \n    Chinstrap\n49.5\nmale\n0\n0.32\n0.5\n    ...\n...\n...\n...\n...\n...\n    Adelie\n46.0\nmale\n1\n0.64\n0.5\n  \n\n\n\n\n\n\nCalculate pMSE, which is mean squared difference between the propensity scores and expected probabilities.\n\n\\[pMSE = \\frac{(0.32 - 0.5)^2 + ... + (0.64-0.5)^2}{N} \\]\nOften people use the pMSE ratio, which is the average pMSE score across all records, divided by the null model (Snoke et al. 2018b).\nThe null model is the the expected value of the pMSE score under the best case scenario when the model used to generate the data reflects the confidential data perfectly.\npMSE ratio = 1 means that your synthetic data and confidential data are indistinguishable, although values this low are almost never achieved.\n\n\n\n\nSPECKS: Synthetic data generation; Propensity score matching; Empirical Comparison via the Kolmogorov-Smirnov distance.\n\nAfter generating propensity scores (i.e., steps 1 and 2 from above), you:\n\nCalculate the empirical CDF’s of the propensity scores for the synthetic and confidential data, separately.\nCalculate the Kolmogorov-Smirnov (KS) distance between the 2 empirical CDFs. The KS distance is the maximum vertical distance between 2 empirical CDF distributions.\n\n\n\n\n\nReceiver Operating Characteristic (ROC) curves show the trade off between false positives and true positives. Area under the curve (AUC) is a single number summary of the ROC curve.\n\nAUC is a common tool for evaluating classification models. High values for AUC are bad because they suggest the model can distinguish between confidential and synthetic observations.\nAfter generating propensity scores (i.e., steps 1 and 2 from above),\n\n\nIn our context, High AUC = good at discriminating = poor synthesis.\nWe want in the best case, AUC = 0.5 because that means the discriminator is no better than a random guess\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nMany predictive models for generating propensities can memorize chance features of the data used to fit the models. We suggest using a training/testing split and v-fold cross validations for hyperparameter tuning to compare in-sample and out-of-sample propensities and model accuracy.\n\n\n\nLook at Figure 3.5 to see calculations for pMSE ratio, SPECKS, and AUC.\nIt is useful to look at variable importance for predictive models when observing poor discriminant based metrics. Variable importance can help diagnose which variables are poorly synthesized."
  },
  {
    "objectID": "03_disclosure_utility_metrics.html#exercise-1-using-utility-metrics",
    "href": "03_disclosure_utility_metrics.html#exercise-1-using-utility-metrics",
    "title": "3  Utility and Disclosure Risk Metrics",
    "section": "3.6 Exercise 1: Using Utility Metrics",
    "text": "3.6 Exercise 1: Using Utility Metrics\n\nQuestionNotes\n\n\nConsider the following two syntheses of x.\nWhich synthesis do you think is better?\n\n\n\n\n\nBoth syntheses have issues? What do you think are the issues?\n\n\nConsider the following two syntheses of x. Which synthesis do you think is better?\n\nset.seed(20230710)\nbind_rows(\n  synth1 = tibble(\n    x_conf = rnorm(n = 1000),\n    x_synth = rnorm(n = 1000, mean = 0.2)\n  ),\n  synth2 = tibble(\n    x_conf = rnorm(n = 1000),\n    x_synth = rnorm(n = 1000, sd = 0.5)\n  ),\n  .id = \"synthesis\"\n) |&gt;\n  pivot_longer(-synthesis, names_to = \"variable\") |&gt;\n  ggplot(aes(x = value, color = variable)) +\n  stat_ecdf() +\n  facet_wrap(~ synthesis) +\n  scatter_grid()\n\n\n\n\nBoth syntheses have issues? What do you think are the issues?\n\nWe consider synth1 to be slightly better than synth2 based on the large vertical distances between the lines for synth2.\nsynth1 looks to match the variance of the confidential data but the mean is a little too high. synth2 matches the mean, but it contains far too little variance. There aren’t enough observations in the tails of the synthetic data."
  },
  {
    "objectID": "03_disclosure_utility_metrics.html#exercise-2-correlation-difference",
    "href": "03_disclosure_utility_metrics.html#exercise-2-correlation-difference",
    "title": "3  Utility and Disclosure Risk Metrics",
    "section": "3.7 Exercise 2: Correlation Difference",
    "text": "3.7 Exercise 2: Correlation Difference\n\nQuestionAnswer\n\n\nConsider the following correlation matrices:\n\n\n[1] \"Synthetic\"\n\n\n     [,1] [,2] [,3]\n[1,] 1.00  0.5 0.75\n[2,] 0.50  1.0 0.80\n[3,] 0.75  0.8 1.00\n\n\n[1] \"Confidential\"\n\n\n     [,1] [,2] [,3]\n[1,] 1.00 0.35  0.1\n[2,] 0.35 1.00  0.9\n[3,] 0.10 0.90  1.0\n\n\n\nConstruct the difference matrix\nCalculate MAE\nOptional: Calculate RMSE\nOptional: What is the main difference between MAE and RMSE?\n\n\n\n\n\n[1] \"Synthetic\"\n\n\n     [,1] [,2] [,3]\n[1,] 1.00  0.5 0.75\n[2,] 0.50  1.0 0.80\n[3,] 0.75  0.8 1.00\n\n\n[1] \"Confidential\"\n\n\n     [,1] [,2] [,3]\n[1,] 1.00 0.35  0.1\n[2,] 0.35 1.00  0.9\n[3,] 0.10 0.90  1.0\n\n\n\nConstruct the difference matrix\n\n\ndiff &lt;- mat_synth - mat_conf\n\ndiff[!lower.tri(diff)] &lt;- NA\n\ndiff\n\n     [,1] [,2] [,3]\n[1,]   NA   NA   NA\n[2,] 0.15   NA   NA\n[3,] 0.65 -0.1   NA\n\n\n\nCalculate MAE\n\n\nmean(abs(diff[lower.tri(diff)]))\n\n[1] 0.3\n\n\n\nOptional: Calculate RMSE\n\n\nsqrt(mean(diff[lower.tri(diff)] ^ 2))\n\n[1] 0.389444\n\n\n\nOptional: What is the main difference between MAE and RMSE?\n\nRMSE gives extra weight to large errors because it squares values instead of using absolute values. We like to think of this as the difference between the mean and the median error."
  },
  {
    "objectID": "03_disclosure_utility_metrics.html#exercise-3-correlation-difference",
    "href": "03_disclosure_utility_metrics.html#exercise-3-correlation-difference",
    "title": "3  Utility and Disclosure Risk Metrics",
    "section": "3.8 Exercise 3: Correlation Difference",
    "text": "3.8 Exercise 3: Correlation Difference\n\nQuestionSolution\n\n\n\npenguins_conf &lt;- read_csv(here::here(\"data\", \"penguins_synthetic_and_confidential.csv\")) |&gt;\n  filter(data_source == \"confidential\")\n\npenguins_synth &lt;- read_csv(here::here(\"data\", \"penguins_synthetic_and_confidential.csv\")) |&gt;\n  filter(data_source == \"synthetic\")\n# Fill in the blanks below:\n\n# The cor() function can take in a dataframe and compute correlations \n# between all columns in the dataframe and spit out a correlation matrix\nconf_data_corr &lt;- cor(###)\nsynth_data_corr &lt;- cor(###)\n\nconf_data_corr &lt;- conf_data_corr[lower.tri(conf_data_corr)]\nsynth_data_corr &lt;- synth_data_corr[lower.tri(synth_data_corr)]\n  \ncorrelation_diff &lt;- conf_data_corr - synth_data_corr\n\n# Correlation fit is the sum of the sqrt of the squared differences between each correlation in the difference matrix.\ncor_fit &lt;- sum(sqrt( ### ^2))\n\ncor_fit\n\n\n\n\npenguins_conf &lt;- read_csv(here::here(\"data\", \"penguins_synthetic_and_confidential.csv\")) |&gt;\n  filter(data_source == \"confidential\")\n\npenguins_synth &lt;- read_csv(here::here(\"data\", \"penguins_synthetic_and_confidential.csv\")) |&gt;\n  filter(data_source == \"synthetic\")\n\n# The cor() function can take in a dataframe and compute correlations \n# between all columns in the dataframe and spit out a correlation matrix\nconf_data_corr &lt;- cor(select(penguins_conf, where(is.numeric)))\nsynth_data_corr &lt;- cor(select(penguins_synth, where(is.numeric)))\n\nconf_data_corr &lt;- conf_data_corr[lower.tri(conf_data_corr)]\nsynth_data_corr &lt;- synth_data_corr[lower.tri(synth_data_corr)]\n  \ncorrelation_diff &lt;- conf_data_corr - synth_data_corr\n\n# Correlation fit is the sum of the sqrt of the squared differences between each correlation in the difference matrix.\ncor_fit &lt;- sum(sqrt(correlation_diff ^2))\n\ncor_fit\n\n[1] 0.6178178\n\n\n\n\n\n\nQuestionSolution\n\n\n\nconf_data &lt;- read_csv(here::here(\"data/lesson_03_conf_data.csv\"))\nsynth_data &lt;- read_csv(here::here(\"data/lesson_03_synth_data.csv\"))\n\ncombined_data &lt;- bind_rows(\n  \"synthetic\" = synth_data, \n  \"confidential\" = conf_data,\n  .id = \"type\"\n)\n\n# Create a density plot of the mass distributions\ncombined_data %&gt;% \n  ggplot(aes(x = ###,\n             fill = type,),\n         position = \"dodge\",\n         color = \"white\") +\n  geom_density(alpha = 0.4)\n\n# Create a density plot of the height distributions\ncombined_data %&gt;% \n  ggplot(aes(x = ###,\n             fill = type,),\n         position = \"dodge\",\n         color = \"white\") +\n  geom_density(alpha = 0.4)\n\n\n\n\nconf_data &lt;- read_csv(here::here(\"data/lesson_03_conf_data.csv\"))\nsynth_data &lt;- read_csv(here::here(\"data/lesson_03_synth_data.csv\"))\n\ncombined_data &lt;- bind_rows(\n  \"synthetic\" = synth_data, \n  \"confidential\" = conf_data,\n  .id = \"type\"\n)\n\n# Create a density plot of the mass distributions\ncombined_data %&gt;% \n  ggplot(aes(x = mass,\n             fill = type),\n         position = \"dodge\",\n         color = \"white\") +\n  geom_density(alpha = 0.4)\n\n\n\n# Create a density plot of the height distributions\ncombined_data %&gt;% \n  ggplot(aes(x = height,\n             fill = type),\n         position = \"dodge\",\n         color = \"white\") +\n  geom_density(alpha = 0.4)"
  },
  {
    "objectID": "03_disclosure_utility_metrics.html#specific-utility-metrics",
    "href": "03_disclosure_utility_metrics.html#specific-utility-metrics",
    "title": "3  Utility and Disclosure Risk Metrics",
    "section": "3.9 Specific Utility Metrics",
    "text": "3.9 Specific Utility Metrics\n\nSpecific utility metrics measure how suitable a synthetic dataset is for specific analyses.\nThese specific utility metrics will change from application to application, depending on common uses of the data.\nA helpful rule of thumb: general utility metrics are useful for the data synthesizers to be convinced that they’re doing a good job. Specific utility metrics are useful to convince downstream data users that the data synthesizers are doing a good job."
  },
  {
    "objectID": "03_disclosure_utility_metrics.html#recreating-inferences",
    "href": "03_disclosure_utility_metrics.html#recreating-inferences",
    "title": "3  Utility and Disclosure Risk Metrics",
    "section": "3.10 Recreating Inferences",
    "text": "3.10 Recreating Inferences\n\nIt can be useful to compare statistical analyses on the confidential data and synthetic data:\n\nDo the estimates have the same sign?\nDo the estimates have the same statistical inference at a common \\(\\alpha\\) level?\nDo the confidence intervals for the estimates overlap?\n\nEach of these questions is useful. Barrientos et al. (2021) combine all three questions into sign, significance, and overlap (SSO) match. SSO is the proportion of times that intervals overlap and have the same sign and significance."
  },
  {
    "objectID": "03_disclosure_utility_metrics.html#regression-confidence-interval-overlap",
    "href": "03_disclosure_utility_metrics.html#regression-confidence-interval-overlap",
    "title": "3  Utility and Disclosure Risk Metrics",
    "section": "3.11 Regression confidence interval overlap:",
    "text": "3.11 Regression confidence interval overlap:\n\n\n\n\n\n\nRegression Confidence Interval Overlap\n\n\n\nRegression confidence interval overlap quantifies how well confidence intervals from estimates on the synthetic data recreate confidence intervals from the confidential data.\n1 indicates perfect overlap. 0 indicates intervals that are adjacent but not overlapping. Negative values indicate gaps between the intervals.\nIt is common the compare intervals from linear regression models and logistic regression models.\n\n\n\n\nThe interpretability of confidence interval overlap diminishes when disclosure control methods generate very wide confidence intervals."
  },
  {
    "objectID": "03_disclosure_utility_metrics.html#microsimulation-results",
    "href": "03_disclosure_utility_metrics.html#microsimulation-results",
    "title": "3  Utility and Disclosure Risk Metrics",
    "section": "3.12 Microsimulation results",
    "text": "3.12 Microsimulation results\n\nThe Urban Institute and Tax Policy Center are heavy users of microsimulation.\nWhen synthesizing administrative tax data, we compare microsimulation results from tax calculators applied to the confidential data and synthetic data. Figure 3.6 shows results from the 2012 Synthetic Supplement PUF.\n\n\n\nFigure 3.6: microsim\n\n\n\n\nFigure 3.6 compares distributional output from baseline runs. It is also useful to compare tax reforms on the confidential and synthetic data."
  },
  {
    "objectID": "03_disclosure_utility_metrics.html#exercise-4-sso",
    "href": "03_disclosure_utility_metrics.html#exercise-4-sso",
    "title": "3  Utility and Disclosure Risk Metrics",
    "section": "3.13 Exercise 4: SSO",
    "text": "3.13 Exercise 4: SSO\n\nQuestionSolution\n\n\nSuppose we are interested in the following null and alternative hypotheses:\n\\[H_0: \\mu = 0\\]\n\\[H_a: \\mu \\ne 0\\]\nConsider the following output:\n\n\n[1] \"Confidential Mean: 2.7926609008331\"\n\n\n[1] \"Confidendital Confidence Interval\"\n\n\n[1] 2.308338 3.276984\nattr(,\"conf.level\")\n[1] 0.95\n\n\n[1] \"Synthetic Mean: 2.08452909904545\"\n\n\n[1] \"Synthetic Confidence Interval\"\n\n\n[1] 1.512416 2.656643\nattr(,\"conf.level\")\n[1] 0.95\n\n\nDo the synthetic data achieve SSO match?\n\n\nSuppose we are interested in the following null and alternative hypotheses:\n\\[H_0: \\mu = 0\\]\n\\[H_a: \\mu \\ne 0\\]\nConsider the following output:\n\n\n[1] \"Confidential Mean: 2.7926609008331\"\n\n\n[1] \"Confidendital Confidence Interval\"\n\n\n[1] 2.308338 3.276984\nattr(,\"conf.level\")\n[1] 0.95\n\n\n[1] \"Synthetic Mean: 2.08452909904545\"\n\n\n[1] \"Synthetic Confidence Interval\"\n\n\n[1] 1.512416 2.656643\nattr(,\"conf.level\")\n[1] 0.95\n\n\nDo the synthetic data achieve SSO match?\nYes! The confidence intervals overlap, the signs are the same, and the statistical significance is the same."
  },
  {
    "objectID": "03_disclosure_utility_metrics.html#disclosure-risk-metrics",
    "href": "03_disclosure_utility_metrics.html#disclosure-risk-metrics",
    "title": "3  Utility and Disclosure Risk Metrics",
    "section": "3.14 Disclosure Risk Metrics",
    "text": "3.14 Disclosure Risk Metrics\nWe now pivot to evaluating the disclosure risks of synthetic data."
  },
  {
    "objectID": "03_disclosure_utility_metrics.html#identity-disclosure-metrics",
    "href": "03_disclosure_utility_metrics.html#identity-disclosure-metrics",
    "title": "3  Utility and Disclosure Risk Metrics",
    "section": "3.15 Identity Disclosure Metrics",
    "text": "3.15 Identity Disclosure Metrics\n\n\n\n\n\n\nIdentity Disclosure Metrics\n\n\n\nIdentity disclosure metrics evaluate how often we correctly re-identify confidential records in the synthetic data.\nNote: These metrics require major assumptions about attacker information.\n\n\n\nFor fully synthetic datasets, there is no one to one relationship between individuals and records so identity disclosure risk is ill-defined. Generally identity disclosure risk applies to partially synthetic datasets (or datasets protected with traditional SDC methods).\nMost of these metrics rely on data maintainers essentially performing attacks against their synthetic data and seeing how successful they are at identifying individuals.\n\n\n3.15.1 Basic matching approaches\n\nWe start by making assumptions about the knowledge an attacker has (i.e., external publicly accessible data they have access to).\nFor each confidential record, the data attacker identifies a set of partially synthetic records which they believe contain the target record (i.e., potential matches) using the external variables as matching criteria.\nThere are distance-based and probability-based algorithms that can perform this matching. This matching process could be based on exact matches between variables or some relaxations (i.e., matching continuous variables within a certain radius of the target record, or matching adjacent categorical variables).\nWe then evaluate how accurate our re-identification process was using a variety of metrics.\n\nAs a simple example for the metrics we’re about to cover, imagine a data attacker has access to the following external data:\n\n\n\n\n\n\n  \n    \n      \n    \n    \n    \n      homeworld\n      species\n      name\n    \n  \n  \n    Naboo\nGungan\nJar Jar Binks\n    Naboo\nDroid\nR2-D2\n  \n  \n  \n\n\n\n\nAnd imagine that the partially synthetic released data looks like this:\n\n\n\n\n\n\n  \n    \n      \n    \n    \n    \n      homeworld\n      species\n      skin_color\n    \n  \n  \n    Tatooine\nHuman\nfair\n    Tatooine\nDroid\ngold\n    Naboo\nDroid\nwhite, blue\n    Tatooine\nHuman\nwhite\n    Alderaan\nHuman\nlight\n    Tatooine\nHuman\nlight\n  \n  \n  \n\n\n\n\nNote that the released partially synthetic data does not have names. But using some basic matching rules in combination with the external data, an attacker is able to identify the following potential matches for Jar Jar Binks and R2D2, two characters in the Starwars universe:\n\n\n\n\n\n\n  \n    \n      \n    \n    \n    \n      homeworld\n      species\n      skin_color\n    \n  \n  \n    \n      Potential Jar Jar matches\n    \n    Naboo\nGungan\norange\n    Naboo\nGungan\ngrey\n    Naboo\nGungan\ngreen\n    \n      Potential R2-D2 Matches\n    \n    Naboo\nDroid\nwhite, blue\n  \n  \n  \n\n\n\n\nAnd since we are the data maintainers, we can take a look at the confidential data and know that the highlighted rows are “true” matches.\n\n\n\n\n\n\n  \n    \n      \n    \n    \n    \n      homeworld\n      species\n      skin_color\n    \n  \n  \n    \n      Potential Jar Jar matches\n    \n    Naboo\nGungan\norange\n    Naboo\nGungan\ngrey\n    Naboo\nGungan\ngreen\n    \n      Potential R2-D2 Matches\n    \n    Naboo\nDroid\nwhite, blue\n  \n  \n  \n\n\n\n\nThese matches above are counted in various ways to evaluate identity disclosure risk. Below are some of those specific metrics. Generally for a good synthesis, we want a low expected match rate and true match rate, and a high false match rate.\n\nExpected Match RateTrue Match RateFalse Match Rate\n\n\n\nExpected Match Rate: On average, how likely is it to find a “correct” match among all potential matches? Essentially, the expected number of observations in the confidential data expected to be correctly matched by an intruder.\n\nHigher expected match rate = higher identification disclosure risk.\nThe two other risk metrics below focus on the subset of confidential records for which the intruder identifies a single match.\nIn our example, this is \\(\\frac{1}{3} + 1 = 1.333\\).\n\n\n\n\n\nTrue Match Rate: The proportion of true unique matches among all confidential records. Higher true match rate = higher identification disclosure risk.\nAssuming there are 100 rows in the confidential data in our example, this is \\(\\frac{1}{100} = 1\\%\\).\n\n\n\n\nFalse Match Rate: The proportion of false matches among the set of unique matches. Lower false match rate = higher identification disclosure risk.\nIn our example, this is \\(\\frac{0}{1} = 0\\%\\)."
  },
  {
    "objectID": "03_disclosure_utility_metrics.html#attribute-disclosure-risk-metrics",
    "href": "03_disclosure_utility_metrics.html#attribute-disclosure-risk-metrics",
    "title": "3  Utility and Disclosure Risk Metrics",
    "section": "3.16 Attribute Disclosure risk metrics",
    "text": "3.16 Attribute Disclosure risk metrics\n\nWe were able to learn about Jar Jar and R2D2 by re-identifying them in the data. It is possible to learn confidential attributes without perfectly re-identifying observations in the data.\n\n\n3.16.1 Predictive Accuracy\n\n\n\n\n\n\nPredictive Accuracy\n\n\n\nPredictive accuracy measures how well an attacker can learn about attributes in the confidential data using the synthetic data (and possibly external data).\n\n\n\nSimilar to above, you start by matching synthetic records to confidential records. Alternatively, you can build a predictive model using the synthetic data to make predictions on the confidential data.\nkey variables: Variables that an attacker already knows about a record and can use to match.\ntarget variables: Variables that an attacker wishes to know more or infer about using the synthetic data.\nPick a sensitive variable in the confidential data and use the synthetic data to make predictions. Evaluate the accuracy of the predictions.\n\n\n\n3.16.2 Membership Inference Tests\n\n\n\n\n\n\nMemebership Inference Test\n\n\n\nMembership inference tests explore how well an attacker can determine if a given observations was in the training data for the synthetic data.\n\n\n\nWhy is this important? Sometimes membership in a synthetic dataset is also confidential (e.g., a dataset of HIV positive patients or people who have experienced homelessness).\nAlso particularly useful for fully synthetic data where identity disclosure and attribute disclosure metrics don’t really make a lot of sense.\nAssumes that attacker has access to a subset of the confidential data, and wants to tell if one or more records was used to generate the synthetic data.\nSince we as data maintainers know the true answers, we can evaluate whether the attackers guess is true and can break it down many ways (e.g., true positives, true negatives, false positives or false negatives).\n\n\nsource for figure: Mendelevitch and Lesh (2021)\n\nThe “close enough” threshold is usually determined by a custom distance metric, like edit distance between text variables or numeric distance between continuous variables.\nOften you will want to choose different distance thresholds and evaluate how your results change.\n\n\n\n3.16.3 Copy Protection\n\n\n\n\n\n\nCopy Protection Metrics\n\n\n\nCopy protection metrics measure how often the synthesizer memorizes or inadvertantly duplicates confidential records.\n\n\n\nDistance to Closest record: Measures distance between each real record (\\(r\\)) and the closest synthetic record (\\(s_i\\)), as determined by a distance calculation.\nMany common distance metrics used in the literature including Euclidean distance, cosine distance, Gower distance, or Hamming distance (Mendelevitch and Lesh 2021).\nGoal of this metric is to easily expose exact copies or simple perturbations of the real records that exist in the synthetic dataset.\n\n\n\n\n\n\n\n\n\n\n\n\nNote that having DCR = 0, doesn’t necessarily mean a high disclosure risk because in some datasets the “space” spanned by the variables in scope is relatively small.\n\n\n\n3.16.4 Hold Out Data\n\n\n\n\n\n\nHoldout Data\n\n\n\nMembership inference tests and copy protection metrics are informative but lack context. When possible, create a holdout data set similar to the training data. Then calculate membership inference tests and copy protections metrics replacing the synthetic data with the hold out data. The results are useful for benchmarking the original membership inference tests and copy protection metrics."
  },
  {
    "objectID": "03_disclosure_utility_metrics.html#exercise-5-disclosure-metrics",
    "href": "03_disclosure_utility_metrics.html#exercise-5-disclosure-metrics",
    "title": "3  Utility and Disclosure Risk Metrics",
    "section": "3.17 Exercise 5: Disclosure Metrics",
    "text": "3.17 Exercise 5: Disclosure Metrics\n\nQuestionNotes\n\n\n\nFigure 3.7: Attacker information and partially synthetic data\n\n\n\n\n\n\n\n\n  \n    \n      \n    \n    \n    \n      Name\n      Year\n      Elective\n    \n  \n  \n    Adam\n2009\nChorus\n    Betsy\n2010\nBand\n  \n  \n  \n\n\nAttacker Information\n\n\n\n\n\n\n\n\n  \n    \n      \n    \n    \n    \n      Year\n      Elective\n      Synthetic SAT\n    \n  \n  \n    2008\nChorus\n1100\n    2008\nChorus\n1420\n    2009\nChorus\n900\n    2009\nBand\n1100\n    2010\nBand\n1420\n    2010\nBand\n900\n    2010\nBand\n1200\n  \n  \n  \n\n\nPartially Synthetic Data\n\n\n\n\n\n\nAre there any matches for Adam?\nAre there any matches for Betsy?\nWhat risks are created by the release?\n\n\n\n\nFigure 3.8: Attacker information and partially synthetic data\n\n\n\n\n\n\n\n\n  \n    \n      \n    \n    \n    \n      Name\n      Year\n      Elective\n    \n  \n  \n    Adam\n2009\nChorus\n    Betsy\n2010\nBand\n  \n  \n  \n\n\nAttacker Information\n\n\n\n\n\n\n\n\n  \n    \n      \n    \n    \n    \n      Year\n      Elective\n      Synthetic SAT\n    \n  \n  \n    2008\nChorus\n1100\n    2008\nChorus\n1420\n    2009\nChorus\n900\n    2009\nBand\n1100\n    2010\nBand\n1420\n    2010\nBand\n900\n    2010\nBand\n1200\n  \n  \n  \n\n\nPartially Synthetic Data\n\n\n\n\n\n\nAre there any matches for Adam?\n\nUsing Year and Elective as key variables, Adam has a unique match (highlighted in pink)\n\nAre there any matches for Betsy?\n\nUsing Year and Elective as key variables, Betsy has a three matches (highlighted in yellow)\n\nWhat risks are created by the release?\n\nIt is tough to say without context but here are a few considerations:\n\nIs SAT easily observable outside of the data?\nAre the values of Synthetic SAT close to the true values for SAT?\nAre SAT and Synthetic SAT likely to be close under random guessing because it has low sample variance?"
  },
  {
    "objectID": "03_disclosure_utility_metrics.html#exercise-6-disclosure-metrics",
    "href": "03_disclosure_utility_metrics.html#exercise-6-disclosure-metrics",
    "title": "3  Utility and Disclosure Risk Metrics",
    "section": "3.18 Exercise 6: Disclosure Metrics",
    "text": "3.18 Exercise 6: Disclosure Metrics\n\nQuestionNotes\n\n\nWhat is an example of a utility metric you might design to ensure your synthetic data are meeting the needs of those users well?\nSuppose you have a colleague that generated fully synthetic data and wants to measure identity disclosure risk. Your colleague proposes using the expected match rate, false match rate, and true match rate to evaluate how well the synthetic data protects the identity of people in the data. Why is their approach flawed and what disclosure metrics would you suggest instead?\n\n\nWhat is an example of a utility metric you might design to ensure your synthetic data are meeting the needs of those users well?\n\nIs the metric you proposed above a general or specific utility metric?\n\nSpecific Utility Metric\nGeneral Utility Metric\n\n\nSuppose you have a colleague that generated fully synthetic data and wants to measure identity disclosure risk. Your colleague proposes using the expected match rate, false match rate, and true match rate to evaluate how well the synthetic data protects the identity of people in the data. Why is their approach flawed and what disclosure metrics would you suggest instead?\nThese metrics apply only to partially synthetic data. Consider instead a metric like the membership inference test."
  },
  {
    "objectID": "03_disclosure_utility_metrics.html#suggested-reading",
    "href": "03_disclosure_utility_metrics.html#suggested-reading",
    "title": "3  Utility and Disclosure Risk Metrics",
    "section": "3.19 Suggested Reading",
    "text": "3.19 Suggested Reading\nSnoke, Joshua, Gillian M Raab, Beata Nowok, Chris Dibben, and Aleksandra Slavkovic. 2018b. “General and Specific Utility Measures for Synthetic Data.” Journal of the Royal Statistical Society: Series A (Statistics in Society) 181 (3): 663–88.\nBowen, Claire McKay, Victoria Bryant, Leonard Burman, Surachai Khitatrakun, Robert McClelland, Philip Stallworth, Kyle Ueyama, and Aaron R Williams. 2020. “A Synthetic Supplemental Public Use File of Low-Income Information Return Data: Methodology, Utility, and Privacy Implications.” In International Conference on Privacy in Statistical Databases, 257–70. Springer.\n\n\n\n\nBarrientos, Andrés F., Aaron R. Williams, Joshua Snoke, and Claire McKay Bowen. 2021. “A Feasibility Study of Differentially Private Summary Statistics and Regression Analyses with Evaluations on Administrative and Survey Data.” https://doi.org/10.48550/ARXIV.2110.12055.\n\n\nMendelevitch, Ofer, and Michael D Lesh. 2021. “Fidelity and Privacy of Synthetic Medical Data.” arXiv Preprint arXiv:2101.08658.\n\n\nSnoke, Joshua, Gillian M. Raab, Beata Nowok, Chris Dibben, and Aleksandra Slavkovic. 2018a. “General and Specific Utility Measures for Synthetic Data.” Journal of the Royal Statistical Society: Series A (Statistics in Society) 181 (3): 663–88. https://doi.org/10.1111/rssa.12358.\n\n\nSnoke, Joshua, Gillian M Raab, Beata Nowok, Chris Dibben, and Aleksandra Slavkovic. 2018b. “General and Specific Utility Measures for Synthetic Data.” Journal of the Royal Statistical Society: Series A (Statistics in Society) 181 (3): 663–88.\n\n\nWoo, Mi-Ja, Jerome P Reiter, Anna Oganian, and Alan F Karr. 2009. “Global Measures of Data Utility for Microdata Masked for Disclosure Limitation.” Journal of Privacy and Confidentiality 1 (1)."
  },
  {
    "objectID": "04_synthesis-demo.html#demo-creating-fully-synthetic-palmerpenguins-data",
    "href": "04_synthesis-demo.html#demo-creating-fully-synthetic-palmerpenguins-data",
    "title": "4  Data Synthesis Demo",
    "section": "4.1 Demo: Creating Fully Synthetic palmerpenguins Data",
    "text": "4.1 Demo: Creating Fully Synthetic palmerpenguins Data\nThe palmerpenguins package contains data about three species of penguins collected from three islands in the Palmer Archipelago, Antarctica. We will use an adapted version of the dataset to demonstrate some of the concepts discussed above.\n\n# create dataset we will be using\npenguins &lt;- penguins |&gt; \n  filter(species == \"Adelie\") |&gt; \n  select(\n    sex, \n    bill_length_mm, \n    flipper_length_mm\n  ) |&gt;\n  drop_na() \n\npenguins |&gt; \n  head() |&gt; \n  create_table()\n\n\n\n\n\n  \n    \n      \n    \n    \n    \n      sex\n      bill_length_mm\n      flipper_length_mm\n    \n  \n  \n    male\n39.1\n181\n    female\n39.5\n186\n    female\n40.3\n195\n    female\n36.7\n193\n    male\n39.3\n190\n    female\n38.9\n181\n  \n  \n  \n\n\n\n\n\nThe above code simplifies the dataset to only three variables and removes missing values in those variables. We will synthesize the sex, bill_length_mm, and flipper_length_mm variables in this dataset using some of the methods discussed above. Since we are synthesizing all three variables, our final version of the dataset is considered fully synthetic.\n\n4.1.1 Synthesize sex variable\nLet’s start by synthesizing sex, which is a binary variable that can take a value of either “male” or “female”. To synthesize this variable, we will identify the underlying percentages of the data that fall into each category and use it to generate records that mimic the properties of the confidential data.\n\n# identify percentage of total that each category (sex) makes up\npenguins |&gt;\n  count(sex) |&gt;\n  mutate(relative_frequency = n / sum(n)) |&gt; \n  create_table()\n\n\n\n\n\n  \n    \n      \n    \n    \n    \n      sex\n      n\n      relative_frequency\n    \n  \n  \n    female\n73\n0.5\n    male\n73\n0.5\n  \n  \n  \n\n\n\n\n\nUsing these proportions, we will now randomly sample with replacement to mimic the underlying distribution of gender.\n\n# set a seed so pseudo-random processes are reproducible\nset.seed(20220301)\n\n# vector of gender categories\nsex_categories &lt;- c(\"female\", \"male\")\n\n# size of sample to generate\nsynthetic_data_size &lt;- nrow(penguins)\n\n# probability weights\nsex_probs &lt;- penguins |&gt;\n  count(sex) |&gt;\n  mutate(relative_frequency = n / sum(n)) |&gt;\n  pull(relative_frequency)\n\n# use sample function to generate synthetic vector of genders\nsex_synthetic &lt;- sample(\n  x = sex_categories, \n  size = synthetic_data_size, \n  replace = TRUE, \n  prob = sex_probs\n)\n\nOur new sex_synthetic variable will form the foundation of our synthesized data.\n\n# use vector to generate synthetic gender column\npenguins_synthetic &lt;- tibble(\n  sex = sex_synthetic\n)\n\npenguins_synthetic |&gt; \n  head() |&gt; \n  create_table()\n\n\n\n\n\n  \n    \n      \n    \n    \n    \n      sex\n    \n  \n  \n    female\n    male\n    female\n    male\n    female\n    male\n  \n  \n  \n\n\n\n\n\n\n\n4.1.2 Synthesize bill_length_mm variable\nUnlike sex, bill_length_mm is numeric.\n\nsummary(penguins$bill_length_mm)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  32.10   36.73   38.85   38.82   40.77   46.00 \n\n\n\nTo synthesize this variable, we are going to predict the bill_length_mm for each penguin using a linear regression with sex as a predictor.\nNote that sex is a factor variable with possible values of “male” and “female” which can’t directly be used in a regression. So under the hood, R converts that factor variable into a binary numeric variables (i.e. 0 or 1), and then runs the regression.\n\n# linear regression\nbill_length_lm &lt;- lm(\n  formula = bill_length_mm ~ sex, \n  data = penguins\n)\n\nsummary(bill_length_lm)\n\n\nCall:\nlm(formula = bill_length_mm ~ sex, data = penguins)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-5.790 -1.357  0.076  1.393  5.610 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  37.2575     0.2524 147.608  &lt; 2e-16 ***\nsexmale       3.1329     0.3570   8.777 4.44e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.157 on 144 degrees of freedom\nMultiple R-squared:  0.3485,    Adjusted R-squared:  0.344 \nF-statistic: 77.03 on 1 and 144 DF,  p-value: 4.44e-15\n\n\n \nNow that we have coefficients for the linear regression, we can generate our synthetic values. First, we try a straightforward prediction of bill lengths using the synthetic sex variable.\n\n# predict bill length with model coefficients\nbill_length_synthetic_method1 &lt;- predict(\n  object = bill_length_lm, \n  newdata = penguins_synthetic\n)\n\n# add predictions to synthetic data as bill_length\npenguins_synthetic_method1 &lt;- penguins_synthetic |&gt;\n  mutate(bill_length_mm = bill_length_synthetic_method1)\n\nAnd now we compare the univariate distributions of the confidential data to our newly synthesized variable with a graph.\n\n# create dataframe with both confidential and synthesized values\ncompare_penguins &lt;- bind_rows(\n  \"confidential\" = penguins,\n  \"synthetic\" = penguins_synthetic_method1,\n  .id = \"data_source\"\n)\n\n# plot comparison of bill_length_mm distributions\ncompare_penguins |&gt;\n  select(data_source, bill_length_mm) |&gt;\n  pivot_longer(-data_source, names_to = \"variable\") |&gt;\n  ggplot(aes(x = value, fill = data_source)) +\n  geom_density(alpha = 0.3) +\n  labs(title = \"Comparison of Univariate Distributions\",\n       subtitle = \"Method 1\") +\n  scatter_grid()\n\n\n\n\n\n\n4.1.3 What is the problem here?\nSimply using the predicted values from our linear regression does not give us enough variation in the synthetic variable.\nTo understand more about the predictions made from the linear regression model, let’s dig into the predicted values for the first five rows of data and the corresponding synthetic sex.\n\n# Look at first few rows of synthetic data\npenguins_synthetic_method1 |&gt; \n  head() |&gt; \n  create_table()\n\n\n\n\n\n  \n    \n      \n    \n    \n    \n      sex\n      bill_length_mm\n    \n  \n  \n    female\n37.25753\n    male\n40.39041\n    female\n37.25753\n    male\n40.39041\n    female\n37.25753\n    male\n40.39041\n  \n  \n  \n\n\n\n\n\nWe know from our regression analysis output from above that the intercept (\\(\\beta_0\\)) is 42.1 and the coefficient for a male penguin (\\(\\beta_1\\)) is 3.8. Therefore, if the penguin is male, we have a predicted value (\\(\\hat{y}\\)) of 42.1 + 3.8 = 45.9. If the penguin is female, our predicted value (\\(\\hat{y}\\)) is only the intercept, 42.1.\nBecause sex will only take two values, our synthetic bill_length_mm also only takes two values. The model fit limits the variation that is possible, making our synthetic variable significantly less useful.\nInstead, we can try a second method. We will create a version of the variable, where the synthetic value is a draw from a normal distribution with a mean of the regression line for the given predictions, and standard deviation equal to the residual standard error.\n\n# set a seed so pseudo-random processes are reproducible\nset.seed(20220301)\n\n# predict bill length with model coefficients\nbill_length_predicted &lt;- predict(\n  object = bill_length_lm, \n  newdata = penguins_synthetic\n)\n\n# synthetic column using normal distribution centered on predictions with sd of residual standard error\nbill_length_synthetic_method2 &lt;- rnorm(\n  n = nrow(penguins_synthetic), \n  mean = bill_length_predicted, \n  sd = sigma(bill_length_lm)\n)\n\n# add predictions to synthetic data as bill_length\npenguins_synthetic_method2 &lt;- penguins_synthetic |&gt;\n  mutate(bill_length_mm = bill_length_synthetic_method2)\n\nNow, we again compare the univariate distributions of the confidential data and the synthetic data we generated with this second method.\n\n\nCode\n# create dataframe with both confidential and synthesized values\ncompare_penguins &lt;- bind_rows(\n  \"confidential\" = penguins,\n  \"synthetic\" = penguins_synthetic_method2,\n  .id = \"data_source\"\n)\n\n# plot comparison of bill_length_mm distributions\ncompare_penguins |&gt;\n  select(data_source, bill_length_mm) |&gt;\n  pivot_longer(-data_source, names_to = \"variable\") |&gt;\n  ggplot(aes(x = value, fill = data_source)) +\n  geom_density(alpha = 0.3) +\n  labs(title = \"Comparison of Univariate Distributions\",\n       subtitle = \"Method 2\") +\n  scatter_grid()\n\n\n\n\n\nWe have much more variation with this new method, though the distributions still do not match perfectly. We choose this method’s output to add as the synthetic bill_length_mm in our final synthesized dataset. And now our synthesized dataset has two columns.\n\n# using method 2 as synthesized variable\npenguins_synthetic &lt;- penguins_synthetic |&gt;\n  mutate(bill_length_mm = bill_length_synthetic_method2)\n\n\npenguins_synthetic |&gt; \n  head() |&gt; \n  create_table()\n\n\n\n\n\n  \n    \n      \n    \n    \n    \n      sex\n      bill_length_mm\n    \n  \n  \n    female\n38.76954\n    male\n42.85524\n    female\n38.52128\n    male\n38.15742\n    female\n35.04525\n    male\n39.22174\n  \n  \n  \n\n\n\n\n\n\n\n4.1.4 Synthesize flipper_length_mm\nThe flipper_length_mm variable is also numeric, so we can follow the same steps we used to synthesize bill_length_mm.\n\nsummary(penguins$flipper_length_mm)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  172.0   186.0   190.0   190.1   195.0   210.0 \n\n\n\nThis time, we regress flipper_length_mm on both sex and bill_length_mm.\n\n# linear regression\nflipper_length_lm &lt;- lm(\n  formula = flipper_length_mm ~ sex + bill_length_mm, \n  data = penguins\n)\n\nsummary(flipper_length_lm)\n\n\nCall:\nlm(formula = flipper_length_mm ~ sex + bill_length_mm, data = penguins)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-16.0907  -2.8058  -0.0534   3.3284  15.8789 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    170.6183     8.7497  19.500   &lt;2e-16 ***\nsexmale          3.1721     1.2422   2.554   0.0117 *  \nbill_length_mm   0.4610     0.2341   1.970   0.0508 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.058 on 143 degrees of freedom\nMultiple R-squared:  0.1492,    Adjusted R-squared:  0.1373 \nF-statistic: 12.54 on 2 and 143 DF,  p-value: 9.606e-06\n\n\n \nSince we already know we prefer the method using draws from the normal distribution centered on the mean of the regression line, we will default to that.\n\n# set a seed so pseudo-random processes are reproducible\nset.seed(20220301)\n\n# predict flipper length with model coefficients\nflipper_length_predicted &lt;- predict(\n  object = flipper_length_lm, \n  newdata = penguins_synthetic\n)\n\n# synthetic column using normal distribution centered on predictions with sd of residual standard error\nflipper_length_synthetic &lt;- rnorm(\n  n = nrow(penguins_synthetic), \n  mean = flipper_length_predicted, \n  sd = sigma(flipper_length_lm)\n)\n\n# add predictions to synthetic data as flipper_length\npenguins_synthetic &lt;- penguins_synthetic |&gt;\n  mutate(flipper_length_mm = flipper_length_synthetic)\n\n\n\n4.1.5 Final (fully synthesized) product and discussion\nWith all three synthesized variables, we can compare the the univariate distributions of the confidential data and the synthetic data (we have already done this for bill_length_mm).\n\n# create dataframe with both confidential and synthesized values\ncompare_penguins &lt;- bind_rows(\n  \"confidential\" = penguins,\n  \"synthetic\" = penguins_synthetic,\n  .id = \"data_source\"\n)\n\n# Write out final compare_penguins df for use in future exercises\ndir.create(here::here(\"data/\"), showWarnings = FALSE)\n\ncompare_penguins |&gt; \n  write_csv(here::here(\"data\", \"penguins_synthetic_and_confidential.csv\"))\n\nFirst, we can compare the distributions of the sex variable in the confidential and synthetic data.\n\nsex_comparison &lt;- compare_penguins |&gt;\n  select(data_source, sex) |&gt;\n  count(data_source, sex) |&gt;\n  group_by(data_source) |&gt;\n  mutate(relative_frequency = n / sum(n))\n\nsex_comparison |&gt; \n  ggplot(aes(x = n, y = sex, fill = data_source)) +\n  geom_text(aes(label = n), \n            position = position_dodge(width = 0.5),\n            hjust = -0.4) + \n  geom_col(position = \"dodge\", alpha = 0.7) +\n  scale_x_continuous(expand = expansion(add = c(0, 15))) +\n  labs(y = \"\", x = \"N\", title = \"Comparison of sex distribution\")\n\n\n\n\n\n# plot comparison of distributions\ncompare_penguins |&gt;\n  select(\n    data_source, \n    bill_length_mm, \n    flipper_length_mm\n  ) |&gt;\n  pivot_longer(-data_source, names_to = \"variable\") |&gt;\n  ggplot(aes(x = value, fill = data_source)) +\n  geom_density(alpha = 0.3) +\n  facet_wrap(~variable, scales = \"free\") +\n  labs(title = \"Comparison of Univariate Distributions\",\n       subtitle = \"Final synthetic product\") +\n  scatter_grid()"
  },
  {
    "objectID": "04_synthesis-demo.html#exercise-1",
    "href": "04_synthesis-demo.html#exercise-1",
    "title": "4  Data Synthesis Demo",
    "section": "4.2 Exercise 1",
    "text": "4.2 Exercise 1\n\n4.2.1 Discuss sex synthesis\n\nQuestion 1Question 1 Notes\n\n\n\nWas the method we used parametric or nonparametric? Why?\nWhat do we notice about the synthetic variable compared to the original?\nWe generated a new sex variable that was the same length as the confidential data. Was this necessary for the method to be applied correctly?\n\n\n\n\nWas the method we used parametric or nonparametric? Why?\n\nNonparametric; the data generation process was based on underlying frequencies rather than a distribution or generative model.\n\nWhat do we notice about the synthetic variable compared to the original?\nWe generated a new sex variable that was the same length as the confidential data. Was this necessary for the method to be applied correctly?\n\nNo, the number of rows in the dataset does not matter as long as the underlying frequencies are preserved\n\n\n\n\n\n\n\n4.2.2 Discuss bill_length_mm synthesis\n\nQuestion 2Question 2 Notes\n\n\n\nWas the method we just used parametric or nonparametric? Why?\nWhat do we notice about the synthetic variable compared to the original?\nAre we synthesizing these data sequentially? How do you know?\n\n\n\n\nWas the method we just used parametric or nonparametric? Why?\n\nParametric: the data generation process was based on a (linear) model.\n\nWhat do we notice about the synthetic variable compared to the original?\nAre we synthesizing these data sequentially? How do you know?\n\nYes, the previously synthesized sex variable was used as a predictor.\n\n\n\n\n\n\n\n4.2.3 Discuss flipper_length_mm synthesis\n\nQuestion 3Question 3 Notes\n\n\n\nWhat do we notice about the synthetic variable compared to the original?\nWhat are benefits and drawbacks to generating this variable sequentially?\n\n\n\n\nWhat do we notice about the synthetic variable compared to the original?\nWhat are benefits and drawbacks to generating this variable sequentially?\n\nBenefits: generating this variable sequentially allows us to preserve more of the multivariate relationships present in the data.\nDrawbacks: if we synthesized previous variables poorly, our synthesis of this variable will also be affected.\n\n\n\n\n\n\n\n4.2.4 Overall Discussion\nQuestion 4\n\nWould you feel comfortable using this version of the dataset for analysis? Why or why not? What additional tests might you run to determine this?"
  },
  {
    "objectID": "05_synthetic-data-case-studies.html#case-studies",
    "href": "05_synthetic-data-case-studies.html#case-studies",
    "title": "5  Synthetic Data Case Studies",
    "section": "5.1 Case Studies",
    "text": "5.1 Case Studies\n\n5.1.1 Fully Synthetic PUF for IRS Non-Filers (Bowen et al. 2020)\n\nData: A 2012 file of “non-filers” created by the IRS Statistics of Income Division.\nMotivation: Non-filer information is important for modeling certain tax reforms and this was a proof-of-concept for a more complex file.\nMethods: Sequential CART models with additional noise added based on the sparsity of nearby observations in the confidential distribution.\nImportant metrics:\n\nGeneral utility: Proportions of non-zero values, first four moments, correlation fit\nSpecific utility: Tax microsimulation, regression confidence interval overlap\nDisclosure: Node heterogeneity in the CART model, rates of recreating observations\n\nLessons learned:\n\nSynthetic data can work well for tax microsimulation.\nIt is difficult to match certain utility metrics for sparse variables.\n\n\n\n\n5.1.2 Fully Synthetic SIPP data (Benedetto et al. 2018)\n\nData: Survey of Income and Program Participation linked to administrative longitudinal earnings and benefits data from IRS and SSA.\nMotivation: To expand access to detailed economic data that is highly restricted without heavy disclosure control.\nMethods: Sequential regression multiple imputation (SRMI) with OLS regression, logistic regression, and Bayesian bootstrap. They released four implicates of the synthetic data.\nImportant metrics:\n\nGeneral utility: pMSE\nSpecific utility: None\nDisclosure: Distance based re-identification, RMSE of the closest record to measure attribute disclosure\n\nLessons learned:\n\nOne of the first major synthetic files in the US.\nThe file includes complex relationships between family members that are synthesized.\n\n\n\n\n5.1.3 Partially Synthetic Geocodes (Drechsler and Hu 2021)\n\nData: Integrated Employment Biographies (German administrative data) with linked geocodes (latitude and longitude)\nMotivation: Rich geographic information can be used to answer many important labor market research questions. This data would otherwise would be too sensitive to release, due to the possibility of identifying an individual based on the combination of their location and other attributes.\nMethods: CART with categorical geocodes. Also evaluated CART with continuous geocodes and a Bayesian latent class model.\nImportant metrics:\n\nGeneral utility: Relative frequencies of cross tabulations\nSpecific utility: Zip Code comparisons of tabulated variables, Ripley’s K- and L-functions\nDisclosure: Probabilities of re-identification (Reiter and Mitra, 2009) -&gt; comparison of expected match risk and the true match rate\n\nLessons learned:\n\nThe synthetic data with geocodes had more measured disclosure risk than the original data.\nSynthesizing more variables made a huge difference in the measured disclosure risks.\nAdjusting CART hyperparameters was not an effective way to manage the risk-utility tradeoff.\nThey stratified the data before synthesis for computational reasons."
  },
  {
    "objectID": "05_synthetic-data-case-studies.html#suggested-reading",
    "href": "05_synthetic-data-case-studies.html#suggested-reading",
    "title": "5  Synthetic Data Case Studies",
    "section": "5.2 Suggested Reading",
    "text": "5.2 Suggested Reading\nSnoke, Joshua, Gillian M Raab, Beata Nowok, Chris Dibben, and Aleksandra Slavkovic. 2018b. “General and Specific Utility Measures for Synthetic Data.” Journal of the Royal Statistical Society: Series A (Statistics in Society) 181 (3): 663–88.\nBowen, Claire McKay, Victoria Bryant, Leonard Burman, Surachai Khitatrakun, Robert McClelland, Philip Stallworth, Kyle Ueyama, and Aaron R Williams. 2020. “A Synthetic Supplemental Public Use File of Low-Income Information Return Data: Methodology, Utility, and Privacy Implications.” In International Conference on Privacy in Statistical Databases, 257–70. Springer.\n\n\n\n\nBenedetto, Gary, Jordan C Stanley, Evan Totty, et al. 2018. “The Creation and Use of the SIPP Synthetic Beta V7. 0.” US Census Bureau.\n\n\nBowen, Claire McKay, Victoria Bryant, Leonard Burman, Surachai Khitatrakun, Robert McClelland, Philip Stallworth, Kyle Ueyama, and Aaron R Williams. 2020. “A Synthetic Supplemental Public Use File of Low-Income Information Return Data: Methodology, Utility, and Privacy Implications.” In International Conference on Privacy in Statistical Databases, 257–70. Springer.\n\n\nDrechsler, Jörg, and Jingchen Hu. 2021. “Synthesizing Geocodes to Facilitate Access to Detailed Geographical Information in Large-Scale Administrative Data.” Journal of Survey Statistics and Methodology 9 (3): 523–48."
  },
  {
    "objectID": "c_differential-privacy.html",
    "href": "c_differential-privacy.html",
    "title": "Differential Privacy",
    "section": "",
    "text": "In this section, we’ll introduce what differential privacy is, cover the fundamentals of differential privacy, and review a few differential privacy use cases.\nBy the end, you will be learn the difference between traditional statistical data privacy methods and differentially private methods, implement basic differential privacy mechanisms, and comprehend applications of differential privacy."
  },
  {
    "objectID": "06_introduction_to_differential_privacy.html#recap",
    "href": "06_introduction_to_differential_privacy.html#recap",
    "title": "6  Introduction to Differential Privacy and Formal Privacy",
    "section": "6.1 Recap",
    "text": "6.1 Recap\nIn the last session, we discussed general and specific utility metrics for evaluating synthetic data.\n\nGeneral utility metrics include summary statistics, correlation fit, and discriminant based methods such as the pMSE ratio.\nSpecific utility metrics include regression confidence interval overlap and microsimulation results.\n\nWe also discussed disclosure risk evaluation, including metrics for identity disclosure, attribute disclosure, and other metrics such as membership inference tests and copy protection metrics.\nA common theme throughout this lesson was that many of these metrics require judgement calls on the part of the data stewards. For example,\n\nWhat should be considered a “good” pMSE ratio score for a synthetic dataset? Does this change in the context of the domain?\nHow much disclosure risk is “too much”? Are there types of disclosure risk data users can tolerate more than others?\nWhen evaluating disclosure risk, are we making assumptions about how the attacker will approach the data or resources the attacker has access to? Do these assumptions hold in the context of the domain?\n\nMotivation for today: what if we could create a mathematical bound on the disclosure risk for any question asked of the confidential data?\nIn this session, we will cover a high-level overview of formal privacy, differential privacy, and formally private mechanisms. This summary will involve some mathematical intuition and present some mathematical equations."
  },
  {
    "objectID": "06_introduction_to_differential_privacy.html#formal-privacy",
    "href": "06_introduction_to_differential_privacy.html#formal-privacy",
    "title": "6  Introduction to Differential Privacy and Formal Privacy",
    "section": "6.2 Formal Privacy",
    "text": "6.2 Formal Privacy\nAfter learning about evaluating disclosure risks, several questions have arisen:\n\nWhat level of disclosure risk should be considered acceptable, and what type?\nWhen assessing disclosure risk, what assumptions can be made about the methods or approach a potential data intruder may employ?\nHow do we account for the resources that a data intruder could potentially access?\nDo these assumptions hold within the context of the specific real-world application?\n\nAddressing these questions is why applying statistical disclosure control (SDC) methods is difficult. When developing a SDC method, privacy researchers must predict how a data intruder might attack the data, considering what sensitive information they want and what resources they have now or in the future.\nFormal privacy did away with those assumptions. It provides a mathematical bound on the disclosure risk for any statistic applied to the confidential data. Although methods developed within the formal privacy framework are considered SDC methods, data privacy researchers often separate formal privacy from other SDC methods. We will refer to the SDC methods and disclosure risk measures not developed under formal privacy as traditional SDC methods and traditional disclosure risk definitions.\n\n6.2.1 Definition of Formal Privacy\nAlthough the privacy community has not fully agreed on a common definition, the U.S. Census Bureau1 defines formal privacy as a subset of SDC methods that give “formal and quantifiable guarantees on inference disclosure risk and known algorithmic mechanisms for releasing data that satisfy these guarantees.”\nIn general, formally private methods have the following features (Bowen and Garfinkel 2021):\n\nAbility to quantify and adjust the privacy-utility trade-off, typically through parameters.\nAbility to rigorously and mathematically prove the maximum privacy loss that can result from the release of information.\nFormal privacy definitions also allow one to compose multiple statistics. In other words, a data curator can compute the total privacy loss from multiple individual information releases.\n\n\n\n\n\n\n\nNote on terminology\n\n\n\nIn the formal privacy literature, privacy researchers often use the terms mechanism, algorithm, and method interchangeably to describe the process of releasing a private statistical output. Sometimes these researchers refer to a simple process, such as adding noise directly to a computed statistic. Other times they refer to more complicated processes, such as post-processing (explained later in this section). We do not see a clear delineation in the literature when using the three terms. More crucially is that anything referred to as a formally private method, formally private mechanism, or formally private algorithm must provably satisfy the relevant definition of formal privacy.\n\n\n\n\n6.2.2 Data Products\nIn most of the cases we’ve discussed so far, the released data product is a full dataset. However, a spectrum of data products could be released by a data curator after applying privacy methods. Here are a list examples of possible data products that a data curator could release after applying SDC methods, roughly from most to least detailed:\n\nmicrodata (e.g., public use microdata series or PUMS)\nsummary tables (e.g., American Community Survey tables)\nsummary statistics (e.g., multiple statistics on income in a state)\nsingle statistics (e.g., maximum age in a county)\n\nCurators could release one of these products after applying a data privacy method, or they could release them “on demand,” to answer different questions using the data.\n\nQuestions asked of the data are referred to in computer science terminology as queries which are statistics.\nThe below image shows how the on-demand (or interactive) version of this process might work, with a user asking a question of the confidential data and receiving an answer that has been manipulated with algorithm \\(\\mathcal{A}\\).\n\nNote that while in the example the statistic in question is a single number, any of the above data products are available as potential output.\n\n\n\n\nCurators must consider how much noise should be added and how many statistics should be made available.\nIf too many questions are answered with enough accuracy, all the data could be compromised, so the type and number of questions asked of the data are limited by the curators (Bowen and Garfinkel 2021).\n\nThe main difference between traditional SDC methods and formally private methods is the ability to account for all information being “leaked” from the confidential data. We can think of traditional SDC methods as someone charging a limitless credit card; formally private methods are when someone charges to a debit card with a set budget. In both scenarios, there is a running bill, but only one requires constantly checking the balance. We can easily imagine that not tracking that bill is the equivalent of releasing too many statistics with enough accuracy, which could compromise the confidential data (Bowen and Garfinkel 2021). Although data stewards must limit the type and number of questions asked of the data in both traditional and formal privacy settings, they are faced with “tracking the bill” under a formal privacy framework."
  },
  {
    "objectID": "06_introduction_to_differential_privacy.html#exercise-1",
    "href": "06_introduction_to_differential_privacy.html#exercise-1",
    "title": "6  Introduction to Differential Privacy and Formal Privacy",
    "section": "6.3 Exercise 1",
    "text": "6.3 Exercise 1\n\nQuestionHints\n\n\nImagine you are in charge of safeguarding a dataset against an intruder. Brainstorm and discuss features of the intruder that you would consider a “worst-case scenario” in terms of privacy (short of the intruder having access to the entire confidential dataset).\n\n\n\nHow much computational power might they have?\nMight they have access to other information about the observations?\nMight they have access to other, supplemental datasets?"
  },
  {
    "objectID": "06_introduction_to_differential_privacy.html#differential-privacy",
    "href": "06_introduction_to_differential_privacy.html#differential-privacy",
    "title": "6  Introduction to Differential Privacy and Formal Privacy",
    "section": "6.4 Differential Privacy",
    "text": "6.4 Differential Privacy\nDifferential privacy (DP) is just one type of formal privacy.\n\n\nDP is a strict mathematical definition that a method must satisfy (or meet the mathematical conditions) to be considered differentially private, not a statement or description of the data itself.\nInformally, DP does not make assumptions about how a data intruder will attack the data and the amount of external information or computing power an actor has access to, now or in the future.\nCurators control the strength of this privacy guarantee by adjusting the privacy loss budget.\n\n\n6.4.1 Privacy Loss Budget\nFormal privacy uses the concept of a privacy loss budget, typically represented mathematically as \\(\\epsilon\\). The privacy loss budget bounds the disclosure risk associated with releasing data or statistics (Bureau 2021).\n\n\n\n\n\n\nNote on the privacy loss parameter\n\n\n\nThere are many other privacy loss parameters, but we will use \\(\\epsilon\\) here as a general representation of the privacy loss budget for simplicity.\n\n\nThe privacy loss budget can be thought of as a knob that adjusts the trade-off between data privacy and utility. Some things to keep in mind about the privacy loss budget are as follows:\n\nThe data curator must decide the privacy loss budget (i.e., the total amount of \\(\\epsilon\\)) before the release of any data or statistic. Like a real budget, when privacy loss budget is exhausted, no more information from the confidential data is released.\nA larger value of \\(\\epsilon\\) increases the maximum disclosure risk (i.e., the upper bound of the disclosure risk) associated with a given release of information. Simply put,\n\nlarger \\(\\epsilon\\) = less noise potentially added to a statistic = more accuracy, but less privacy, and\nsmaller \\(\\epsilon\\) = more noise potentially added to a statistic = less accuracy, but more privacy.\n\n\n\n\nExtreme cases (note that these cases are not realistic in the sense of real-world applications, but are presented to demonstrate the intuition):\n\n\\(\\epsilon \\to \\infty\\)\n\nall privacy will be lost; data retains all utility, but no privacy\n\\(\\epsilon = \\infty\\) would indicate that no noise is added and the confidential data is released\n\n\\(\\epsilon \\to 0\\)\n\nno privacy is lost; data is completely distorted and no utility remains\n\\(\\epsilon = 0\\) would indicate that no data is released\n\n\n\n\n\n\n\n\n\nA key takeaway\n\n\n\nDisclosure risk can be adjusted by adjusting the privacy loss budget, but not eliminated. Adjusting the privacy loss budget is really about adjusting the strength of the privacy guarantee made by formal privacy.\n\n\n\n\n6.4.2 Who sets the privacy loss budget?\n\nThis is very much still an open question, with implications for data stewards, researchers, data users, and policymakers.\nAlthough policymakers are the most equipped to understand consequences of privacy loss, they are likely the least equipped to understand what \\(\\epsilon\\) means."
  },
  {
    "objectID": "06_introduction_to_differential_privacy.html#two-models-of-differential-privacy",
    "href": "06_introduction_to_differential_privacy.html#two-models-of-differential-privacy",
    "title": "6  Introduction to Differential Privacy and Formal Privacy",
    "section": "6.5 Two Models of Differential Privacy",
    "text": "6.5 Two Models of Differential Privacy\n\n6.5.1 Trusted Curator Model\nIn the trusted curator model, a centralized data curator that receives confidential data, creates the data products, applies the formally private method, and releases the results.\n\nIf a privacy loss budget is in place, the curator must stop releasing information when the budget is reached.If the data curator does not, then this leads to the \\(\\epsilon \\to \\infty\\) scenario, where a data intruder could make precise statistical inferences about the confidential data.\nExample: Uber created a differentially private system that allowed analysts within the company to evaluate customer experience through targeted requests without seeing confidential individual trip or rider details (Bowen and Garfinkel 2021).\n\n\n\n\n6.5.2 Local Differential Privacy\nIn the local differential privacy model (LDP), data participants or data collection points receive their own privacy loss budget instead of using a global budget that is applied to the entire confidential data. In other words, the participants add formally private noise locally to their own data before sending their information to the data curator.\n\nLDP “trusts no one”, not even the curator!\nEach individual user or data collection point receives a privacy budget \\(\\epsilon\\), rather than \\(\\epsilon\\) being applied to the entire confidential data (Bowen and Garfinkel 2021).\nSubstantially more noise is added to locally noised microdata than on data products created by a trusted curator (Bowen and Garfinkel 2021).\nExample: In 2014, Google began using a local differentially private model with Chrome web browsers to collect sensitive statistics from users; noise would be added to local browser microdata, and the resulting noised data was collected for analysis. However, the data was deemed too noisy for accurate analysis, and Google now uses a hybrid model where data is aggregated from local machines, noise is added to the aggregations, and then the noised aggregations are collected for analysis (Bowen and Garfinkel 2021).\n\n\nLDP comes from randomized response. Randomized response was developed in the 1960s to improve response accuracy for sensitive questions. RR is sometimes called the “Warner Model” (Warner 1965).\n\n\n\n\n\n\nRandomized response\n\n\n\nBinary randomized response (BRR) is a technique that introduces randomness into responses to sensitive survey questions with two possible responses, which allows for valid analysis while protecting individual responses.\nThe randomized response procedure with a coin is:\n\nAsk a yes/no question (e.g., Did you take drugs this month?).\nThe respondent flips a coin with \\(P(heads) = p\\) and \\(P(tails) = 1 - p = q\\).2\nThe respondent answers truthfully if “heads” and lies if “tails”.\n\nStatisticians can condition on the coin toss and calculate an unbiased estimate of the parameter of interest.\n\n\nSuppose we are interested in the number of people who smoke in a population.\nLet \\(n\\) be the number of respondents to the question. Let \\(n_v\\) be the true number of smokers. Let \\(p\\) be the probability of responding truthfully and \\(q\\) be the probability of responding untruthfully. Finally, let \\(\\tilde{n}\\) be the number of people who report smoking.\n\\(\\tilde{n}\\) is a biased estimator of \\(n_v\\). We need to correct for people who smoke who report as non-smokers and non-smokers who report as smokers.\n\n\n\n\n\n\nNote\n\n\n\n\\[E\\left[\\tilde{n}\\right] = n_v - n_v \\cdot q + (n - n_v) \\cdot q\\]\n\\[E\\left[\\tilde{n}\\right] = n_v \\cdot p + (n - n_v) \\cdot q\\]\n\\[E\\left[\\tilde{n}\\right] = n_v \\cdot p + n \\cdot q - n_v \\cdot q\\]\n\\[E\\left[\\tilde{n}\\right] - n \\cdot q= n_v  (p - q)\\]\n\\[\\frac{E\\left[\\tilde{n}\\right] - n \\cdot q}{  (p - q)}= n_v\\]\n\n\nAccordingly, the unbiased estimator of \\(n_v\\) is \\(\\frac{\\tilde{n} - n \\cdot q}{p - q}\\).\nLet \\(n = 100,000\\), \\(p = 0.6\\), and \\(q = 0.4\\). Suppose \\(\\tilde{n} = 44,166\\).\nThen \\(\\hat{n_v} = \\frac{44,166 - 100,000 \\cdot 0.4}{0.6 - 0.4} = 20,830\\).\nTo confirm our result, let’s simulate some data where 20% of respondents smoke:\n\nset.seed(1)\n\np &lt;- 0.6\nq &lt;- 1 - p\nn &lt;- 100000\n\ntrue_responses &lt;- sample(\n  c(\"smoker\", \"non-smoker\"),\n  size = n, \n  replace = TRUE, \n  prob = c(0.2, 0.8)\n)\n\ncoins &lt;- sample(\n  c(\"truth\", \"lie\"),\n  size = n, \n  replace = TRUE, \n  prob = c(p, q)\n)\n\nnoisy_responses &lt;- \n  case_when(\n    coins == \"lie\" & true_responses == \"non-smoker\" ~ \"smoker\",\n    coins == \"lie\" & true_responses == \"smoker\" ~ \"non-smoker\",\n    TRUE ~ true_responses\n  )\n    \n  \n(sum(noisy_responses == \"smoker\") - q * n) / (p - q)\n\n[1] 20830\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\epsilon\\)-LDP protocol\n\n\n\nConsider two probabilities \\(p &gt; q\\). A mechanism, \\(\\mathcal{M}\\), such that a user reports the true value with \\(p\\) and each of the other values with \\(q\\), satisifies \\(\\epsilon\\)-LDP if and only if \\(p \\leq q\\cdot\\exp(\\epsilon)\\) or \\(\\log(\\frac{p}{q}) \\leq \\epsilon\\).\n\n\nLet \\(v\\) be the smoking status of the individual and \\(v^*\\) be the noisy output of some mechanism \\(\\mathcal{M}\\). Then the mechanism is formally expressed as\n\\[\nP\\left[\\mathcal{M}(v) = v^*\\right] =\n\\begin{cases}\np = \\frac{\\exp(\\epsilon)}{\\exp(\\epsilon) + 1} \\text{ if } v^* = v,\\\\\nq = \\frac{1}{\\exp(\\epsilon) + 1} \\text{ if } v^* \\neq v\n\\end{cases}\n\\]\nGeneralized randomized response (GRR) generalizes BRR to questions with multiple discrete responses. BRR and GRR are sometimes called direct encoding. Uniary encoding and binary local hash are competing methods that are minor extensions of direct encoding.\n\n\n\n\n\n\nCaution\n\n\n\nLDP is not efficient. The confidentiality gains of the local model come at the cost of requiring many observations and the methods do not scale well to applications with high cardinality.\n\n\nWang et al. (2020) offers a thorough introduction to LDP."
  },
  {
    "objectID": "06_introduction_to_differential_privacy.html#exercise-2",
    "href": "06_introduction_to_differential_privacy.html#exercise-2",
    "title": "6  Introduction to Differential Privacy and Formal Privacy",
    "section": "6.6 Exercise 2",
    "text": "6.6 Exercise 2\n\nQuestionSolution\n\n\nAssume that your firm applies a formally private algorithm in the following way. Individual satellites collect location data and independently add formally private noise. The altered data is then sent to a centralized server for analysis. Which model of DP is being used?\n\nTrusted Curator\nLocal Differential Privacy\nHybrid\n\n\n\nAssume that your firm applies a formally private algorithm in the following way. Individual satellites collect location data and independently add formally private noise. The altered data is then sent to a centralized server for analysis. Which model of DP is being used?\n\nTrusted Curator\nLocal Differential Privacy\nHybrid"
  },
  {
    "objectID": "06_introduction_to_differential_privacy.html#exercise-3",
    "href": "06_introduction_to_differential_privacy.html#exercise-3",
    "title": "6  Introduction to Differential Privacy and Formal Privacy",
    "section": "6.7 Exercise 3",
    "text": "6.7 Exercise 3\n\nQuestionSolution\n\n\nAssume that in the previous example, executives at your company decide to increase the epsilon (privacy loss budget) used in the differentially private mechanism. You can expect the newly released data to be:\n\nMore noisy than before\nLess noisy than before\n\n\n\nAssume that in the previous example, executives at your company decide to increase the epsilon (privacy loss budget) used in the differentially private mechanism. You can expect the newly released data to be:\n\nMore noisy than before\nLess noisy than before\n\n\n\n\n\n\n\n\nBowen, Claire M, and Simson Garfinkel. 2021. “Philosophy of Differential Privacy.” Notices of the American Mathematical Society 68 (10): 1727–39.\n\n\nBureau, US Census. 2021. “Disclosure Avoidance for the 2020 Census: An Introduction.” US Government Publishing Office Washington, DC.\n\n\nWang, Teng, Xuefeng Zhang, Jingyu Feng, and Xinyu Yang. 2020. “A Comprehensive Survey on Local Differential Privacy Toward Data Statistics and Analysis.” Sensors 20 (24): 7030. https://doi.org/10.3390/s20247030.\n\n\nWarner, Stanley L. 1965. “Randomized Response: A Survey Technique for Eliminating Evasive Answer Bias.” Journal of the American Statistical Association 60 (309): 63–69. https://doi.org/10.1080/01621459.1965.10480775."
  },
  {
    "objectID": "06_introduction_to_differential_privacy.html#footnotes",
    "href": "06_introduction_to_differential_privacy.html#footnotes",
    "title": "6  Introduction to Differential Privacy and Formal Privacy",
    "section": "",
    "text": "“Consistency of data products and formally private methods for the 2020 census,” US Census Bureau, p. 43, https://irp.fas.org/agency/dod/jason/census-privacy.pdf↩︎\nThe coin doesn’t need to be fair. In fact, the fairness of the coin is a parameter we can use to tune the utility-disclosure risk tradeoff.↩︎"
  },
  {
    "objectID": "07_differential_privacy_details.html#recap",
    "href": "07_differential_privacy_details.html#recap",
    "title": "7  Formally Private Definitions, Fundamental Mechanisms, and Algorithms",
    "section": "7.1 Recap",
    "text": "7.1 Recap\n\n7.1.1 Formal privacy definitions\nIn general, formally private methods have the following features (Bowen and Garfinkel 2021):\n\nAbility to quantify and adjust the privacy-utility trade-off, typically through parameters.\nAbility to rigorously and mathematically prove the maximum privacy loss that can result from the release of information.\nFormal privacy definitions also allow one to compose multiple statistics. In other words, a data curator can compute the total privacy loss from multiple individual information releases.\n\n\n\n7.1.2 Privacy loss budget\nDifferential privacy and other formal privacy uses the concept of a privacy loss budget, typically represented mathematically as \\(\\epsilon\\). The privacy loss budget bounds the privacy risk associated with releasing data or query results.\n(Note: \\(\\epsilon\\) is not the only privacy loss parameter, but we will use it here as a general representation of the privacy loss budget.)\n\nA larger value of \\(\\epsilon\\) increases the maximum disclosure risk (the upper bound of the disclosure risk) associated with a given release of information.\n\nlarger \\(\\epsilon\\) = less noise added to data = more accuracy, but less privacy\nsmaller \\(\\epsilon\\) = more noise added to data = less accuracy, but more privacy\n\nExtreme cases (note that these cases are not realistic in the sense of real-world applications, but are presented to demonstrate the intuition):\n\n\\(\\epsilon \\to \\infty\\)\n\nall privacy will be lost; data retains all utility, but no privacy\n\n\\(\\epsilon \\to 0\\)\n\nno privacy is lost; data is completely distorted and no utility remains"
  },
  {
    "objectID": "07_differential_privacy_details.html#formal-privacy-features",
    "href": "07_differential_privacy_details.html#formal-privacy-features",
    "title": "7  Formally Private Definitions, Fundamental Mechanisms, and Algorithms",
    "section": "7.2 Formal Privacy Features",
    "text": "7.2 Formal Privacy Features\nFormal privacy is a relatively new set of definitions for quantifying the worst-case amount of information disclosed from statistics calculated on a private database. We provide conceptual and mathematical definitions below.\n\n7.2.1 \\(\\epsilon\\)-Differential privacy\n\nAssumptions Underlying Privacy GuaranteeMathematical Definition\n\n\nFormal privacy does not make assumptions about:\n\nhow a data intruder will attack the data;\nthe amount of external information or computing power an intruder has access to, now or in the future;\nwhich information in the data poses a higher disclosure risk (Near 2020).\n\nInstead, formal privacy assumes the worst-case scenario:\n\nthe intruder has information on every observation except one;\nthe intruder has unlimited computational power;\nmissing observation is the most extreme possible observation (or an extreme outlier) that could alter the statistic.\n\n\n\nWe mathematically define several formally private definitions and key theorems. We use the following notation: \\(X\\in\\mathbb{R}^{n\\times r}\\) is the confidential data set representing \\(n\\) data points and \\(r\\) variables and \\(M:\\mathbb{R}^{n\\times r}\\rightarrow\\mathbb{R}^k\\) denotes the statistical query, i.e., \\(M\\) is a function mapping \\(X\\) to \\(k\\) real numbers. We denote a randomized or noisy version of \\(M\\) using \\(\\mathcal{M}:\\mathbb{R}^{n\\times r}\\rightarrow\\mathbb{R}^k\\), which is a function that satisfies a formally private definition.\nDP is the most widely known formal privacy definition. Privacy experts often refer to the original definition of DP as pure-DP or \\(\\epsilon\\)-DP.\nDifferential Privacy (Dwork, McSherry, et al. 2006): A sanitization algorithm, \\(\\mathcal{M}\\), satisfies \\(\\epsilon\\)-DP if for all subsets \\(S\\subseteq Range(\\mathcal{M})\\) and for all \\(X,X'\\) such that \\(d(X,X')=1\\), \\[\\begin{equation}\\label{eqn:dp}\n      \\frac{\\Pr(\\mathcal{M}( X) \\in S)}{ \\Pr(\\mathcal{M}( X')\\in S)}\\le \\exp(\\epsilon)\n  \\end{equation}\\] where \\(\\epsilon&gt;0\\) is the privacy loss budget and \\(d(X,X')=1\\) represents the possible ways that \\(X'\\) differs from \\(X\\) by one record.\n\n\n\n\n\n\n\n\n\nFeatures to note\n\n\n\n\n\\(\\epsilon\\) is logarithmic.\nThis is an inequality, not an equation; \\(\\epsilon\\) is up to us to define and represents an upper bound for disclosure risk that we are comfortable with for our particular data.\n\n\n\n\n\n7.2.2 Global Sensitivity\nIn addition to the privacy loss budget, most formally private methods rely on the concept called global sensitivity, which describes how resistant the formally private sanitizer is to the presence of outliers (Bowen and Garfinkel 2021). We can think of the global sensitivity as another value that helps determine how much noise is needed to protect the released data or statistic, because some information is more sensitive than other information to outliers.\nImagine the data we want to protect contains socioeconomic information and the question we want answered is, “What is the median wealth?” Under formal privacy, we must consider the change of the most extreme possible record that could exist in any given data that has demographic and financial information. In our example, that person is Elon Musk, who was the wealthiest person in the world in 2023.1 If Musk is present or absent in the data, the median wealth should not change too much. This means we can provide a more accurate answer by applying fewer alterations to the median income statistic, because it is less sensitive to the extreme outlier, Musk Consider, however, the question, “What is the average wealth?” Unlike the previous statistic, the answer would significantly change if Musk were present or absent from the data. To protect the extreme case at a given level of privacy loss, a formally private algorithm would need to provide a significantly less accurate answer by altering the statistic more.\n\n\n7.2.3 \\(l_1\\)-Global Sensitivity\n\nConceptualTechnical\n\n\n\\(l_1\\)-Global Sensitivity (Dwork, McSherry, et al. 2006): The \\(l_1\\)-global sensitivity calculates the maximum amount a statistic can change in absolute value terms with the addition or removal of the most extreme possible observation.\n\n\n\\(l_1\\)-Global Sensitivity (Dwork, McSherry, et al. 2006): For all \\(X,X'\\) such that \\(d(X,X')=1\\), the global sensitivity of a function \\(M\\) is \\[\\begin{equation}\\label{eqn:gs}\n        \\Delta_1 (M)= \\underset{d(X,X')=1}{\\text{sup}} \\|M(X)-M(X') \\|_1\n    \\end{equation}\\]\nFor scalars, the \\(l_1\\)-Global Sensitivity is \\(|M(X) - M(X')|\\).\n\n\n\n\n\n7.2.4 \\(l_2\\)-Global Sensitivity\n\nConceptualTechnical\n\n\n\\(l_2\\)-Global Sensitivity (Dwork, McSherry, et al. 2006): \\(l_2\\)-global sensitivity calculates the maximum amount a statistic can change when the statistic is squared, summed, and rooted with the addition or removal of the most extreme possible observation.\n\n\n\\(l_2\\)-Global Sensitivity (Dwork, McSherry, et al. 2006): For all \\(X,X'\\) such that \\(d(X,X')=1\\), the global sensitivity of a function \\(M\\) is\n\\[\\Delta_2 (M)= \\underset{d(X,X')=1}{\\text{sup}} \\|M(X)-M(X') \\|_2\\] For scalars, the \\(l_2\\)-Global Sensitivity is \\(\\sqrt{(M(X) - M(X'))^2}\\).\n\n\n\n\n\n\n\n\n\nImportant note\n\n\n\nGlobal sensitivity is straightforward but calculating the global sensitivity for some statistics can be very difficult. For instance, we cannot calculate a finite global sensitivity of sample mean if we do not bound the variable."
  },
  {
    "objectID": "07_differential_privacy_details.html#exercise-1",
    "href": "07_differential_privacy_details.html#exercise-1",
    "title": "7  Formally Private Definitions, Fundamental Mechanisms, and Algorithms",
    "section": "7.3 Exercise 1",
    "text": "7.3 Exercise 1\n\nQuestionHintSolution\n\n\nSuppose we are interested in counting the number of sole proprietorships in Washington, DC. What are the \\(l_1\\) and \\(l_2\\) global sensitivities of this statistic?\n\n\nIn other words, what is the maximum difference between \\(M(X)\\) and \\(M(X')\\) when \\(d(X,X')=1\\)?\n\n\nThe answer is one. The most a count can change by adding or subtracting one observation is one!\n\\(\\Delta_1 (M) = \\Delta_2 (M) = 1\\)"
  },
  {
    "objectID": "07_differential_privacy_details.html#exercise-2",
    "href": "07_differential_privacy_details.html#exercise-2",
    "title": "7  Formally Private Definitions, Fundamental Mechanisms, and Algorithms",
    "section": "7.4 Exercise 2",
    "text": "7.4 Exercise 2\n\nQuestionHintSolution\n\n\nSuppose we are interested in calculating the total income of sole proprietorships in Washington, DC. What are the \\(l_1\\) and \\(l_2\\) global sensitivities of this statistic?\n\n\nIn other words, what is the maximum difference between \\(M(X)\\) and \\(M(X')\\) when \\(d(X,X')=1\\)?\n\n\nThe answer is \\(\\infty\\). A total can theoretically change by any amount with the addition or deletion of one observation."
  },
  {
    "objectID": "07_differential_privacy_details.html#statistics",
    "href": "07_differential_privacy_details.html#statistics",
    "title": "7  Formally Private Definitions, Fundamental Mechanisms, and Algorithms",
    "section": "7.5 Statistics",
    "text": "7.5 Statistics\n\n7.5.1 Counts\nCounts are the best explored statistics in differential privacy. With unbounded differential privacy, the global sensitivity of a count is always 1.\nFor example, assume we are counting the number of billionaires in the United States. The most the count can change with the addition or removal of Elon Musk is one.\n\n\n7.5.2 Sums\nCalculating the global sensitivity is more difficult for other statistics than counts. The global sensitivity of a sum is unbounded because the addition or removal of one row can theoretically change the sum by any amount.\nOne approach is to clip or truncate values. If we assume that all observations are between 6 and 10, inclusive, then we can treat the global sensitivity as \\(10 - 6 = 4\\).\n\nDifferential privacy does not hold if we look at the data to determine the bounds.\n\nBounds that truncate actual values bias statistics.\nThis assumption is often problematic with economic data where distributions can be highly skewed.\n\n\n\n7.5.3 Means\nMeans can be rewritten as two queries: a total divided by a count.\n\nGS(sum) / GS(count)\n\nSometimes the number of observations is assumed to be known. In this case, the global sensitivity is smaller.\n\nGS(sum) / n if we assume n is known"
  },
  {
    "objectID": "07_differential_privacy_details.html#dp-sanitizers",
    "href": "07_differential_privacy_details.html#dp-sanitizers",
    "title": "7  Formally Private Definitions, Fundamental Mechanisms, and Algorithms",
    "section": "7.6 DP Sanitizers",
    "text": "7.6 DP Sanitizers\nA sanitizer protects against disclosure risk. A differentially private sanitizer protects against disclosure risk and meets the definition of differential privacy. If we know the global sensitivity of statistics, then we can often add noise in a way that sanitizers satisfy differential privacy. We review three fundamental formally private sanitizers. We call these sanitizers fundamental because they are the original formally private sanitizers that privacy researchers still often use as building blocks for more sophisticated formally private methods.\n\n7.6.1 Laplace sanitizer\nDwork, McSherry, et al. (2006) first proposed protecting statistics by adding noise from a Laplace distribution, called the Laplace mechanism, but we can think of it as a Laplace sanitizer. The Laplace distribution is centered at zero and the distribution variability is the ratio of the privacy loss budget, \\(\\epsilon\\), over the \\(l_1\\)-global sensitivity of the statistic. Since the distribution is centered at zero, there is a higher probability of adding very little or no noise to the statistic. For the noise variability, if \\(\\epsilon\\) is large or the sensitivity of the statistic is low, then there is a higher probability of adding very little noise to the confidential data statistic. If \\(\\epsilon\\) is small or the sensitivity of the statistic is high, then there is a higher probability of adding a lot of noise to the statistic.\n\nConceptualTechnical\n\n\nThe Laplace sanitizer satisfies \\(\\epsilon\\)-DP by adding noise from a Laplace distribution to statistics. More sensitivity means more expected noise. More \\(\\epsilon\\) means less expected noise.\n\n\n\n\n\n\n\n\n\n\n\n\nLaplace Mechanism (Dwork, McSherry, et al. 2006): Given any function \\(M:\\mathbb{R}^{n\\times r}\\rightarrow\\mathbb{R}^k\\), the Laplace mechanism is defined as: \\[\\begin{equation}\\label{eqn:lap}\n        \\mathcal{M}(X)=M(X)+(\\eta_1,\\ldots,\\eta_k).\n    \\end{equation}\\] where \\((\\eta_1,\\ldots,\\eta_k)\\) are i.i.d. \\(Laplace(0, \\frac{\\Delta_1(M)}{\\epsilon})\\).\n\n\n\n\n\n7.6.2 Laplace sanitizer Example\nLet’s consider a simple example with the Palmer Penguins data set. The data set contains 333 observations about Adelie, Chinstrap, and Gentoo penguins in Antarctica. Suppose we want to count how many penguins are Adelie penguins.\n\npenguins &lt;- palmerpenguins::penguins |&gt;\n  drop_na()\n\npenguins\n\n# A tibble: 333 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           36.7          19.3               193        3450\n 5 Adelie  Torgersen           39.3          20.6               190        3650\n 6 Adelie  Torgersen           38.9          17.8               181        3625\n 7 Adelie  Torgersen           39.2          19.6               195        4675\n 8 Adelie  Torgersen           41.1          17.6               182        3200\n 9 Adelie  Torgersen           38.6          21.2               191        3800\n10 Adelie  Torgersen           34.6          21.1               198        4400\n# ℹ 323 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nThe global sensitivity is \\(\\frac{\\Delta_1(M)}{\\epsilon} = \\frac{1}{\\epsilon}\\). This means our formally private statistic is one draw from a Laplace distribution with center at the confidential statistics and scale parameter equal to \\(\\frac{1}{\\epsilon}\\).\nBelow is code for drawing values from a Laplace distribution, which we will call laplace_sanitizer().\n\n# function to draw Laplace noise for one statistic \nlaplace_sanitizer &lt;- function(sensitivity, epsilon, n = 1) {\n  \n  # lambda (distribution width) is sensitivity/privacy loss budget\n  l &lt;- sensitivity / epsilon \n  \n  # draw from Laplace distribution\n  noise &lt;- smoothmest::rdoublex(n = n, # draw one observation (adding noise to one statistic)\n                                mu = 0, # centered at 0\n                                lambda = l) # scale based on l calculated above\n  \n  return(noise)\n  \n}\n\nLet’s calculate the statistic without any noise.\n\nanswer_conf &lt;- sum(penguins$species == \"Adelie\")\n\nanswer_conf\n\n[1] 146\n\n\nNow, let’s calculate the statistic with noise that satisfies the definition of \\(\\epsilon\\)-differential privacy.\n\nset.seed(1)\nanswer_dp &lt;- answer_conf + laplace_sanitizer(sensitivity = 1, epsilon = 0.1)\n\nanswer_dp\n\n[1] 153.5518\n\n\nMaybe we got a lucky or unlucky draw from the Laplace distribution. Let’s calculate this statistic 100 times to understand the distribution of noisy statistics. This is purely for demonstration to understand the expectation of the noisy statistic.\n\n\n\n\n\n\n\n7.6.3 Gaussian Sanitizer\nSimilar to the Laplace mechanism, the Gaussian mechanism adds random noise from a Gaussian distribution. The Gaussian distribution is also centered at zero and the distribution variability is the ratio of the privacy loss budget, \\(\\epsilon\\), over the \\(l_2\\)-global sensitivity of the statistic (mathematically, the Gaussian mechanism does not satisfy \\(l_1\\)-global sensitivity).\n\nConceptualTechnical\n\n\nThe Gaussian sanitizer satisfies \\((\\epsilon,\\delta)\\)-DP by adding noise from a Gaussian distribution (also known as Normal distribution or bell curve) to statistics. More sensitivity means more expected noise. More \\(\\epsilon\\) means less expected noise. More \\(\\delta\\) means less expected noise.\n\n\n\n\n\n\n\n\n\n\n\n\nGaussian Mechanism (Dwork and Roth 2014): The Gaussian Mechanism satisfies \\((\\epsilon,\\delta)\\)-DP by adding Gaussian noise with zero mean and variance, \\(\\sigma^2\\), such that\n\\[\\mathcal{M}(X)=M(X)+(\\eta_1,\\ldots,\\eta_k)\\]\nwhere \\(\\eta_1,\\ldots,\\eta_k\\) are independently drawn and \\(\\sigma=\\frac{\\Delta_2(M)\\sqrt{2 \\log(1.25/\\delta)}}{\\epsilon}\\).\nThis sanitizer includes two parameters: \\(\\epsilon\\) and \\(\\delta\\). We can think of \\(\\delta\\) as a small probability that the bound created by \\(\\epsilon\\) does not hold.\nThe Gaussian sanitizer uses \\(l_2\\)-Global Sensitivity.\n\n\n\n\n\n7.6.4 Gaussian Sanitizer Example\nWe repeat the last example, except this time using the Gaussian sanitizer.\n\n# function to draw Gaussian noise for one statistic \ngaussian_sanitizer &lt;- function(sensitivity, epsilon, delta) {\n  \n  # lambda (distribution width) is sensitivity/privacy loss budget\n  sigma &lt;- (sensitivity * sqrt(2 * log(1.25 / delta))) / epsilon \n  \n  # draw from Gaussian distribution\n  noise &lt;- rnorm(n = 1, # draw one observation (adding noise to one statistic)\n                 mean = 0,\n                 sd = sigma) # scale based on l calculated above\n  \n  return(noise)\n  \n}\n\nLet’s calculate the statistic without any noise.\n\nanswer_conf &lt;- sum(penguins$species == \"Adelie\")\n\nanswer_conf\n\n[1] 146\n\n\nNow, let’s calculate the statistic with noise that satisfies the definition of \\((\\epsilon, \\delta)\\)-differential privacy.\n\nset.seed(1)\nanswer_dp &lt;- answer_conf + gaussian_sanitizer(sensitivity = 1, epsilon = 0.1, delta = 10^-7)\n\nanswer_dp\n\n[1] 110.1865\n\n\nMaybe we got a lucky or unlucky draw from the Normal distribution. Let’s calculate this statistic 100 times to understand the distribution of noisy statistics. This is purely for demonstration to understand the expectation of the noisy statistic.\n\n\n\n\n\nThe Gaussian sanitizer is worse than the Laplace sanitizer! So why do we even need a Gaussian sanitizer?\nThe Gaussian sanitizer can compose better for multiple queries. This is because the sum of two normally distributed random variables is normally distributed but the sum of two Laplacian distributed variables is not Laplacian."
  },
  {
    "objectID": "07_differential_privacy_details.html#important-theorems",
    "href": "07_differential_privacy_details.html#important-theorems",
    "title": "7  Formally Private Definitions, Fundamental Mechanisms, and Algorithms",
    "section": "7.7 Important Theorems",
    "text": "7.7 Important Theorems\nAs mentioned earlier, formal privacy requires methods to compose or account for the total privacy loss from each public data release or statistic. For example, composition or accounting allows the data curator to track the total privacy loss from multiple summary tables or multiple statistics requests from several data users. This is the main advantage of formal privacy compared to traditional SDC methods, which cannot quantify the total privacy loss. There are two main composition theorems: sequential and parallel. We also cover another important theorem (post-processing) that is essential in developing formally private methods.\n\n7.7.1 Sequential Composition Theorem\nThe sequential composition theorem allows the data users to calculate the privacy loss budget from multiple noisy statistics on the same part of the confidential data (Bun and Steinke 2016; McSherry 2009).\n\n\nFigure 7.1: An Example of Sequential Composition\n\n\n\nTo help explain this concept, suppose we have establishment economic data set that reports the state of operation, the number of employees, and the average income for each establishment. We want to conduct three different analyses that cost \\(\\epsilon_1=1\\), \\(\\epsilon_2=0.5\\), and \\(\\epsilon_3=0.5\\), respectively. Since we are applying the three analyses on the entire data, sequential composition requires us to add up the individual privacy loss budgets for the total. i.e., \\(\\epsilon_{total}=\\epsilon_1+\\epsilon_2+\\epsilon_3=2\\). Figure 7.1 shows the application of sequential composition to our fictitious economic data set.\nMathematically, a mechanism, \\(\\mathcal{M}_j\\), provides \\(\\epsilon_j\\)-DP. The sequence of \\(\\mathcal{M}_j(X)\\) applied on the same \\(X\\) provides \\(\\sum_{j=1}^J\\epsilon_j\\)-DP.\n\n\n7.7.2 Sequential Composition Theorem Example\nLet’s return the penguins example from above. Suppose \\(\\epsilon = 1\\) and we want to count the number of “Adelie” penguins and the number of “Chinstrap” penguins.\n\nepsilon &lt;- 1\n\nset.seed(20220505)\n\nsum(penguins$species == \"Adelie\") + laplace_sanitizer(sensitivity = 1, epsilon = epsilon / 2)\n\n[1] 146.7052\n\nsum(penguins$species == \"Chinstrap\") + laplace_sanitizer(sensitivity = 1, epsilon = epsilon / 2)\n\n[1] 78.36747\n\n\nFor reference, let’s look at the truth.\n\nsum(penguins$species == \"Adelie\")\n\n[1] 146\n\nsum(penguins$species == \"Chinstrap\")\n\n[1] 68\n\n\n\n\n7.7.3 Parallel Composition Theorem\nThe parallel composition theorem allows data users to calculate the privacy loss budget from multiple noisy statistics on different or disjoint parts of the confidential data (Bun and Steinke 2016; McSherry 2009).\n\n\nFigure 7.2: An Example of Parallel Composition\n\n\n\nUsing the same example as before in Figure 7.1, suppose we apply three analyses to partitions of the data (i.e., the three different states) that cost \\(\\epsilon_1=1\\), \\(\\epsilon_2=0.5\\), and \\(\\epsilon_3=0.5\\), respectively. Since we are applying the three analyses on disjoint subsets of the data, parallel composition states that the total privacy loss budget is the maximum privacy loss budget of the three analyses. i.e., \\(\\epsilon_{total}=\\max(\\epsilon_1,\\epsilon_2,\\epsilon_3)=1\\). Figure 7.2 shows the application of sequential composition to our fictitious economic data set.\nMathematically, let \\(D_j\\) be disjoint subsets of the input domain \\(D\\). The sequence of \\(\\mathcal{M}_j(X\\cap D_j)\\) provides \\(\\max_{j \\in \\{1,\\ldots,J\\}} \\epsilon_j\\)-DP\n\n\n7.7.4 Parallel Composition Theorem Example\nLet’s consider a larger data set with 53,940 observations about diamonds. Suppose we want to calculate a differenitally private histogram of diamond sizes with bins [0, 1], (1, 2], (2, 3], (3, 4], (4, 5], and (5,6] with \\(\\epsilon = 0.1\\).\n\ndiamonds_conf &lt;- count(diamonds, carat = ceiling(carat))\n\ndiamonds_conf\n\n# A tibble: 6 × 2\n  carat     n\n  &lt;dbl&gt; &lt;int&gt;\n1     1 36438\n2     2 15613\n3     3  1857\n4     4    27\n5     5     4\n6     6     1\n\n\nOne approach is to use \\(\\frac{\\epsilon = 0.1}{6}\\) for each of the six counting queries. This is based on sequential composition.\n\nepsilon &lt;- 0.1\n\nset.seed(10)\n\ndiamonds_conf &lt;- bind_cols(\n  diamonds_conf,\n  sequential = diamonds_conf$n + laplace_sanitizer(sensitivity = 1, epsilon = epsilon / 6, n = 6)\n)\n\ndiamonds_conf\n\n# A tibble: 6 × 3\n  carat     n sequential\n  &lt;dbl&gt; &lt;int&gt;      &lt;dbl&gt;\n1     1 36438    36437. \n2     2 15613    15558. \n3     3  1857     1902. \n4     4    27      -67.5\n5     5     4       17.9\n6     6     1       66.2\n\n\nThe bins for carat partition the data set and each bin is a disjoint subset of the data. Therefore, we can use parallel composition and get more accurate differentially private counts!\n\nset.seed(11)\n\ndiamonds_conf &lt;- bind_cols(\n  diamonds_conf,\n  parallel = diamonds_conf$n + laplace_sanitizer(sensitivity = 1, epsilon = epsilon, n = 6)\n)\n\ndiamonds_conf\n\n# A tibble: 6 × 4\n  carat     n sequential parallel\n  &lt;dbl&gt; &lt;int&gt;      &lt;dbl&gt;    &lt;dbl&gt;\n1     1 36438    36437.  36446.  \n2     2 15613    15558.  15683.  \n3     3  1857     1902.   1857.  \n4     4    27      -67.5    69.0 \n5     5     4       17.9    28.6 \n6     6     1       66.2     9.53\n\n\n\n\n7.7.5 Post-Processing Theorem\nAnother important theorem is the post-processing theorem that allows the continued use of formally private information without losing the privacy guarantee (Bun and Steinke 2016; Dwork, McSherry, et al. 2006; Nissim, Raskhodnikova, and Smith 2007). In other words, if someone modifies a formally private data set or statistic without using additional information from the confidential data, then that data set or statistic is still formally private.\nFor example, if the number of employees from a formally private method said there are 3.7 employees, then we could round that value to 4 without leaking more information. Simply put, the post-processing theorem makes the data usable after formally private noise is added.\nMathematically, if \\(\\mathcal{M}\\) is a sanitizer that satisfies \\(\\epsilon\\)-DP and \\(g\\) is any function, then \\(g\\left(\\mathcal{M}(X)\\right)\\) also satisfies \\(\\epsilon\\)-DP.\nPost-processing also provides the opportunity to improve utility. Data stewards can use available public or expert knowledge to reduce the amount of noise without accruing additional privacy loss. The public information can come from data released without formal privacy or from individuals who are comfortable sharing their information without noise. Rounding and eliminating impossible values like negative counts are common types of post-processing. There are also types of post-processing that can improve accuracy by leveraging information calculated from the same data set.\n\n\n\n\n\n\nA key takeaway\n\n\n\nFormal privacy is transparent and allows users to account for the noise introduced into statistics. Post-processing can give up some of this transparency and make it more difficult to account for the noise added to statistics."
  },
  {
    "objectID": "07_differential_privacy_details.html#exercise-3",
    "href": "07_differential_privacy_details.html#exercise-3",
    "title": "7  Formally Private Definitions, Fundamental Mechanisms, and Algorithms",
    "section": "7.8 Exercise 3",
    "text": "7.8 Exercise 3\nConsider a simulated data set with information about small businesses (0-20 employees) in Texas and Vermont.\n\nset.seed(20220509)\nsmall_businesses &lt;- bind_rows(\n  Texas = tibble(\n    employees = rpois(n = 100010, lambda = 10),\n    income = rlnorm(n = 100010, meanlog = 10, sdlog = 2)\n  ),\n  Vermont = tibble(\n    employees = rpois(n = 403, lambda = 10),\n    income = rlnorm(n = 403, meanlog = 10, sdlog = 2)\n  ),\n  .id = \"state\"\n) |&gt;\n  mutate(employees = if_else(employees &gt; 20, 20L, employees))\n\n\nQuestionHint\n\n\nUsing the Laplace sanitizer, calculate the number of small businesses in Texas and Vermont (count) with the overall \\(\\epsilon = 0.1\\). Use the parallel composition theorem.\n\nex3_conf &lt;- count(small_businesses, state)\n\nex3_conf\n\nset.seed(46)\nbind_cols(\n  ex3_conf,\n  ex3_conf$n + laplace_sanitizer(\n    sensitivity = ### ______, \n    epsilon = ### ______, \n    n = 2\n  )\n)\n\n\nWhich state has more absolute error introduced into its count?\nWhich state has more relative error introduced into its count?\n\n\n\nThe observations from Texas and Vermont are disjoint, so we can use the full \\(\\epsilon = 0.1\\) for each statistics instead of splitting it across the statistics.\n\nex3_conf &lt;- count(small_businesses, state)\n\nex3_conf\n\n# A tibble: 2 × 2\n  state        n\n  &lt;chr&gt;    &lt;int&gt;\n1 Texas   100010\n2 Vermont    403\n\nset.seed(46)\nbind_cols(\n  ex3_conf,\n  n_dp = ex3_conf$n + laplace_sanitizer(\n    sensitivity = 1, \n    epsilon = 0.1, \n    n = 2\n  )\n)\n\n# A tibble: 2 × 3\n  state        n    n_dp\n  &lt;chr&gt;    &lt;int&gt;   &lt;dbl&gt;\n1 Texas   100010 100029.\n2 Vermont    403    418.\n\n\nThe absolute error is larger for Texas, but the relative error is much bigger for Vermont."
  },
  {
    "objectID": "07_differential_privacy_details.html#exercise-4",
    "href": "07_differential_privacy_details.html#exercise-4",
    "title": "7  Formally Private Definitions, Fundamental Mechanisms, and Algorithms",
    "section": "7.9 Exercise 4",
    "text": "7.9 Exercise 4\n\nQuestionHint\n\n\nUsing the Laplace sanitizer, calculate the number of employees in the entire data set (sum) with the overall \\(\\epsilon = 0.1\\). We know from auxiliary information that the number of employees varies from 0 to 20 because they are small businesses.\n\nex4_conf &lt;- small_businesses |&gt;\n  summarize(employees = sum(employees))\n\nset.seed(47)\nbind_cols(\n  ex4_conf,\n  employees_dp = ex4_conf$employees + laplace_sanitizer(\n    sensitivity = ### ______, \n    epsilon = ### ______, \n    n = 1\n  )\n)\n\n\n\n\nex4_conf &lt;- small_businesses |&gt;\n  summarize(employees = sum(employees))\n\nset.seed(47)\nbind_cols(\n  ex4_conf,\n  employees_dp = ex4_conf$employees + laplace_sanitizer(\n    sensitivity = 20, \n    epsilon = 0.1, \n    n = 1\n  )\n)\n\n# A tibble: 1 × 2\n  employees employees_dp\n      &lt;int&gt;        &lt;dbl&gt;\n1   1003755     1003807."
  },
  {
    "objectID": "07_differential_privacy_details.html#other-formal-privacy-definitions",
    "href": "07_differential_privacy_details.html#other-formal-privacy-definitions",
    "title": "7  Formally Private Definitions, Fundamental Mechanisms, and Algorithms",
    "section": "7.10 Other Formal Privacy Definitions",
    "text": "7.10 Other Formal Privacy Definitions\n\n7.10.1 Approximate Differential Privacy\nApproximate Differential Privacy, also known as \\((\\epsilon, \\delta)\\)-Differential Privacy is a relxation of \\(\\epsilon\\)-Differential Privacy. We saw this definition above with the Gaussian sanitizer.\n\\((\\epsilon, \\delta)\\)-Differential Privacy (Dwork, Kenthapadi, et al. 2006): A sanitization algorithm, \\(\\mathcal{M}\\), satisfies \\((\\epsilon, \\delta)\\)-DP if for all \\(X, X'\\) that are \\(d(X,X')=1\\),\n\\[\\Pr(\\mathcal{M}( X) \\in S)\\le \\exp(\\epsilon) \\Pr(\\mathcal{M}( X')\\in S) + \\delta\\] where \\(\\delta\\in [0,1]\\).\nWe can think of \\(\\delta\\) as a small probability that the bound created by \\(\\epsilon\\) does not hold. \\(\\epsilon\\)-DP is a special case of \\((\\epsilon, \\delta)\\)-DP when \\(\\delta=0\\).\n\n\n7.10.2 Zero-Concentrated Differential Privacy\nZero-Concentrated Differential Privacy is another relaxation of \\(\\epsilon\\)-Differential Privacy. This definition is used by the Census Bureau for the 2020 Decennial Census.\nZero-Concentrated Differential Privacy (Bun and Steinke 2016): A sanitization algorithm, \\(\\mathcal{M}\\), satisfies \\((\\xi, \\rho)\\)-zero-concentrated differential privacy if for all \\(X, X'\\) that are \\(d(X,X')=1\\) and \\(\\alpha\\in (1, \\infty)\\),\n\\[D_\\alpha(\\mathcal{M}(X)||\\mathcal{M}(X'))\\leq\\xi+\\rho\\alpha\\]\nwhere \\(D_\\alpha(\\mathcal{M}(X)||\\mathcal{M}(X'))\\) is the \\(\\alpha\\)-R'enyi divergence % between the distribution of \\(\\mathcal{M}(X)\\) and the distribution of \\(\\mathcal{M}(X')\\), \\(\\xi\\) and \\(\\rho\\) are positive constants, and \\(\\alpha \\in (1,\\infty)\\).\nZero-Concentrated Differential Privacy, also known as R'enyi Differential Privacy, only holds for counts."
  },
  {
    "objectID": "07_differential_privacy_details.html#unpacking-epsilon",
    "href": "07_differential_privacy_details.html#unpacking-epsilon",
    "title": "7  Formally Private Definitions, Fundamental Mechanisms, and Algorithms",
    "section": "7.11 Unpacking \\(\\epsilon\\)",
    "text": "7.11 Unpacking \\(\\epsilon\\)\nDifferential privacy states that the log of the ratio of the probability that any individual observation was in the data that generated the output vs. not in the data that generated the output is bounded by the value of \\(\\epsilon\\).\n\\[\\frac{\\Pr(\\mathcal{M}( X) \\in S)}{ \\Pr(\\mathcal{M}( X')\\in S)}\\le \\exp(\\epsilon)\\]\nThe bound is in exponential units, so modest increases in \\(\\epsilon\\) correspond with large increases in the ratio of the probabilities.\nEarly differential privacy researchers thought \\(\\epsilon = 1\\) or \\(\\epsilon = 2\\) were upper bounds on \\(\\epsilon\\). Today, much higher values of \\(\\epsilon\\) are used. The April 2021 Decennial Census demonstration data used \\(\\epsilon = 4.0\\) and \\(\\epsilon = 10.3\\) for the person-level file. The Decennial Census ended up using \\(\\epsilon = 17.14\\) for the person-level file and \\(\\epsilon = 2.47\\) for the housing unit data, with \\(\\delta = 10^{−10}\\) for each.\nLet’s consider the ratios of the probabilities for different values of \\(\\epsilon\\):\n\n\n# A tibble: 10 × 2\n   epsilon    ratio\n     &lt;dbl&gt;    &lt;dbl&gt;\n 1    0.25        1\n 2    0.5         2\n 3    0.75        2\n 4    1           3\n 5    2           7\n 6    4          55\n 7    6         403\n 8    8        2981\n 9   10.3     29733\n10   17.1  27784809\n\n\nIt is tough to reason what a ratio of 27784809 even means."
  },
  {
    "objectID": "07_differential_privacy_details.html#key-takeaways",
    "href": "07_differential_privacy_details.html#key-takeaways",
    "title": "7  Formally Private Definitions, Fundamental Mechanisms, and Algorithms",
    "section": "7.12 Key Takeaways",
    "text": "7.12 Key Takeaways\n\nDifferential privacy places a bound on the amount of information released under extreme assumptions about the knowledge of an attacker and their computing power.\nGlobal sensitivity measures how much a statistic can change with the addition or removal of the most extreme possible value.\nSanitizers, like the Laplace sanitizer, satisfy differential privacy by adding a specific amount of random noise to statistics.\nHigher values of \\(\\epsilon\\) mean more information is potentially released.\nSanitizers applied to statistics with higher global sensitivity require more noise to satisfy a definition of differential privacy than with statistics with lower global sensitivity."
  },
  {
    "objectID": "07_differential_privacy_details.html#bonus-exercises",
    "href": "07_differential_privacy_details.html#bonus-exercises",
    "title": "7  Formally Private Definitions, Fundamental Mechanisms, and Algorithms",
    "section": "7.13 Bonus Exercises",
    "text": "7.13 Bonus Exercises\n\n7.13.1 Exercise 5\n\nQuestionSolution\n\n\nThe Laplace sanitizer uses l2-global sensitivity.\n\nTrue\nFalse\n\n\n\nThe Laplace sanitizer uses l2-global sensitivity.\n\nTrue\nFalse\n\n\n\n\n\n\n7.13.2 Exercise 6\n\nQuestionSolution\n\n\nThis theorem improves accuracy when data can be broken into disjoint subsets.\n\nSequential composition theorem\nPostprocessing theorem\nParallel composition theorem\n\n\n\nThis theorem improves accuracy when data can be broken into disjoint subsets.\n\nSequential composition theorem\nPostprocessing theorem\nParallel composition theorem\n\n\n\n\n\n\n\n\nBowen, Claire M, and Simson Garfinkel. 2021. “Philosophy of Differential Privacy.” Notices of the American Mathematical Society 68 (10): 1727–39.\n\n\nBun, Mark, and Thomas Steinke. 2016. “Concentrated Differential Privacy: Simplifications, Extensions, and Lower Bounds.” In Theory of Cryptography Conference, 635–58. Springer.\n\n\nDwork, Cynthia, Krishnaram Kenthapadi, Fang McSherry, Ilya Mironov, and Moni Naor. 2006. “Our Data, Ourselves: Privacy via Distributed Noise Generation.” In Annual International Conference on the Theory and Applications of Cryptographic Techniques, 486–503. Springer.\n\n\nDwork, Cynthia, Frank McSherry, Kobbi Nissim, and Adam Smith. 2006. “Calibrating Noise to Sensitivity in Private Data Analysis.” In Theory of Cryptography: Third Theory of Cryptography Conference, TCC 2006, New York, NY, USA, March 4-7, 2006. Proceedings 3, 265–84. Springer.\n\n\nDwork, Cynthia, and Aaron Roth. 2014. “The Algorithmic Foundations of Differential Privacy.” Foundations and Trends in Theoretical Computer Science 9 (3–4): 211–407.\n\n\nMcSherry, Frank D. 2009. “Privacy Integrated Queries: An Extensible Platform for Privacy-Preserving Data Analysis.” In Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data, 19–30.\n\n\nNear, Joseph. 2020. “Differential Privacy for Privacy-Preserving Data Analysis: An Introduction to Our Blog Series.”\n\n\nNissim, Kobbi, Sofya Raskhodnikova, and Adam Smith. 2007. “Smooth Sensitivity and Sampling in Private Data Analysis.” In Proceedings of the 39th Annual Association for Computing Machinery Symposium on Theory of Computing, 75–84. Association for Computing Machinery."
  },
  {
    "objectID": "07_differential_privacy_details.html#footnotes",
    "href": "07_differential_privacy_details.html#footnotes",
    "title": "7  Formally Private Definitions, Fundamental Mechanisms, and Algorithms",
    "section": "",
    "text": "At the time of session, Elon Musk is the wealthiest person in the world.↩︎"
  },
  {
    "objectID": "08_differential_privacy_case_studies.html#recap",
    "href": "08_differential_privacy_case_studies.html#recap",
    "title": "8  Formally Private Case Studies",
    "section": "8.1 Recap",
    "text": "8.1 Recap\n\n8.1.1 Formal privacy\n\nAbility to quantify and adjust the privacy-utility trade-off, typically through parameters.\n\nlarger \\(\\epsilon\\) = less noise added to data = more accuracy, but less privacy\nsmaller \\(\\epsilon\\) = more noise added to data = less accuracy, but more privacy\n\nAbility to rigorously and mathematically prove the maximum privacy loss that can result from the release of information.\nFormal privacy definitions also allow one to compose multiple statistics. In other words, a data curator can compute the total privacy loss from multiple individual information releases.\n\n\n\n8.1.2 Statistical Disclosure Control Framework\n\nGenerally releasing confidential data involves the following steps to protect privacy:\n\n\n\n2010 Disclosure Avoidance System Framework\n\n\n\n\n\n8.1.3 Statistical Disclosure Control Method Steps\n\nPre-Processing Step: determining priorities for which statistics to preserve.\nPrivacy Step: applying a sanitizer to the desired statistic.\nPost-Processing Step: ensuring the results of the statistics are consistent with realistic constraints, such as negative population counts.\n\n\n\nStatistical Disclosure Control Terminology"
  },
  {
    "objectID": "08_differential_privacy_case_studies.html#case-studies",
    "href": "08_differential_privacy_case_studies.html#case-studies",
    "title": "8  Formally Private Case Studies",
    "section": "8.2 Case Studies",
    "text": "8.2 Case Studies\nWe will cover two case studies, where we will…\n\nMotivation: review the goal and context for the confidential data.\nData: analyze the data features and structure.\nDisclosure Risk: state which privacy definition and sanitizer is used.\nUtility Measures: outline the utility metrics.\nStatistical Disclosure Control Method: identify what is done at each step in creating the formally private synthetic data method.\nLimitations: discuss any limitations of the formally private synthetic data method.\n\nNote: Although the disclosure risk measures are different than synthetic data, we can use the same utility metrics as before to evaluate the quality of the formally private synthetic data."
  },
  {
    "objectID": "08_differential_privacy_case_studies.html#decennial-census",
    "href": "08_differential_privacy_case_studies.html#decennial-census",
    "title": "8  Formally Private Case Studies",
    "section": "8.3 2020 Decennial Census",
    "text": "8.3 2020 Decennial Census\n\n\n2020 Census Logo\n\n\n\n\n8.3.1 Motivation\nThe 2020 Census affects how the United States apportion the 435 seats for the United States House of Representatives, redistrict voting lines, plan for natural disasters, and allocate the $1.5 trillion budget, among many other things.\nSince the 1920 Privacy Act, decennial census data have been altered with a privacy-preserving method. Several laws now require the U.S. Census Bureau to protect census data products, but the most cited law is Title 13, which protects individual level data. In addition to the legal requirements, ethically, some might not be comfortable with people knowing there is a high presence of certain racial groups, such as Asian Americans, considering the lingering legacy of internment camps during World War II.\nWhy is the U.S. Census Bureau updating their disclosure avoidance system (DAS)?\n\n\n2010 Disclosure Avoidance System Framework\n\n\n\n\nThe technological landscape is constantly evolving. For instance, the tiny computers that fit in our pockets (i.e., smart phones) have more computational power than the average desktop in 2010. The last time Census Bureau updated DAS* was for the 1990 Census by applying data swapping (see Figure 3 for a summary of their process).\n\nNote: DAS is the overall statistical disclosure control methodology that the Census Bureau applies to protect their data products.\n\nTo reassess if the U.S. Census Bureau needed an update, they conducted a database reconstruction attack. Essentially, this type of attack evaluates whether there is too many independent statistics being published based on confidential data to recreate the underlying confidential data with little or no error. The Census Bureau tested this by:\n\nrecreating the individual level 2010 Census (i.e., age, sex, race, and Hispanic or Non-Hispanic ethnicity for every individual in each census block) from nine summary tables.\nthen uniquely identifying one-in-six records using publicly available data, such as what could be found on Facebook (Leclerc, 2019). This rate is higher for smaller groups.\n\nThese results are troubling, but a data intruder could not confirm whether a match was correct, or the reconstructed data were correct before the match without access to the actual data.\n\n\n\n2010 Census Reconstruction Attack Framework\n\n\n\nFor more information about the reconstruction attack: “The Census Bureau’s Simulated Reconstruction-Abetted Re-identification Attack on the 2010 Census” webinar materials.\nNote: The U.S. Census Bureau has received criticism for their reconstruction attack. Ruggles, Cleveland, and Van Riper (2021) claim that the Census Bureau did not test whether identifying individuals through their reconstruction attack is greater than random guess. Another way to think of it is the analogy of clinical trials, where you must have a control group to see if people get better or not with a treatment. The authors describe the U.S. Census Bureau’s reconstruction attack as just the “treatment” group without a good comparison of a control group. In other words, one would expect that some people in the treatment group would get better regardless of getting a treatment or not.\n\n\n8.3.2 Data\n\n\nU.S. Census Bureau Geographic Levels\n\n\n\nThe entire population of the United States of America. “The goal is to count everyone once, only once, and in the right place.” The image above shows the Census Bureau geographic levels for the U.S.\n\nThe 2020 Census Questionnaire contains a total of twelve questions.\nA person’s answer is limited to the check boxes or fill in the blank boxes, restricting the possible outcomes.\nThe form does not ask how the individuals within the household are related to anyone else filling out their own form.\n\n\n\n8.3.3 Disclosure Risk\n\nThe Census Bureau used zero-concentrated differential privacy (zCDP), which can be converted to \\((\\epsilon, \\delta)\\)-DP. This is why the U.S. Census Bureau reported the privacy parameter values as \\(\\epsilon=17.14\\) and \\(\\delta = 10^{-10}\\) for the persons file (Abowd et al. 2022).\nThey used the Gaussian Mechanism as the sanitizer.\n\n\n\n8.3.4 Utility Metrics\nThe Census Bureau checked for several things, such as, but not limited to:\n\nglobal metrics\n\nmean absolute error\nmean numeric error\nroot mean squared error\nmean absolute percent error\ncoefficient of variation\ntotal absolute error of shares\n\nSpecific metrics\n\nredistricting voting lines or school districts\ntotal absolute error of shares metric by county within each state as a share of that state, by incorporated place as a share of that state, and by minor civil divisions as a share of that state\n\n\n\n\n8.3.5 Statistical Disclosure Control Method\nBelow is a summary of the method (Abowd et al. 2022).\n\nPre-Processing Step: Calculate the crosstabulation of all variables for each geographic level (from states to census blocks).\n\n1 total count\n63 race, 2 ethnicity, 2 voting age\n3 institutional vs. non-institutional group quarter types\n1 residential and 7 possible group quarter types for a total of 8 (e.g., dorms and prisons)\n126 possible combinations of race and ethnicity\n126 possible combinations of race and voting age\n4 possible combinations of ethnicity and voting age\n252 possible combinations of race, ethnicity, and voting age\n2,016 = 252 x 8 possible combinations of race, ethnicity, and voting age at each residential and group quarter type\n\nPrivacy Step: Apply the Gaussian Mechanism to the 2,016 possible combinations unless that combination has no observations (i.e., treat as a structural zero) at each geographic level.\nPost-Processing Step: Implement the TopDown Algorithm (TDA) that enforces the following invariant statistics and constraints.\n\nInvariant statistics\n\nTotal population in each state, District of Columbia, and Puerto Rico.\nTotal number of housing units within each census block.\nNumber of group quarter facilities by type within each census block.\n\nConstraints:\n\nCounts must be integers\nSums of rows and column margins must sum to the total populations\nCounts must be consistent within tables, across tables, and across geographies\nIf there are zero housing units and zero group quarters at a geographic level, then no people may be assigned to that geography\nNumber of people in a group quarter is equal to or greater than 1\nNumber of people in a housing unit or group quarter is less than or equal to 99,999\nNo one is less than 18 in a group quarter\n\n\n\n\n\n2020 Disclosure Avoidance Framework\n\n\n\n\n\n8.3.6 Limitations\n\nThe TDA is optimized to the block group level, which resulted in the Census Bureau advising census data users to not use block level information.\nMiscommunication has caused the data user community to be at odds with the U.S. Census Bureau disclosure avoidance system team.\nAlthough the Census Bureau selected a value for the privacy-loss parameters, what is considered an appropriate value for the privacy-loss parameters is still an open research question for practical applications."
  },
  {
    "objectID": "08_differential_privacy_case_studies.html#exercise-1",
    "href": "08_differential_privacy_case_studies.html#exercise-1",
    "title": "8  Formally Private Case Studies",
    "section": "8.4 Exercise 1",
    "text": "8.4 Exercise 1\n\nQuestion\n\n\nThe TDA finds the optimal distribution of counts starting from the state down to census block. Other hierarchies can be applied, such as botton-up from census block to state or from 4-digit NAICS code to 2-digit NAICS code.\nSuppose we have establishment data that has the number of establishments at the state and county level with total net profit/loss and the associated 3-digit NAICS code (i.e., four variable data).\nWhat hierarchy do you think would be best for this dataset and why?"
  },
  {
    "objectID": "08_differential_privacy_case_studies.html#nist-pscr-differential-privacy-synthetic-data-challenge",
    "href": "08_differential_privacy_case_studies.html#nist-pscr-differential-privacy-synthetic-data-challenge",
    "title": "8  Formally Private Case Studies",
    "section": "8.5 NIST PSCR Differential Privacy Synthetic Data Challenge",
    "text": "8.5 NIST PSCR Differential Privacy Synthetic Data Challenge\n\n\nScreenshot from the 2018 NIST Differential Privacy Synthetic Data Challenge\n\n\n\n\n8.5.1 Motivation\nThe National Institute of Standards and Technology Public Safety Communications Research Division (NIST PSCR) hosted two Differential Privacy Data Challenges to encourage new innovations in formally private synthetic data methodologies (Ridgeway et al. 2020).\n\n2018 Differential Privacy Synthetic Data Challenge used emergency response data and census public micro use data.\n2020 Differential Privacy Temporal Map Challenge used 911 incidences data, American Community Survey demographic data, and Chicago taxi ride data.\n\nWe will focus on the 2018 challenge, because NIST and the contestants have released more publications that are associated with the challenge.\n\n\n8.5.2 Data\nThe data challenged had three matches (Bowen and Snoke 2021):\n\nMatches #1 and #2 used the San Francisco Fire Department’s Call for Service data, with a different year for each match.\n\nThe Service data contained a total of 32 categorical and continuous variables with roughly 236,000 to 314,000 observations, respectively. For example, Call Type Group, Number of Alarms, City, Zip Code of Incident, Neighborhood, Emergency Call Received Date and Time, Emergency Call Response Date and Time, Supervisor District, and Station Area.\nOne of the main challenges was accounting for the structural zeros, e.g., not all emergency calls resulted in someone being dispatched to a location.\n\n\nA subset of the 2017 San Francisco Fire Department’s Call for Service data\n\n\n# A tibble: 313,607 × 5\n   `ALS Unit` `Final Priority` `Call Type Group` `Original Priority` Priority\n        &lt;dbl&gt;            &lt;dbl&gt;             &lt;dbl&gt;               &lt;dbl&gt;    &lt;dbl&gt;\n 1          0                0                 0                   0        0\n 2          0                0                 0                   0        0\n 3          0                0                 0                   0        0\n 4          0                0                 0                   0        0\n 5          0                0                 0                   0        0\n 6          0                0                 1                   0        0\n 7          0                0                 0                   0        0\n 8          1                0                 0                   0        0\n 9          1                0                 1                   0        0\n10          0                0                 0                   0        0\n# ℹ 313,597 more rows\n\n\n\nFor Match #3, challenge participants trained their methods on the Colorado Public Use Microdata Sample (PUMS) data, and their methods were evaluated on the Arizona and Vermont PUMS data for final scoring.\n\nAll three PUMS data had 98 categorical and continuous variables with the number of observations ranging from about 210,000 to 662,000. Gender, Race, Age, City, City Population, School, Veteran Status, Rent, and Income Wage were a few of the 98 variables.\n\n\nA subset of the Vermont PUMS data\n\n\n# A tibble: 211,228 × 10\n   METAREA METAREAD SPLIT METRO SCHOOL   SEX RESPONDT SLREC LABFORCE SSENROLL\n     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1       0        0     0     1      1     1        1     1        2        0\n 2       0        0     0     1      1     2        2     1        1        0\n 3       0        0     0     1      1     2        1     1        2        0\n 4       0        0     0     1      2     1        1     1        1        0\n 5       0        0     0     1      2     1        1     1        0        0\n 6       0        0     0     1      2     2        1     1        0        0\n 7       0        0     0     1      2     1        1     1        0        0\n 8       0        0     0     1      1     2        1     1        0        0\n 9       0        0     0     1      1     2        1     1        0        0\n10       0        0     0     1      1     1        2     1        2        0\n# ℹ 211,218 more rows\n\n\n\nCompetitors were given the utility metrics and training data to test and benchmark their methods.\nNumber of observations in the confidential data could be treated as known, but the released formally private data did not need to match the confidential data.\nData can be downloaded in the Urban Institute Data Catelog.\n\n\n\n8.5.3 Disclosure Risk\n\nContestants used both \\(\\epsilon\\)-DP (i.e., pure-DP) and \\((\\epsilon, \\delta)\\)-DP (i.e., approximate-DP) definitions, where NIST PSCR set \\(\\epsilon=\\{0.01, 0.1, 1\\}\\) and \\(\\delta = 0.001\\) for Matches #1 and #2 and \\(\\epsilon=\\{1, 3, 8\\}\\) and \\(\\delta = 0.001\\) for Match #3\nThe contestants used the Laplace Mechanism and the Gaussian Mechanism as the sanitizer.\n\nNote: Some contestants chose not to use \\(\\delta\\), because they used \\(\\epsilon\\)-DP.\n\n\n8.5.4 Utility Metrics\nNIST PSCR created a “clustering task”, “classification task”, and “regression task” (Bowen and Snoke 2021).\n\nThe “clustering” analysis compared the 3-way marginal density distributions between the original and synthetic data sets, where the utility score was the absolute difference in the density distributions. NIST PSCR repeated this calculation 100 times on randomly selected variables, and then averaged for the final clustering score.\nThe “classification” analysis at a high level tests similarity between the original and synthetic data in the randomly selected subsets of the joint distributions, so the term “classification” is slightly misleading.\nThe “regression” analysis used a two-part score system.\n\nThe first score calculated the mean-square deviation of the Gini indices in the original and synthetic data for every city on the gender wage gap, and then averaged those values over the total number of cities in the original data.\nThe second score compared how the cities in the original and the synthetic data sets were ranked on gender pay gap, calculating the rank differences by the mean-square deviation.\nNIST PSCR averaged these two scores for the overall regression analysis score.\n\nFor all three NIST PSCR scoring metrics, a larger value in the challenge indicated that the synthetic data preserved the original data well.\n\n\n\n8.5.5 Statistical Disclosure Control Method\nWe will look at two of the finalist submissions to compare their approaches that performed well overall in the contest.\nDPFieldGroups - nonparametric formally private method.\n\nPre-Processing Step: cluster highly correlated variables based on public data (i.e., identify marginals that are highly correlated).\nPrivacy Step: apply the Laplace Mechanism on the original data cell counts based on the marginals from the previous step.\nPost-Processing Step: reduce the noisy counts to zero if the counts fall below a threshold calculated from \\(\\epsilon\\) and the \\(\\log_{10}\\) number of bins in the particular marginal histogram.\n\n\n\nA visual representation of DPFieldGroups\n\n\n\nMaximum Spanning Tree (NIST-MST) - a parametric formally private method (McKenna, Miklau, and Sheldon 2021).\n\nPre-Processing Step: select the highly correlated variables in two ways; a) manually by domain expert knowledge or, b) automatically by an algorithm.\nPrivacy Step: apply the Gaussian Mechanism and the 1-, 2-, and 3-way marginals on those identified correlated variables.\nPost-Processing Step: ensure consistency using the Private-Probabilistic Graphic Model (PGM) algorithm, that estimates a high-dimensional data distribution from the noisy measurements and then generates the synthetic data.\n\n\n\nA viusal representation of the Maximum Spanning Tree\n\n\n\n\n\n8.5.6 Limitations\n\nNIST PSCR announced the metric criteria at the start of each match, so the competitors could modify their approach based on the scoring metrics. This is a known issue in most data challenges.\nAssumed the data are counts and treated the training data as public."
  },
  {
    "objectID": "08_differential_privacy_case_studies.html#exercise-2",
    "href": "08_differential_privacy_case_studies.html#exercise-2",
    "title": "8  Formally Private Case Studies",
    "section": "8.6 Exercise 2",
    "text": "8.6 Exercise 2\n\nQuestion 1Solution 1\n\n\nWhich method do you think performed the best for the NIST Data Challenge when applied to all three datasets?\n\n\nNIST-MST won the entire competition and went on to win the 2020 Differential Privacy Temporal Map Challenge.\n\n\n\n\nQuestion 2Hint 2\n\n\nDoes the result from the previous question surprise you? Why or why not?\n\n\nThink about the metrics NIST PSCR created for the challenge.\n\n\n\n\nQuestion 3Solution 3\n\n\nWhich method do you think performed the best for wider range of utility metrics?\n\n\nBowen and Snoke (2021) used the formally private synthetic datasets from the 2018 competition and applied various other utiity metrics. They found that DPFieldGroups performed better overall compared to the other methods. The reasons are:\n\nParametric SDC method like NIST-MST add noise in a less direct fashion, drawing values from a high-dimensional distribution that must first be approximated.\nThe nonparametric SDC method like DPFieldGroups add noise directly to the marginals.\nMethods that post-process the data more introduce additional noise into the data if the specific utility metric is not kept in mind. DPFieldGroups had the least involved post-processed step out of all the top competitors but had the best overall privacy-utility trade-off."
  },
  {
    "objectID": "08_differential_privacy_case_studies.html#googles-covid-19-mobility-reports",
    "href": "08_differential_privacy_case_studies.html#googles-covid-19-mobility-reports",
    "title": "8  Formally Private Case Studies",
    "section": "8.7 Google’s COVID-19 Mobility Reports",
    "text": "8.7 Google’s COVID-19 Mobility Reports\n\n\nScreenshot of Google’s COVID-19 Mobility Reports landing page\n\n\n\n\n8.7.1 Motivation\nGoogle created the COVID-19 Community Mobility Reports to provide movement trends over time at different geographic regions for various categories, such as transit stations and workplaces (Aktay et al. 2020).\nThe figure below is a screenshot of the Google COVID-19 Community Mobility Report for Santa Fe County from December 4, 2020 to January 15, 2021. The plots show average movement increase or decrease for each category from the baseline.\nNote: Google calculated the baseline as the median value for the corresponding day of the week, during the 5-week period January 3 to February 6, 2020. This is fixed.\n\n\nScreenshot of the Google’s COVID-19 Mobility Reports for Santa Fe County in New Mexico\n\n\n\n\n\n8.7.2 Data\n\nGeospatial and temporal (i.e., space and time) data of all Google users who consented to their data being collected. Google researchers refer to the movement count as the “metric”. We will refer to this as the “target statistic” to not be confused with other terms, such as “utility metric”.\n\n\n\n8.7.3 Disclosure Risk\n\nGoogle researchers used \\(\\epsilon\\)-DP (i.e., pure-DP) with \\(\\epsilon = 0.11\\) and \\(0.22\\) per 1 hour*, depending on geographic granularity level.\nThey used the Laplace Mechanism as the sanitizer.\n\n\n\n8.7.4 Utility Metrics\n\nGoogle researchers computed the 97.5 percent confidence interval of the noisy target statistic (i.e., \\([m_{min}, m_{max}]\\)) and confidence interval of the baseline (i.e., \\([b_{min}, b_{max}]\\)) of the aggregated counts.\nNext, they computed the ratios \\(m_{min}/b_{max}\\) and \\(m_{max}/b_{min}\\). -If either ratio had “…a 5 percent chance (or higher) of being wrong by more than \\(\\pm 10\\) absolute percentage points”, then they removed the information.\n\n*Note: For most location types, the Google researchers focused on the 12 active hours.\n\n\n8.7.5 Statistical Disclosure Control Method\n\nThe Google researchers simplified the geospatial problem by releasing the information at larger geography levels (e.g., county) and with no other types of information (e.g., demographic information).\nFigure below is a diagram of Google’s approach (Aktay et al. 2020).\n\n\n\nSystem diagram of Google’s COVID-19 Mobility Reports method\n\n\n\n\nPre-Processing Step: Calculate each user’s movement for the current week (i.e., target statistic) and the baseline week.\nPrivacy Step: Add Laplace noise to the target statistic and baseline week values based on geographic granularity level (see the figure below for specific privacy parameter values).\nPost-Processing Step: Check whether the ratio of the median baseline against the noisy target statistic exceeds the “unreliable metric” criteria. If so, remove the value from the final report.\n\nFigure below shows the privacy or noise parameters used for the Google COVID-19 Mobility Reports at each geographic level. Granularity level 0 corresponds to country / region, level 1 corresponds to level geopolitical subdivsions (e.g., U.S. States), and level 2 corresponds to higher-resolution granularity (e.g., U.S. counties).\nNote: The Google researchers did not publish any target statistics for geographic regions smaller than 3km\\(^2\\).\n\n\n8.7.6 Limitations\n\nNo smaller geographic levels than county and suppressed data if the counts were too small (see figure below).\nConverted the data to counts.\nMuch higher privacy-loss budget. i.e., \\(\\epsilon = 1.32\\) and \\(2.64\\) per day or \\(\\epsilon = 36.9\\) and \\(79.2\\) per month.\n\nThe figure below is a screenshot of the Google COVID-19 Community Mobility Report for Los Alamos County from December 4, 2020 to January 15, 2021. The plots show average movement increase or decrease for each category from the baseline. The * indicates that “The data doesn’t meet quality and privacy thresholds for every day in the chart.”\n\n\nScreenshot of the Google’s COVID-19 Mobility Reports for Los Alamos County in New Mexico"
  },
  {
    "objectID": "08_differential_privacy_case_studies.html#exercise-3",
    "href": "08_differential_privacy_case_studies.html#exercise-3",
    "title": "8  Formally Private Case Studies",
    "section": "8.8 Exercise 3",
    "text": "8.8 Exercise 3\n\nQuestion 1Question 2\n\n\nDo you agree or disagree with the Google researchers’ choice of utility metrics and privacy parameters?\n\n\nWhat other utility metrics would you use to assess the quality of location and time data?"
  },
  {
    "objectID": "08_differential_privacy_case_studies.html#bonus-exercises",
    "href": "08_differential_privacy_case_studies.html#bonus-exercises",
    "title": "8  Formally Private Case Studies",
    "section": "8.9 Bonus Exercises",
    "text": "8.9 Bonus Exercises\n\n8.9.1 Exercise 4\n\nQuestionSolution\n\n\nWhich of these is a step (and not part of a step) in the overall statistical disclosure control framework?\n\nPre-processing\nAdding noise\nIdentifying invariants\n\n\n\nWhich of these is a step (and not part of a step) in the overall statistical disclosure control framework?\n\nPre-processing\nAdding noise\nIdentifying invariants\n\n\n\n\n\n\n8.9.2 Exercise 5\n\nQuestionSolution\n\n\nThe U.S. Census Bureau used what privacy definition for the 2020 DAS?\n\nPure differential privacy\nApproximate differential privacy\nZero-concentrated differential privacy\n\n\n\nThe U.S. Census Bureau used what privacy definition for the 2020 DAS?\n\nPure differential privacy\nApproximate differential privacy\nZero-concentrated differential privacy\n\n\n\n\n\n\n8.9.3 Exercise 6\n\nQuestionSolution\n\n\nFor the NIST PSCR Differential Privacy Data Challenge, what was NOT an assumption when developing their formally private methods prior to scoring?\n\nNumber of observations\nInformation from the training dataset is public\nUtility metrics for scoring\n\n\n\nFor the NIST PSCR Differential Privacy Data Challenge, what was NOT an assumption when developing their formally private methods prior to scoring?\n\nNumber of observations\nInformation from the training dataset is public\nUtility metrics for scoring\n\n\n\n\n\n\n\n\nAbowd, John M, Robert Ashmead, Ryan Cumings-Menon, Simson Garfinkel, Micah Heineck, Christine Heiss, Robert Johns, et al. 2022. “The 2020 Census Disclosure Avoidance System Topdown Algorithm.” Harvard Data Science Review, no. Special Issue 2.\n\n\nAktay, Ahmet, Shailesh Bavadekar, Gwen Cossoul, John Davis, Damien Desfontaines, Alex Fabrikant, Evgeniy Gabrilovich, et al. 2020. “Google COVID-19 Community Mobility Reports: Anonymization Process Description (Version 1.1).” arXiv Preprint arXiv:2004.04145.\n\n\nBowen, Claire McKay, and Joshua Snoke. 2021. “Comparative Study of Differentially Private Synthetic Data Algorithms from the NIST PSCR Differential Privacy Synthetic Data Challenge.” Journal of Privacy and Confidentiality 11 (1).\n\n\nMcKenna, Ryan, Gerome Miklau, and Daniel Sheldon. 2021. “Winning the NIST Contest: A Scalable and General Approach to Differentially Private Synthetic Data.” arXiv Preprint arXiv:2108.04978.\n\n\nRidgeway, Diane, Christine Task, Gary Howarth, and David Van Ballegooijen. 2020. “Crisis Collaborations: Challenges for Safe Data Sharing with Differential Privacy.” NIST (PSCR 2020).\n\n\nRuggles, Steven, Lara Cleveland, and David Van Riper. 2021. “Risk Assessment Procedures for the 2020 US Census.”"
  },
  {
    "objectID": "d_where-to-go.html#an-appropriate-and-interpretable-privacy-loss-budget",
    "href": "d_where-to-go.html#an-appropriate-and-interpretable-privacy-loss-budget",
    "title": "Where to Go From Here",
    "section": "An appropriate and interpretable privacy loss budget",
    "text": "An appropriate and interpretable privacy loss budget\nEarlier today, we discussed how early differential privacy researchers thought \\(\\epsilon=1\\) or \\(\\epsilon=2\\) were considered the upper bounds, whereas we are seeing larger values in more recent applications. Rogers et al. (2020) created the table below on what were the daily and monthly cost for various formally private methods implemented in industry. The 2020 Census used \\(\\epsilon=17.14\\), which converts to a ratio of 27,784,809. However, in the table below, the largest value of \\(\\epsilon\\) is 769, which converts to a ratio rapidly approaching \\(\\infty\\).\nFig. 7: Table from Rogers et al. (2020) \nSecond, it is difficult to interpret parameters for formally private methods. This may explain why it is difficult to know what values of \\(\\epsilon\\) are appropriate and why \\(\\epsilon\\) values are higher than originally theorized. For pure DP, \\(\\epsilon\\) places a bound on a log ratio of two probabilities. This ratio represents a difficult to conceive relative risk and is on a log scale.\n\n\nTable 1: The ratio increases exponentially with \\(\\epsilon\\)\n\n\n\\(\\epsilon\\)\nRatio\n\n\n\n\n0.25\n1.28\n\n\n0.5\n1.65\n\n\n0.75\n2.12\n\n\n1\n2.72\n\n\n2\n7.39\n\n\n4\n54.60\n\n\n6\n403.43\n\n\n8\n2980.96\n\n\n10\n22026.47\n\n\n\n\nTable 1 shows demonstrates how quickly the ratio in the definition of pure DP explodes as \\(\\epsilon\\) increases by modest amounts. The community does not have a clear way to reason about the difference between a ratio of 2,000 and a ratio of 22,000. On its own, \\(\\epsilon\\) and other privacy parameters in formal privacy, are like a speedometer without any labels. The parameters can show you faster or slower, but they can’t tell you how quickly you are going. And without knowing exactly how fast you are going, it’s tough to drive near the speed limit. This is why we need people to add context and interpretability to the speedometer."
  },
  {
    "objectID": "d_where-to-go.html#more-applied-work",
    "href": "d_where-to-go.html#more-applied-work",
    "title": "Where to Go From Here",
    "section": "More Applied Work",
    "text": "More Applied Work\nPart of the reason we do not know what are reasonable values of \\(\\epsilon\\) is because most differential privacy research is still theoretical. We need to apply differential privacy to understand the privacy-utility trade-off more fully under several conditions. For example, privacy experts should explore more small, practical formally private applications rather than highly complicated, theoretical scenarios to better discern what are some of the data challenges and how we should address them. The same idea applies to other SDC methods, such as synthetic data, where we do not have enough use cases.\nFor synthetic data generation, tuning risk-utility trade-off is a much under-researched area that holds promise for creating more effective and efficient synthesis. We also have not developed good synthetic data methods that handle survey weights, longitudinal, and texts well despite the great demand and abundance of such data types requiring privacy protection. Additionally, we need more computationally-feasible synthetic data approaches for spatial information."
  },
  {
    "objectID": "d_where-to-go.html#communication-education-and-resources",
    "href": "d_where-to-go.html#communication-education-and-resources",
    "title": "Where to Go From Here",
    "section": "Communication, Education, and Resources",
    "text": "Communication, Education, and Resources\n\nLack of resources\nSuppose someone told you that they had data that contained records of individuals, including demographics such as their age, their sex, and their race along with financial information. They want to explore applying machine learning methods to gain unique insights into the data. What resources would you recommend?\nNow, suppose this person, with the same data, asked you how to apply data privacy and confidentiality methods. Before this course, would you have any idea what resources to recommend?\nLikely, your response to these questions drastically varied!\nHigh profile use cases, like the 2020 Census data products in the United States, have improved communication about synthetic data and other data privacy concepts, such as differential privacy. But, the available materials are limited compared to what is needed.\n\n\nNot enough tools\nAnother challenge is having enough computational tools to implement the various SDC methods. While researchers should understand the basics of SDC methods, they do not necessarily need to thoroughly understand them.\n\n\nFew people who are experts\nThere are very few people who have the technical knowledge and the coding ability to implement SDC methods. Some propose that we need to teach the next generation of data privacy researchers. However, most higher education institutions do not provide data privacy courses. If they are taught, professors usually teach them at the graduate level in computer science departments, which is not representative of those who depend on and contribute to the field."
  },
  {
    "objectID": "d_where-to-go.html#community",
    "href": "d_where-to-go.html#community",
    "title": "Where to Go From Here",
    "section": "Community",
    "text": "Community\nThese examples emphasize the need for greater involvement of statisticians and data scientists in the synthetic data and broader statistical data privacy community. As of now, most meetings and conferences tend to focus on theoretical aspects within the computer science field. Currently, there does not exist a regular conference on the intersection of data privacy and public policy.\n\n\n\n\nHu, Jingchen, and Claire McKay Bowen. 2023. “Advancing Microdata Privacy Protection: A Review of Synthetic Data.” arXiv Preprint arXiv:2308.00872.\n\n\nRogers, Ryan, Subbu Subramaniam, Sean Peng, David Durfee, Seunghyun Lee, Santosh Kumar Kancha, Shraddha Sahay, and Parvez Ahammad. 2020. “LinkedIn’s Audience Engagements API: A Privacy Preserving Data Analytics System at Scale.” arXiv Preprint arXiv:2002.05839.\n\n\nWilliams, Aaron R, and Claire McKay Bowen. 2023. “The Promise and Limitations of Formal Privacy.” Wiley Interdisciplinary Reviews: Computational Statistics, e1615."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Abowd, John M, Robert Ashmead, Ryan Cumings-Menon, Simson Garfinkel,\nMicah Heineck, Christine Heiss, Robert Johns, et al. 2022. “The\n2020 Census Disclosure Avoidance System Topdown Algorithm.”\nHarvard Data Science Review, no. Special Issue 2.\n\n\nAktay, Ahmet, Shailesh Bavadekar, Gwen Cossoul, John Davis, Damien\nDesfontaines, Alex Fabrikant, Evgeniy Gabrilovich, et al. 2020.\n“Google COVID-19 Community Mobility Reports: Anonymization Process\nDescription (Version 1.1).” arXiv Preprint\narXiv:2004.04145.\n\n\nBarrientos, Andrés F., Aaron R. Williams, Joshua Snoke, and Claire McKay\nBowen. 2021. “A Feasibility Study of\nDifferentially Private Summary\nStatistics and Regression\nAnalyses with Evaluations on\nAdministrative and Survey\nData.” https://doi.org/10.48550/ARXIV.2110.12055.\n\n\nBenedetto, Gary, Jordan C Stanley, Evan Totty, et al. 2018. “The\nCreation and Use of the SIPP Synthetic Beta V7. 0.” US Census\nBureau.\n\n\nBowen, Claire McKay, Victoria Bryant, Leonard Burman, Surachai\nKhitatrakun, Robert McClelland, Philip Stallworth, Kyle Ueyama, and\nAaron R Williams. 2020. “A Synthetic Supplemental Public Use File\nof Low-Income Information Return Data: Methodology, Utility, and Privacy\nImplications.” In International Conference on Privacy in\nStatistical Databases, 257–70. Springer.\n\n\nBowen, Claire McKay, Fang Liu, and Bingyue Su. 2021.\n“Differentially Private Data Release via Statistical Election to\nPartition Sequentially.” Metron 79 (1): 1–31.\n\n\nBowen, Claire McKay, and Joshua Snoke. 2021. “Comparative Study of\nDifferentially Private Synthetic Data Algorithms from the NIST PSCR\nDifferential Privacy Synthetic Data Challenge.” Journal of\nPrivacy and Confidentiality 11 (1).\n\n\nBowen, Claire M, and Simson Garfinkel. 2021. “Philosophy of\nDifferential Privacy.” Notices of the American Mathematical\nSociety 68 (10): 1727–39.\n\n\nBun, Mark, and Thomas Steinke. 2016. “Concentrated Differential\nPrivacy: Simplifications, Extensions, and Lower Bounds.” In\nTheory of Cryptography Conference, 635–58. Springer.\n\n\nBureau, US Census. 2021. “Disclosure Avoidance for the 2020\nCensus: An Introduction.” US Government Publishing Office\nWashington, DC.\n\n\nDrechsler, Jörg, and Jingchen Hu. 2021. “Synthesizing Geocodes to\nFacilitate Access to Detailed Geographical Information in Large-Scale\nAdministrative Data.” Journal of Survey Statistics and\nMethodology 9 (3): 523–48.\n\n\nDwork, Cynthia, Krishnaram Kenthapadi, Fang McSherry, Ilya Mironov, and\nMoni Naor. 2006. “Our Data, Ourselves: Privacy via Distributed\nNoise Generation.” In Annual International Conference on the\nTheory and Applications of Cryptographic Techniques, 486–503.\nSpringer.\n\n\nDwork, Cynthia, Frank McSherry, Kobbi Nissim, and Adam Smith. 2006.\n“Calibrating Noise to Sensitivity in Private Data\nAnalysis.” In Theory of Cryptography: Third Theory of\nCryptography Conference, TCC 2006, New York, NY, USA, March 4-7, 2006.\nProceedings 3, 265–84. Springer.\n\n\nDwork, Cynthia, and Aaron Roth. 2014. “The Algorithmic Foundations\nof Differential Privacy.” Foundations and Trends\nin Theoretical Computer Science 9 (3–4): 211–407.\n\n\nFellegi, Ivan P. 1972. “On the Question of Statistical\nConfidentiality.” Journal of the American Statistical\nAssociation 67 (337): 7–18.\n\n\nFienberg, Stephen E, and Jiashun Jin. 2018. “Statistical\nDisclosure Limitation for~ Data~ Access.” In Encyclopedia of\nDatabase Systems (2nd Ed.).\n\n\nHu, Jingchen, and Claire McKay Bowen. 2023. “Advancing Microdata\nPrivacy Protection: A Review of Synthetic Data.” arXiv\nPreprint arXiv:2308.00872.\n\n\nMatthews, Gregory J, and Ofer Harel. 2011. “Data Confidentiality:\nA Review of Methods for Statistical Disclosure Limitation and Methods\nfor Assessing Privacy.” Statistics Surveys 5: 1–29.\n\n\nMayer, Jonathan, Patrick Mutchler, and John C Mitchell. 2016.\n“Evaluating the Privacy Properties of Telephone Metadata.”\nProceedings of the National Academy of Sciences 113 (20):\n5536–41.\n\n\nMcKenna, Ryan, Gerome Miklau, and Daniel Sheldon. 2021. “Winning\nthe NIST Contest: A Scalable and General Approach to Differentially\nPrivate Synthetic Data.” arXiv Preprint\narXiv:2108.04978.\n\n\nMcSherry, Frank D. 2009. “Privacy Integrated Queries: An\nExtensible Platform for Privacy-Preserving Data Analysis.” In\nProceedings of the 2009 ACM SIGMOD International Conference on\nManagement of Data, 19–30.\n\n\nMendelevitch, Ofer, and Michael D Lesh. 2021. “Fidelity and\nPrivacy of Synthetic Medical Data.” arXiv Preprint\narXiv:2101.08658.\n\n\nNear, Joseph. 2020. “Differential Privacy for Privacy-Preserving\nData Analysis: An Introduction to Our Blog Series.”\n\n\nNissim, Kobbi, Sofya Raskhodnikova, and Adam Smith. 2007. “Smooth\nSensitivity and Sampling in Private Data Analysis.” In\nProceedings of the 39th Annual Association for Computing Machinery\nSymposium on Theory of Computing, 75–84. Association for Computing\nMachinery.\n\n\nRidgeway, Diane, Christine Task, Gary Howarth, and David Van\nBallegooijen. 2020. “Crisis Collaborations: Challenges for Safe\nData Sharing with Differential Privacy.” NIST (PSCR\n2020).\n\n\nRogers, Ryan, Subbu Subramaniam, Sean Peng, David Durfee, Seunghyun Lee,\nSantosh Kumar Kancha, Shraddha Sahay, and Parvez Ahammad. 2020.\n“LinkedIn’s Audience Engagements API: A Privacy Preserving Data\nAnalytics System at Scale.” arXiv Preprint\narXiv:2002.05839.\n\n\nRubin, Donald B. 1977. “Formalizing Subjective Notions about the\nEffect of Nonrespondents in Sample Surveys.” Journal of the\nAmerican Statistical Association 72 (359): 538–43.\n\n\nRuggles, Steven, Lara Cleveland, and David Van Riper. 2021. “Risk\nAssessment Procedures for the 2020 US Census.”\n\n\nSnoke, Joshua, Gillian M. Raab, Beata Nowok, Chris Dibben, and\nAleksandra Slavkovic. 2018a. “General and Specific Utility\nMeasures for Synthetic Data.” Journal of the Royal\nStatistical Society: Series A (Statistics in Society) 181 (3):\n663–88. https://doi.org/10.1111/rssa.12358.\n\n\nSnoke, Joshua, Gillian M Raab, Beata Nowok, Chris Dibben, and Aleksandra\nSlavkovic. 2018b. “General and Specific Utility Measures for\nSynthetic Data.” Journal of the Royal Statistical Society:\nSeries A (Statistics in Society) 181 (3): 663–88.\n\n\nSweeney, Latanya. 2000. “Simple Demographics Often Identify People\nUniquely.” Health (San Francisco) 671 (2000): 1–34.\n\n\nWang, Teng, Xuefeng Zhang, Jingyu Feng, and Xinyu Yang. 2020. “A\nComprehensive Survey on Local Differential Privacy Toward Data\nStatistics and Analysis.” Sensors 20 (24): 7030. https://doi.org/10.3390/s20247030.\n\n\nWarner, Stanley L. 1965. “Randomized Response: A Survey Technique\nfor Eliminating Evasive Answer Bias.” Journal of the American\nStatistical Association 60 (309): 63–69. https://doi.org/10.1080/01621459.1965.10480775.\n\n\nWilliams, Aaron R, and Claire McKay Bowen. 2023. “The Promise and\nLimitations of Formal Privacy.” Wiley Interdisciplinary\nReviews: Computational Statistics, e1615.\n\n\nWoo, Mi-Ja, Jerome P Reiter, Anna Oganian, and Alan F Karr. 2009.\n“Global Measures of Data Utility for Microdata Masked for\nDisclosure Limitation.” Journal of Privacy and\nConfidentiality 1 (1)."
  }
]